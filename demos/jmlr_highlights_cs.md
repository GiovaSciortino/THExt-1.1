# [Strong Consistency, Graph Laplacians, and the Stochastic Block Model](https://jmlr.org/papers/volume22/20-391/20-391.pdf)
- In this work, we will study the performance of spectral clustering in community detection for the stochastic block model.

- However, matrix perturbation under 2 -norm, in spite of its convenience, becomes rather limited in studying the exact recovery of hidden community structure.

- We prove that spectral clustering is able to achieve strong consistency when the triple (n, p, q) satisfies the information-theoretic limits Abbe et al. (2016); Mossel et al. (2016) where p = Œ±n ‚àí1 log n and q = Œ≤n ‚àí1 log n. In particular, our analysis of the normalized Laplacian is new and should be of independent interest.

- The analysis of the stochastic block model originated from Holland et al. (1983) in the study of social networks.




# [Towards a Unified Analysis of Random Fourier Features](https://jmlr.org/papers/volume22/20-1369/20-1369.pdf)
- The required number of features can be ‚Ñ¶(log n log log n)

- The ‚Ñ¶(n) requirement on the number of features is too restrictive and precludes any computational savings.

- In particular, we have given generic bounds on the number of features required for consistency of learning with two sampling strategies: leverage weighted and plain random Fourier features.

- Moreover, the number of features can be reduced even further if a data-dependent sampling distribution is employed.




# [Edge Sampling Using Local Network Information](https://jmlr.org/papers/volume22/18-240/18-240.pdf)
- Measuring the strength of local connectivity through local networks is more challenging, and we leave it for future work.

- Parameter Œ± measures the average strength of local network connectivity.

- This simple statistic provides an easy way to measure the strength of network local connectivity through parameter Œ±, which directly controls the accuracy of the sampling method.

- The following theorem shows that the sparsified network obtained from a hypergraph satisfies the strong spectral property.




# [When Does Gradient Descent with Logistic Loss Find Interpolating Two-layer Networks?](https://jmlr.org/papers/volume22/20-1372/20-1372.pdf)
- However, they do not prove that the training loss converges to zero.

- We demonstrated that gradient descent drives the logistic loss of finite-width two-layer Huberized ReLU networks to zero if the initial loss is small enough.

- It guarantees that if the initial loss is small then gradient descent drives the logistic loss to zero.

- In this paper, we show that, under two sets of conditions, training fixed-width two-layer networks with gradient descent drives the logistic loss to zero.




# [Non-attracting Regions of Local Minima in Deep and Wide Neural Networks](https://jmlr.org/papers/volume22/19-586/19-586.pdf)
- In this section we prove Theorem 4, showing the existence of local minima at infinity in neural networks.

- We have proved the existence of suboptimal local minima for regression neural networks with sigmoid activation functions of arbitrary width.

- Being able to reach a saddle point from a local minimum by a path of non-increasing loss shows that we found a non-attracting region of local minima.

- We theoretically proved the possibility of suboptimal local minima in deep and wide networks and empirically validated their existence.




# [Information criteria for non-normalized models](https://jmlr.org/papers/volume22/20-1366/20-1366.pdf)
- In this study, we develop information criteria for non-normalized models estimated by NCE or score matching.

- Experimental results show that these procedures successfully select the appropriate non-normalized model in a data-driven manner.

- In Sections 5 and 6, we develop information criteria for non-normalized models estimated by NCE and score matching, respectively.

- The proposed criteria are approximately unbiased estimators of discrepancy measures for non-normalized models.




# [Preference-based Online Learning with Dueling Bandits: A Survey](https://jmlr.org/papers/volume22/18-546/18-546.pdf)
- The aim of this paper is to provide a survey of the state of the art in the field of preference-based multi-armed bandits (PB-MAB); it updates and significantly extends an earlier review by Busa-Fekete and H√ºllermeier (2014).

- In the following, we review all known methods of the preference-based multi-armed bandit literature for the axiomatic approaches.

- In this paper, we surveyed the state of the art in preference-based online learning with bandit algorithms, a relatively recent research field that we referred to as preference-based multi-armed bandits (PB-MAB), and which is also known under the notion of "dueling bandits".

- Extending the multi-armed bandit setting to the case of preference-based feedback, i.e., the case in which the online learner is allowed to compare arms in a qualitative way, is therefore a promising idea.




# [Ranking and synchronization from pairwise measurements via SVD](https://jmlr.org/papers/volume22/19-542/19-542.pdf)
- The resulting pipeline is reported to improve over state-of-the-art in both simulated scenarios and real data.

- Additionally, we provide extensive numerical experiments on both synthetic and real data, showing that in certain noise and sparsity regimes, our proposed algorithms perform comparable or better than state-of-the-art methods.

- We consider a variety of performance metrics summarized further below, altogether highlighting the competitiveness of our proposed SVD-based algorithms with that of state-of-the-art methods.

- We provide a detailed theoretical consistency analysis for both algorithms for a random measurement model (see Section 3) in terms of robustness against sampling sparsity of the measurement graph and noise level.




# [Phase Diagram for Two-layer ReLU Neural Networks at Infinite-width Limit](https://jmlr.org/papers/volume22/20-1123/20-1123.pdf)
- However, its exact range in the phase diagram remains unclear.

- In this paper, we characterized the linear, critical, and condensed regimes with distinctive features and draw the phase diagram for the two-layer ReLU NN at the infinite-width limit.

- We experimentally demonstrate and theoretically prove the transition across the boundary (critical regime) in the phase diagram.

- Then, we visualize the phase diagram by experimentally scanning S w over the phase space.




# [As You Like It: Localization via Paired Comparisons](https://jmlr.org/papers/volume22/18-105/18-105.pdf)
- 2 )Ô∏Ä = Œò(‚Ñì 2 ) possible paired comparisons

- Here we aim to understand how the paired comparisons change with the introduction of "pre-quantization" Gaussian noise.

- The results of this section apply without considering any particular recovery method or algorithm and relate to the number of paired comparisons which may be flipped due to the noise, not from reconstruction.

- Here we show that given enough comparisons there is an approximate embedding of the preference space into {‚àí1, 1} ùëö via our model.




# [Nonparametric Modeling of Higher-Order Interactions via Hypergraphons](https://jmlr.org/papers/volume22/19-941/19-941.pdf)
- In order to facilitate efficient estimation and computation, we restrict ourself to a class of Simple Lipschitz Hypergraphons (SLH).

- We provided rates of convergence in expectation for estimating a class of Smooth Lipschitz Hypergraphons (SLH) and provided practical algorithms for implementing the estimators.

- As mentioned above, our estimator is based on a stochastic block model approximation to the class of SLH.

- For this class of hypergraphons, we propose an estimator along with its rates of convergence.




# [Explaining by Removing: A Unified Framework for Model Explanation](https://jmlr.org/papers/volume22/20-1316/20-1316.pdf)
- Our contributions include: 1. We present a unified framework that characterizes 26 existing explanation methods and formalizes a new class of removal-based explanations.

- 3. How the method summarizes each feature's influence.

- The third choice for removal-based explanations is how to summarize each feature's influence on the model.

- Removal-based explanations perform a specific type of counterfactual reasoning.




# [Global and Quadratic Convergence of Newton Hard-Thresholding Pursuit](https://jmlr.org/papers/volume22/19-026/19-026.pdf)
- In this way, we establish the global convergence to the Œ∑-stationarity.

- We will establish its global convergence to an Œ∑ stationary point under the restricted strong smoothness of f .

- Suppose NHTP: Newton Hard-Thresholding Pursuit Step 0 Initialize x 0 .

- In this main section, we present our Newton Hard-Thresholding Pursuit (NHTP) algorithm, which largely follows the general framework (3), but with distinctive features.




# [Aggregated Hold-Out](https://jmlr.org/papers/volume22/19-624/19-624.pdf)
- This enables us to prove an oracle inequality for the hold-out and agghoo in regularized kernel regression with a general Lipschitz loss (Theorem 11).

- This section recalls the definition of cross-validation for estimator selection, and introduces a new procedure called aggregated hold-out (agghoo).

- The first theorem is a general oracle inequality for the hold-out.

- By a convexity argument, agghoo always improves on the hold-out, provided that the risk is convex.




# [Sparse and Smooth Signal Estimation: Convexification of 0 -Formulations](https://jmlr.org/papers/volume22/18-745/18-745.pdf)
- In this paper, we derived strong iterative convex relaxations for quadratic optimization problems with M-matrices and indicators, of which signal estimation with smoothness and sparsity is a special case.

- We then give a tailored Lagrangian decomposition method, which is amenable to parallel computing and highly scalable.

- Additionally, we show how to implement the relaxations derived in the paper in a conic quadratic optimization framework.

- The proposed iterative convexification approach substantially closes the gap between the 0 -"norm" and its 1 surrogate and results in significantly better estimators than the standard approaches using 1 surrogates.




# [Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions](https://jmlr.org/papers/volume22/19-716/19-716.pdf)
- We develop both capacity-independent and capacity-dependent learning rates with high probability.

- In this paper, we investigate both capacity-independent and capacity-dependent learning rates for multi-pass SGD with general convex loss functions.

- We now develop capacity-dependent learning rates.

- Optimal capacity-dependent learning rates O(n for the case Œ± + Œ∂/2 < 1.




# [Flexible Signal Denoising via Flexible Empirical Bayes Shrinkage](https://jmlr.org/papers/volume22/19-042/19-042.pdf)
- Our methods are implemented in the R package smashr ("SMoothing by Adaptive SHrinkage in R"), available on GitHub (https://www. github.com/stephenslab/smashr).

- These are implemented in the R package wavethresh (Nason, 2016), for example.

- We also compared against the empirical Bayes shrinkage procedure, "EbayesThresh" (Johnstone and Silverman, 2005a).

- These wavelet coefficients Œºj will be estimated using empirical Bayes shrinkage (14).




# [Particle-Gibbs Sampling for Bayesian Feature Allocation Models](https://jmlr.org/papers/volume22/20-082/20-082.pdf)
- Our PG sampling approach has computational complexity that scales linearly with the number of features.

- We overcome this limitation by using the PG methodology to develop an algorithm which scales linearly in the number of features.

- In contrast to standard SMC, the Particle Gibbs framework is less sensitive to the number of particles.

- The first parameter we explore is the number of particles.




# [Optimal Bounds between f-Divergences and Integral Probability Metrics *](https://jmlr.org/papers/volume22/20-867/20-867.pdf)
- we define a generalization of the log moment-generating function and show that it exactly characterizes the best lower bound of a œï-divergence by a given IPM.

- Throughout this paper, the œï-cumulant generating function has proved central in explicitating the relationship between œï-divergences and integral probability metrics.

- For example, the best lower bound of the Kullback-Leibler divergence by a quadratic function of the total variation distance is known as Pinsker's inequality.

- This establishes a "correspondence principle" by which properties of the relationship between œï-divergences and integral probability metrics translate by duality into properties of the cumulant generating function, and vice versa.




# [Replica Exchange for Non-Convex Optimization](https://jmlr.org/papers/volume22/20-697/20-697.pdf)
- After exchange, GD can converge to the global minimum, and LD keeps exploring.

- We then establish convergence to the global minimum conditional on being in the properly defined small set.

- When Œ≥ = 1 (Figure 6 (a)), we do not see convergence to the global minimum at all.

- Langevin dynamics (SGLD).




# [Finite Time LTI System Identification](https://jmlr.org/papers/volume22/19-725/19-725.pdf)
- We show that it is always possible to learn the parameters of a lower-order approximation of the underlying system.

- In this paper we provide a purely data-driven approach to system identification from a single time-series of finite noisy data.

- This leads to the question of model order selection from data.

- We propose a new approach to system identification when we observe only finite noisy data.




# [Predictive Learning on Hidden Tree-Structured Ising Models](https://jmlr.org/papers/volume22/19-149/19-149.pdf)
- In particular, we derived sample complexity guarantees for exact structure learning and marginal distributions estimation.

- Our main results Theorem 5 and Theorem 7 provide the amount of finite samples needed for exact structure recovery and accurate predictive learning with high probability.

- ‚Ä¢ Determination of the sufficient and necessary number of samples for accurate predictive learning.

- ‚Ä¢ A closed-form expression and a computationally efficient estimator for higher-order moment estimation in tree-structured Ising models.




# [Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent](https://jmlr.org/papers/volume22/20-1067/20-1067.pdf)
- We consider four low-rank matrix estimation tasks: ‚Ä¢ Low-rank matrix sensing.

- Section 4 illustrates the excellent empirical performance of ScaledGD in a variety of low-rank matrix estimation problems.

- , for any low-rank matrix

- ‚Ä¢ Other low-rank recovery problems.




# [Adaptive Estimation of Nonparametric Functionals](https://jmlr.org/papers/volume22/19-892/19-892.pdf)
- As mentioned above, such dependence can even further relaxed by considering estimators based on even higher-order U-statistics.

- In particular, we summarize our results on upper and lower bounds on the adaptive minimax estimation rate of œÜ(P ) in the following theorem.

- First we discuss a general recipe for producing a data-adaptive "best" estimator from a sequence of estimators based on second-order U-statistics -which in turn are constructed from compactly supported wavelet based projection kernels (defined in Section 5).

- However, since our estimators are based on second-order U-statistics, we can only perform a second-order bias correction (see e.g. Robins et al. (2008Robins et al. ( , 2017) and this results in the requirement of sufficient large smoothness of the marginal density of the covariates.




# [Learning a High-dimensional Linear Structural Equation Model via 1 -Regularized Regression](https://jmlr.org/papers/volume22/20-1005/20-1005.pdf)
- In Section 3, we introduce the new algorithm for high-dimensional linear SEM learning with sub-Gaussian and bounded-moment error distributions.

- This paper provides a statistically consistent and computational feasible algorithm for learning high-dimensional linear SEMs via 1 -regularized regression.

- This section introduces a new regression-based algorithm for high-dimensional linear SEMs.

- This section provides the statistical guarantees on Algorithm 1 for learning high-dimensional linear SEMs (2).




# [Consistent Semi-Supervised Graph Regularization for High Dimensional Data](https://jmlr.org/papers/volume22/19-081/19-081.pdf)
- This result is fundamentally responsible for the failure of semi-supervised Laplacian regularization on large dimensional data.

- These semi-supervised learning algorithms of Laplacian regularization are presented in Section 2.1.

- The focus of this article is to promote the usage of centered similarities in graph regularization for semi-supervised learning.

- The theoretical results also point out that the proposed method has an unlabelled data learning efficiency that is at least as good as spectral clustering, as opposed to Laplacian regularization.




# [Estimation and Inference for High Dimensional Generalized Linear Models: A Splitting and Smoothing Approach](https://jmlr.org/papers/volume22/19-132/19-132.pdf)
- We propose a new approach for drawing inference with high dimensional GLMs.

- Unlike many of the existing methods (Zhang and Zhang, 2014;Javanmard and Montanari, 2018), our method is more computationally feasible as it does not require estimating high dimensional precision matrices.

- Indeed, the de-biased methods may be sensitive to tuning parameters, which could explain the gap in the finite sample performance.

- Section 6 provides simulations and comparisons with the existing methods.




# [Convergence Guarantees for Gaussian Process Means With Misspecified Likelihoods and Smoothness](https://jmlr.org/papers/volume22/20-662/20-662.pdf)
- In each setting, our results demonstrate the impact of the choice of hyperparameters and the experimental design.

- In particular, the impact of the experimental design and covariance function is made clear in the bounds.

- ‚Ä¢ Experimental design: The terms h Xn and œÅ Xn quantify the impact of the experimental design.

- We now study the impact of model choice.




# [POT: Python Optimal Transport](https://jmlr.org/papers/volume22/20-451/20-451.pdf)
- The Python Optimal Transport (POT) library takes advantage of Python to make Optimal Transport accessible to the machine learning community.

- We have presented POT, an optimal transport toolbox written in Python.

- Distributed under the MIT license, this open source toolbox is well-documented and community-driven.

- Kantorovich optimal transport problems.




# [Optimal Structured Principal Subspace Estimation: Metric Entropy and Minimax Rates](https://jmlr.org/papers/volume22/20-610/20-610.pdf)
- However, the minimax rates of convergence for subspace constrained PCA/SVD remain unknown.

- Minimax optimal rates of convergence for a collection of structured PCA/SVD problems are established.

- The results yield interesting phase transition phenomena concerning the rates of convergence as functions of the SNRs and the fundamental limit for consistent estimation.

- 4. Spectral Clustering.




# [A General Framework for Adversarial Label Learning](https://jmlr.org/papers/volume22/20-537/20-537.pdf)
- This paper introduces adversarial label learning (ALL), a method for training classifiers without labels by making use of weak supervision.

- We test adversarial label learning on a variety of datasets, comparing it with other approaches for weak supervision.

- We introduced adversarial label learning (ALL), a method to train robust classifiers when access to labeled training data is limited.

- A key feature of ALL is that it is robust to redundant and dependent errors in the weak supervision.




# [Matrix Product States for Inference in Discrete Probabilistic Models](https://jmlr.org/papers/volume22/18-431/18-431.pdf)
- The matrix product state (MPS) literature contains a number of results not present in the tensor train literature, which motivates its introduction here.

- The rank of the matrix product state is the most significant tuning parameter, both in terms of modeling capacity and computational complexity.

- The rank of the tensor train is a fundamental hyperparameter which will effectively correspond to the expressiveness of the approximation.

- As noted previously, we are not the first to consider the relationship between matrix product states and probabilistic models.




# [Pseudo-Marginal Hamiltonian Monte Carlo](https://jmlr.org/papers/volume22/19-486/19-486.pdf)
- (15 ) The pseudo-marginal MH algorithm can mix very poorly if the relative variance of the likelihood estimator is large; e.g. if N = 1 in (13).

- The three plots correspond to the pseudo-marginal HMC sampler, the pseudo-marginal slice sampler, and the Gibbs sampler with conditional importance sampling kernels, respectively.

- The resulting PM-HMC algorithm replaces the original intractable gradient of the log-likelihood by the gradient of the log-likelihood estimator.

- Results for the remaining methods/settings, including the pseudo-marginal MH method and the Particle Gibbs sampler which both performed very poorly, are given in the appendix.




# [Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation](https://jmlr.org/papers/volume22/19-944/19-944.pdf)
- ‚Ä¢ A factor analysis model for smooth graph signals with sparse spectral representation is introduced (Section 5).

- Interpretation of the terms.

- This model provides a probabilistic interpretation of our optimization program and links its objective function to a maximum a posteriori estimation.

- The goal of this model is to provide a probabilistic interpretation of Problem (3) and link its objective function to a maximum a posteriori estimation (Proposition 18).




# [Kernel Operations on the GPU, with Autodiff, without Memory Overflows](https://jmlr.org/papers/volume22/20-275/20-275.pdf)
- As showcased on our website and at the end of this paper, KeOps scripts for kernel and geometric applications generally outperform their Numpy and PyTorch counterparts by several orders of magnitude while keeping a linear memory footprint.

- The KeOps library intends to provide such a solid numerical foundation for all methods that involve large distance or kernel matrices.

- These high-level Python frameworks unlock the use of massively parallel hardware for machine learning research.

- The workhorse of the KeOps library is a C++ engine for generic reductions on sampled data.




# [An Importance Weighted Feature Selection Stability Measure](https://jmlr.org/papers/volume22/20-366/20-366.pdf)
- The choice of the stability measure also influences the predictive performance of the chosen compromise.

- We show here that decision-making is heavily influenced by the choice of the stability measure.

- Each type of feature selection requires dedicated stability measures.

- While most measures proposed in the literature study the stability of feature subsets, we incorporate into the stability value the selected features importance in predictive models.




# [LassoNet: A Neural Network with Feature Sparsity](https://jmlr.org/papers/volume22/20-848/20-848.pdf)
- We test LassoNet on a variety of datasets, and find that it generally outperforms state-of-the-art methods for feature selection and regression.

- Second, implementing the architecture in popular machine learning frameworks requires only modifying a few lines of code from a standard feed-forward neural network.

- We now describe the problem of global feature selection.

- The method can be implemented by adding just a few lines of code to a standard neural network.




# [Sparse Tensor Additive Regression](https://jmlr.org/papers/volume22/19-769/19-769.pdf)
- Next, we derive a non-asymptotic error bound for the estimator from each iteration, which demonstrates the improvement of the estimation error in each update.

- Section 2 introduces our sparse tensor additive regression model.

- Section 3 develops an efficient penalized alternating minimization algorithm for model estimation.

- In this section, we apply the STAR model to click-through rate (CTR) prediction in online advertising.




# [Asymptotic Normality, Concentration, and Coverage of Generalized Posteriors](https://jmlr.org/papers/volume22/20-469/20-469.pdf)
- Here, we provide sufficient conditions for œÄ n to exhibit concentration, asymptotic normality, and an asymptotically correct Laplace approximation.

- In general, we make no assumption of model correctness.

- In this section, we provide sufficient conditions for concentration, asymptotic normality, and the Laplace approximation for a large class of pseudolikelihood-based posteriors.

- In this article, we provide new theoretical results on the asymptotic validity of generalized posteriors.




# [Statistical guarantees for local graph clustering](https://jmlr.org/papers/volume22/20-029/20-029.pdf)
- First, we give a guarantee on the recovery of the target cluster for the optimal solution (12).

- We compare the performance of 1 -regularized PageRank with state-of-the-art local graph clustering algorithms.

- Importantly, when combined with Theorem 6 (full recovery of the cluster), our result Theorem 8 immediately establishes that 1regularized PageRank recovers the target cluster exactly, even when the target cluster is constantsized.

- More precisely, we show that for large and medium values of Œ≥ there exists a solution in the solution path of the 1 -regularized PageRank, which is found by the stagewise algorithm, and it recovers the target cluster.




# [Convex Clustering: Model, Theoretical Guarantee and Efficient Algorithm](https://jmlr.org/papers/volume22/18-694/18-694.pdf)
- 1. We prove the perfect recovery guarantee of the general weighted convex clustering model (2) under mild sufficient conditions.

- In this paper, we established the theoretical recovery guarantee for the general weighted convex clustering model, which includes many popular setting as special cases.

- However, to the best of our knowledge, no theoretical recovery guarantee has been established for the general weighted convex clustering model (2).

- The above theorem has established the theoretical recovery guarantee for the general weighted convex clustering model (2).




# [NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation](https://jmlr.org/papers/volume22/18-803/18-803.pdf)
- Many feature maps can impede the universal approximation property (UAP) of F .

- NEU does this by training a new deep neural model type, called the reconfiguration network and denoted by Œ¶ :d , whose members form a universal class of regular feature maps.

- This paper introduces Non-Euclidean Upgrading (NEU), a meta-algorithm that incorporates a linearizing preprocessing step into (L) as summarized in Meta-Algorithm 1.

- We see that the target function is reflected by each of the feature maps learned by NEU.




# [LocalGAN: Modeling Local Distributions for Adversarial Response Generation](https://jmlr.org/papers/volume22/20-052/20-052.pdf)
- This paper aims at presenting a specific adversarial training schema for neural response generation.

- According to this approximated distribution representation, a new loss function describing the local expansion cost in the fitting of response distribution is presented and finally combined with the traditional GAN loss to form a hybrid training objective for the GAN based NRG model.

- Despite the improvement on the diversity, the adversarial training process of GAN based response generation models is generally unstable and sensitive to the training strategy .

- This observation indicates that modeling the local distribution of response clusters is potentially valuable.




# [GemBag: Group Estimation of Multiple Bayesian Graphical Models](https://jmlr.org/papers/volume22/19-1006/19-1006.pdf)
- In contrast, the theoretical properties we establish for our prior specification help us show that GemBag procedure enjoys optimal theoretical properties in terms of 8 norm estimation accuracy and correct recovery of the graphical structure (see Section 3).

- Our theoretical results show that the MAP estimators have an optimal rate of convergence in 8 norm under a general setting where the graphical models may have different sparsity structures and signal strength.

- Moreover, implementations of existing Bayesian methods for multiple graphical models have severe computational limitations.

- The last two designs are more challenging, since both the sparsity structures and signal strength can be different within a group.




# [A Unified Framework for Spectral Clustering in Sparse Graphs](https://jmlr.org/papers/volume22/20-261/20-261.pdf)
- This article provides a critical analysis of the current state-of-the-art spectral algorithms for community detection in sparse graphs.

- Besides, as a corollary, this provides a convenient alternative method to estimate the number of communities, based on the regularized Laplacian matrix.

- Several contributions have thus tackled the challenging problem of devising efficient spectral clustering in the sparse regime and for an arbitrary degree distribution.

- Let us now discuss how Claim 1 can be exploited in practice to obtain an efficient spectral clustering algorithm for sparse graphs with a heterogeneous degree distribution.




# [When random initializations help: a study of variational inference for community detection](https://jmlr.org/papers/volume22/19-630/19-630.pdf)
- When the parameters are known, we show conditions under which random initializations can converge to the ground truth.

- In this paper, we work with the BCAVI mean field variational algorithm for a simple two class stochastic blockmodel with equal sized classes.

- First, we show that, when the model parameters are known, random initializations centered around half converge to the ground truth a good fraction of time.

- In contrast, when the parameters are not known and estimated iteratively with the mean field parameters, we show that a random initialization converges, with high probability, to a meaningless local optimum.




# [Homogeneity Structure Learning in Large-scale Panel Data with Heavy-tailed Errors](https://jmlr.org/papers/volume22/19-1018/19-1018.pdf)
- We propose a data-driven procedure to robustly estimate the interactive effects model and learn the homogeneity structure.

- We model large-scale panel data with an interactive effects model where both covariates and errors are influenced by some latent factors.

- This paper studies large-scale panel data and addresses some challenges that arise in modern applications.

- In Section 5, we assess the finite sample performance of the proposed learning procedure with simulated experiments.




# [Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory Soon Hoe Lim](https://jmlr.org/papers/volume22/20-620/20-620.pdf)
- (2) Building on our understanding in (1), we identify a universal feature, called the response feature, potentially useful for learning temporal series.

- In particular, we derive two series representations for the output functional of SRNNs and their deep version.

- In particular, we have shown, via a representer theorem, that SRNNs can be viewed as kernel machines operating on a reproducing kernel Hilbert space associated with the response feature.

- This feature turns out to be the signature of tensor product of the input signal and a vector whose components are orthogonal polynomials (see Theorem 4.3).




# [Stochastic Proximal Methods for Non-Smooth Non-Convex Constrained Sparse Optimization *](https://jmlr.org/papers/volume22/20-287/20-287.pdf)
- To the best of our knowledge, we have presented the first non-asymptotic convergence bounds for this class of objective function.

- We proposed a new measure of convergence, the subdifferential mapping, and presented two stochastic proximal gradient algorithms.

- Non-asymptotic convergence bounds were first achieved in (Ghadimi et al., 2016).

- We are not aware of any other works proving non-asymptotic convergence bounds for our general problem setting.




# [TensorHive: Management of Exclusive GPU Access for Distributed Machine Learning Workloads Pawe l Ro≈õciszewski](https://jmlr.org/papers/volume22/20-225/20-225.pdf)
- TensorHive is an open-source tool that facilitates efficient coordination of exclusive access to computing resources for machine learning workloads by introducing a hybrid mechanism of reservations and job queuing.

- In the face of the spectacular improvements in many practical applications introduced by deep learning (LeCun et al., 2015), both research and engineering teams all over the world are equipped with computing accelerators such as GPUs for neural network training.

- In this paper we propose TensorHive, an open-source GPU management tool that combines exclusive GPU reservations with job execution and monitoring, while focusing on user-friendliness, simple configuration and support for frameworks and scenarios typical for high performance machine learning.

- Defining training tasks for chosen frameworks is vastly simplified by templates that automatically configure cluster definition arguments, including TF CONFIG and standard command-line parameters for TensorFlow and PyTorch.




# [Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)](https://jmlr.org/papers/volume22/20-303/20-303.pdf)
- The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process.

- In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research.

- The third component of the reproducibility program involved use of the Machine Learning reproducibility checklist (see Appendix, Figure 8).

- It seems a code submission policy based on volun-tary participation is sufficient at this time.




# [Variance Reduced Median-of-Means Estimator for Byzantine-Robust Distributed Inference](https://jmlr.org/papers/volume22/20-950/20-950.pdf)
- The MOM estimator is computationally efficient and robust against Byzantine failures.

- To the best of our knowledge, this is the first asymptotic normality result in the setting of Byzantine-robust distributed learning.

- The goal of this paper is to propose a communication-efficient statistical inference method, which is robustly against Byzantine failures.

- In this paper, we design a Byzantine tolerant algorithm to address a general class of estimation and inference problems in a distributed setting.




# [Analyzing the discrepancy principle for kernelized spectral filter learning algorithms](https://jmlr.org/papers/volume22/20-358/20-358.pdf)
- In this section, we analyze the discrepancy principle conditional on the design.

- We now turn to a modification of the discrepancy principle based on the smoothing of the residuals that is, on the smoothed empirical risk.

- Next, we establish a concentration inequality for the empirical effective dimension.

- From a practical perspective, our main contribution is the description of data-driven early stopping rules based on the discrepancy principle (Phillips, 1962).




# [On ADMM in Deep Learning: Convergence and Saturation-Avoidance](https://jmlr.org/papers/volume22/20-1006/20-1006.pdf)
- ‚Ä¢ Methodology Novelty: We develop a novel sigmoid-ADMM pair for deep learning.

- Specifically, the performance of the proposed ADMM is slightly better than SGD type methods.

- Theorem 4 establishes the global convergence of ADMM to a KKT point at a rate of O(1/K).

- According to Table 8, the performance of the proposed ADMM is comparable to the state-of-the-art methods in terms of test accuracy.




# [Tighter Risk Certificates for Neural Networks](https://jmlr.org/papers/volume22/20-879/20-879.pdf)
- We elaborate more on the use of PAC-Bayes bounds for model selection in the next subsection.

- In this paper we experiment with probabilistic neural networks from a PAC-Bayes approach.

- We show now that PAC-Bayes bounds can be used not only as training objectives to guide the optimisation algorithm but also for model selection.

- 5. We demonstrate via experimental results that PBB methods might be able to achieve self-certified learning with nontrivial certificates: obtaining competitive test set errors and computing non-vacuous bounds with much tighter values than previous works.




# [Hyperparameter Optimization via Sequential Uniform Designs](https://jmlr.org/papers/volume22/20-058/20-058.pdf)
- In Figure 1, a sequential uniform design (SeqUD) approach is proposed for such HPO-type of computer experiment.

- In summary, this paper contributes to the HPO and AutoML literature in the following three aspects: ‚Ä¢ We develop a novel AugUD algorithm for the efficient augmentation of uniform design points.

- ‚Ä¢ The SeqUD strategy generalizes Bayesian optimization from one-point-at-a-time to batch-by-batch.

- A real-time AugUD algorithm is introduced for fast construction of augmented uniform designs.




# [Learning and Planning for Time-Varying MDPs Using Maximum Likelihood Estimation](https://jmlr.org/papers/volume22/20-006/20-006.pdf)
- ‚Ä¢ Section 5 uses the proposed notion of uncertainty to propose optimal learning and control policies for an agent operating in an unknown, time-varying environment.

- -Theorem 10 relates the proposed measure to measures of uncertainty used in learning and planning techniques for time-invariant MDPs.

- Learning and planning for agents operating in the last two frameworks have been discussed at length (Sutton et al., 1999;Szita et al., 2002).

- The CCMLE problem for general online learning and planning, with all the questions opened in this paper, thus remains largely open.




# [giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration](https://jmlr.org/papers/volume22/20-325/20-325.pdf)
- This makes giotto-tda the most comprehensive Python library for topological machine learning and data exploration to date.

- Topological data analysis (TDA) uses tools from algebraic and combinatorial topology to extract features that capture the shape of data (Carlsson, 2009).

- Facilitating these processes is one of the reasons why giotto-tda maintains and extends compatibility with the scikit-learn API.

- The library appears in scikit-learn's curated list of related projects.




# [On Multi-Armed Bandit Designs for Dose-Finding Clinical Trials](https://jmlr.org/papers/volume22/19-228/19-228.pdf)
- Hence finite-time upper bounds on the number of sub-optimal selection lead to non-asymptotic upper bound on the error probability of the design.

- In Section 3, we propose an analysis of Thompson Sampling with independent Beta priors on the toxicity of each dose: We provide finite-time upper-bounds on the number of sub-optimal selections, which match an (asymptotic) lower bound on those quantities.

- Our second contribution is to show that Thompson Sampling using more sophisticated prior distributions can compete with state-of-the art dose-finding algorithms.

- In this paper, we investigate the use of Thompson Sampling (Thompson, 1933) for dose-finding clinical trials.




# [L-SVRG and L-Katyusha with Arbitrary Sampling](https://jmlr.org/papers/volume22/20-156/20-156.pdf)
- In this section, we develop loopless SVRG and loopless Katyusha for the composite problem (1).

- We establish the connection between these expected smoothness parameters and ESO , which allows us the explore the sparsity of data as well.

- We presented loopless SVRG and loopless Katyusha for finite-sum optimization problems under the arbitrary sampling regime.

- In this subsection, we give the estimations of these expected smoothness parameters under the ESO inequality.




# [Asynchronous Online Testing of Multiple Hypotheses](https://jmlr.org/papers/volume22/19-910/19-910.pdf)
- We have presented a unified framework for the design and analysis of online FDR procedures for asynchronous testing, as well as testing locally dependent p-values.

- This immediately gives two procedures for asynchronous online testing as special cases of LORD* and SAFFRON*. From here forward we will refer to these methods as LORD async and SAFFRON async , respectively.

- In this work, we reinforce this connection between asynchronous online testing and dependence by developing a general abstract framework in which, from an algorithmic point of view, these two issues are treated Figure 1: Testing five hypotheses synchronously (top) and asynchronously (bottom).

- We derive two specific procedures that make use of conflict sets to yield algorithms that provide online mFDR and FDR control.




# [Stochastic Online Optimization using Kalman Recursion](https://jmlr.org/papers/volume22/20-618/20-618.pdf)
- We derive local bounds on the cumulative risk with high probability from a martingale analysis.

- S(Œ¥) is the cumulative excess risk of the convergence phase.

- Note that the dependence of the cumulative excess risk of the convergence phase in terms of Œ¥ is O(log(Œ¥ ‚àí1 ) 3 ).

- In Section 3.2 we compare the excess risk with its second-order expansion thanks to Assumption 5, and we use a martingale analysis to obtain a bound on the cumulative excess risk.




# [Consistency of Gaussian Process Regression in Metric Spaces](https://jmlr.org/papers/volume22/21-0853/21-0853.pdf)
- More precisely, we provide a proof for the consistency of GP regression of an unknown, real-valued function f 0 , whose domain is a separable metric space (T, œÅ).

- Gaussian Process (GP) regression (Rasmussen and Williams, 2006, Chapter 2) is an established tool for nonparametric modelling of real-world phenomena.

- In this paper, we made a crucial first step towards the theoretical legitimization of using GP regression on non-Euclidean manifolds and other metric domain spaces, as has already been done by various authors.

- Finally, this work aimed to establish asymptotic consistency of GP regression under as weak as possible assumptions on the observed function and the GP prior, allowing us to provide results for a large class of domains and kernels.




# [Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction](https://jmlr.org/papers/volume22/20-753/20-753.pdf)
- We refer to the procedure (3) as the Marginal conformal prediction method.

- (2) We propose two approaches to guarantee the containments (2).

- Under appropriate assumptions typical in proving the consistency of prediction methods, Conformalized Quantile Classification (CQC) guarantees conditional coverage asymptotically.

- Our main motivation is to design methods with more robust conditional coverage than the "marginal" split-conformal method (3).




# [DIG: A Turnkey Library for Diving into Graph Deep Learning Research](https://jmlr.org/papers/volume22/21-0343/21-0343.pdf)
- For each direction, DIG provides unified and extensible implementations of data interfaces, common algorithms, and evaluation metrics.

- Deep learning on 3D graphs.

- These are graph generation, selfsupervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs.

- In this paper, we present DIG: Dive into Graphs that contains unified and extensible implementations of data interfaces, common algorithms, and evaluation metrics for several significant research directions, including graph generation, self-supervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs.




# [DeEPCA: Decentralized Exact PCA with Linear Convergence Rate](https://jmlr.org/papers/volume22/21-0298/21-0298.pdf)
- (15 ) Thus, DeEPCA achieves the best communication complexity of decentralized PCA algorithms.

- We summarize our contributions as follows: 1. We propose a novel power-iteration based decentralized PCA called DeEPCA, which can achieve the best known communication complexity.

- In this way, DeEPCA can achieve the best known communication complexity for decentralized PCA.

- The subspace tracking technique in our algorithm is the key to achieving the advantages of DeEPCA.




# [Differentially Private Regression and Classification with Sparse Gaussian Processes](https://jmlr.org/papers/volume22/19-017/19-017.pdf)
- We will first introduce Gaussian processes and differential privacy.

- In summary, we have presented three tools to extend the use of differential privacy for Gaussian process models.

- In the current paper we suggest solutions to some of that method's shortcomings, specifically we propose non-stationary covariance functions to reduce the impact of outliers, and develop a method for using the Laplace approximation to perform differentially private classification.

- Finally we suggest a method for hyperparameter optimisation based on a straightforward grid search combined with the differentially private exponential mechanism.




# [Pykg2vec: A Python Library for Knowledge Graph Embedding](https://jmlr.org/papers/volume22/19-433/19-433.pdf)
- Pykg2vec is a Python library with extensive documentation that includes the implementations of a variety of state-of-the-art Knowledge Graph Embedding methods and modular building blocks of the embedding pipeline.

- (a) Provide access to the latest and state-of-the-art KGE implementations.

- (c) Deliver a modular and flexible software architecture and KGE pipeline that is both educational and of practical use for researchers.

- In recent years, Knowledge Graph Embedding (KGE) has become an active research area and many authors have provided reference software implementations.




# [Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives](https://jmlr.org/papers/volume22/19-1049/19-1049.pdf)
- We also established new estimation error bounds for a class of 0 -regularized classification problems and showed that these bounds compare favorably with the best-known bounds for 1 regularization.

- Our approximate algorithms are based on coordinate descent and local combinatorial search.

- Our Contributions: We summarize our contributions below: ‚Ä¢ We develop fast first-order algorithms based on cyclic CD and local combinatorial search to (approximately) solve Problem (2) (see Section 2).

- To this end, we propose a new MIP-based algorithm that we call "integrality generation", which allows for solving instances of Problem (2) with p ‚âà 50, 000 (where n is small) to optimality within a few minutes.




# [A General Framework for Empirical Bayes Estimation in Discrete Linear Exponential Family](https://jmlr.org/papers/volume22/19-873/19-873.pdf)
- In this paper we propose a Nonparametric Empirical Bayes framework for compound estimation in the discrete linear exponential family.

- This article develops a general non-parametric empirical Bayes (NEB) framework for compound estimation in discrete models.

- This section describes the proposed NEB framework for compound estimation in discrete models.

- This article develops a general framework for empirical Bayes estimation for the discrete linear exponential (DLE) family, also known as the family of discrete power series distributions (Noack, 1950), under both regular and scaled squared error losses.




# [On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests](https://jmlr.org/papers/volume22/17-570/17-570.pdf)
- In this paper, we investigated the performance of kernel embedding based approaches to goodness-of-fit testing from a minimax perspective.

- A particularly attractive approach to goodness-of-fit testing problems in general domains is through RKHS embedding of distributions.

- In particular, we focus on kernel embedding based goodness-of-fit tests and investigate their power under a general composite alternative.

- Throughtout numerical experiments in our paper, we have considered truncated version of the moderated kernel.




# [Interpretable Deep Generative Recommendation Models](https://jmlr.org/papers/volume22/20-1098/20-1098.pdf)
- InDGRM preference similarity and intra-user preference diversity has ability to significantly improve model performance.

- In this section, an Interpretable Deep Generative Recommendation Model (InDGRM) will be presented by investigating user behaviors from the views of inter-user preference similarity and intra-user preference diversity.

- Group structure among items can effectively reflect the intra-user preference diversity.

- In this paper, we propose an interpretable deep generative method (InDGRM) for recommendation by modeling both inter-user preference similarity and intra-user preference diversity to achieve disentanglement from both observed-level and latent-level.




# [Bayesian Text Classification and Summarization via A Class-Specified Topic Model](https://jmlr.org/papers/volume22/18-332/18-332.pdf)
- In this paper, we propose the class-specified topic model (CSTM) as another extension of LDA.

- A two-stage approach can also be used to obtain class-specific text summarization.

- Section 4 develops Bayesian inference of CSTM in the semisupervised scenario, with the supervised scenario as a special case.

- Topics extracted under each class can be used to obtain class-specific text summarization.




# [The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks](https://jmlr.org/papers/volume22/20-1300/20-1300.pdf)
- ‚Ä¢ Finite Sample-Size Error Bounds: Approximation error bounds are obtained which are valid for a finite-dimensional parametrisation of the network, as opposed to relying on asymptotic results such as a central limit theorem.

- A proof-of-concept empirical assessment is contained Section 6.

- In this section we derive a finite-sample-size error bound for the finite-bandwidth ridgelet transform of Definition 2.

- Consider the neural network in (1).




# [Domain Generalization by Marginal Transfer Learning](https://jmlr.org/papers/volume22/17-679/17-679.pdf)
- We propose two data generation models.

- In this section we formally define domain generalization via two possible data generation models together with associated notions of risk.

- We also provide a basic generalization error bound for the first of these data generation models.

- 1 To our knowledge, the problem of domain generalization was first proposed and studied by our earlier conference publication (Blanchard et al., 2011) which this work extends in several ways.




# [A Unified Convergence Analysis for Shuffling-Type Gradient Methods](https://jmlr.org/papers/volume22/20-1238/20-1238.pdf)
- We have conducted an intensive convergence analysis for a wide class of shuffling-type gradient methods for solving a finite-sum minimization problem.

- In this paper, we develop a new and unified convergence analysis framework for general shuffling-type gradient methods to solve (P) and apply it to different shuffling variants in both nonconvex and strongly convex settings under standard assumptions.

- ‚Ä¢ Finally, prior to our work, convergence analysis of shuffling-type gradient schemes has not been rigorously investigated for the nonconvex setting of (P).

- There exists no unified analysis that can cover a wide class of shuffling-type gradient algorithms under different assumptions ranging from strongly convex to nonconvex cases.




# [Collusion Detection and Ground Truth Inference in Crowdsourcing for Labeling Tasks](https://jmlr.org/papers/volume22/19-373/19-373.pdf)
- To address these issues, in the next subsection, we propose a penalized pairwise profile likelihood method for collusion detection.

- The next theorem verifies the consistency of the penalized pairwise profile likelihood method.

- Next, we consider the asymptotic properties of the penalized pairwise profile likelihood estimation method.

- To derive the pairwise profile likelihood




# [Histogram Transform Ensembles for Large-scale Regression](https://jmlr.org/papers/volume22/19-1004/19-1004.pdf)
- In this paper, we propose a randomized ensemble algorithm named histogram transform ensembles (HTE) for large-scale regression problems.

- By conducting a statistical learning treatment, this paper studies the large-scale regression problem with histogram transform estimators.

- Thus, we turn to apply the kernel histogram transform (KHT) which is verified to have almost optimal convergence rates.

- Secondly, almost optimal convergence rates are verified within the regularized empirical risk minimization framework for our histogram transform estimators in the sense of different space C k,Œ± .




# [Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning](https://jmlr.org/papers/volume22/20-410/20-410.pdf)
- ‚Ä¢ 5+1 Phases of Regularization.

- ‚Ä¢ Exhibiting the 5+1 Phases.

- These observations demonstrate that modern, state-of-the-art DNNs exhibit a new form of Heavy-Tailed Self-Regularization.

- Based on this, in Section 5, we present our main theory of 5+1 Phases of Training.




# [mlr3pipelines -Flexible Machine Learning Pipelines in R](https://jmlr.org/papers/volume22/21-0281/21-0281.pdf)
- But real-world applications often require complex combinations of ML (pre-) processing steps, which can be expressed as a directed acyclic graph (DAG); we will call such graphs ML pipelines or ML workflows.

- They support ensemble models and conditional branching that can be represented explicitly as part of the graph structure.

- Outputs from different nodes can be combined in non-trivial ways, for example, joining features created by different preprocessing steps, to create non-linear structures.

- mlr3pipelines and the mlr3 ecosystem are integrated with each other, so that mlr3's Learners can be used as PipeOps and Graphs adhere to the same interface as mlr3 learners and can be, for example, resampled and tuned just like any other Learner.




# [dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python](https://jmlr.org/papers/volume22/20-1473/20-1473.pdf)
- In Figure 1, we present the architecture of a unified interface for model-agnostic responsible machine learning with interactive explainability and fairness.

- Based on these experiences, we implemented a Python package.

- This contribution builds upon the software for explainable machine learning presented by us in "DALEX: Explainers for Complex Predictive Models in R" (Biecek, 2018).

- Overall, the responsible machine learning domain aims to address more principles than explainability and fairness (Barredo Arrieta et al., 2019); thus, the next steps shall address the accountability, robustness, and safety of machine learning models.




# [Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization](https://jmlr.org/papers/volume22/20-1061/20-1061.pdf)
- We now formally introduce the Pairwise Controlled Manifold Approximation Projection (PaCMAP) method.

- This new loss function is used in an algorithm called Pairwise Controlled Manifold Approximation Projection (PaCMAP) introduced in this work.

- First, one could consider the possibility of a continuum between local and global structure.

- Section 4.3 provides the principles of loss functions for local structure preservation.




# [Sparse Convex Optimization via Adaptively Regularized Hard Thresholding](https://jmlr.org/papers/volume22/20-661/20-661.pdf)
- In this work we present a new algorithm called Adaptively Regularized Hard Thresholding (ARHT), that closes the longstanding gap between the O s * Œ∫ f (x 0 )‚àíf (x * ) and O s * Œ∫ 2 bounds by getting a sparsity of O(s * Œ∫) and thus achieving the best of both worlds.

- The algorithms that we will consider are LASSO, Orthogonal Matching Pursuit (OMP), Orthogonal Matching Pursuit with Replacement (OMPR), Adaptively Regularized Hard Thresholding (ARHT) (Algorithm 6), and Exhaustive Local Search (Algorithm 4).

- Our algorithm is essentially a hard thresholding algorithm (and more specifically OMPR, also known as PHT(1)) with the crucial novelty that it is applied on an adaptively regularized objective function.

- For s > s * , it additionally gives a tradeoff between the sparsity and the RIP bound required for the algorithm.




# [Mixing Time of Metropolis-Hastings for Bayesian Community Detection](https://jmlr.org/papers/volume22/18-770/18-770.pdf)
- Then, we present the posterior strong consistency result.

- To the best of our knowledge, ( 22) is the first explicit upper bound on the mixing time of the Markov chain for Bayesian community detection.

- (14 ) We perform the analysis of mixing time for the Markov chain {Œì t } t‚â•0 .

- Hence, both results of posterior strong consistency and rapidly mixing are referring to the clustering space.




# [Individual Fairness in Hindsight](https://jmlr.org/papers/volume22/19-658/19-658.pdf)
- Formally, we design an algorithm that we call Cautious Fair Exploration (CaFE), which is individually fair in hindsight and attains sub-linear regret guarantees as compared to the optimal individually-fair benchmark in a wide range of settings.

- Individual Fairness in Hindsight

- Formally, we propose an online learning algorithm that we call Cautious Fair Exploration or CaFE in Algorithm 1.

- This ensures fairness-in-hindsight.




# [Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach](https://jmlr.org/papers/volume22/19-707/19-707.pdf)
- of the Safe Policy Iteration (SPI) methods

- 5. SSPI was called Multiple-parameter Safe Policy Improvement (MSPI) in Pirotta et al. (2013b).

- , we define the update rule of the policy improvement step as: œÄ = Œ±œÄ + (1 ‚àí Œ±)œÄ, where Œ± ‚àà [0, 1] is the scalar trade-off coefficient.

- Following the approach proposed in CPI, USPI iteratively updates the current policy using a safe policy improvement.




# [Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data](https://jmlr.org/papers/volume22/20-1194/20-1194.pdf)
- In this section, we present a detailed derivation of the mean shift algorithm with directional data.

- Section 5 considers the computational learning theory of the directional mean shift algorithm; we study the ascending and converging properties of the algorithm.

- 4. With regard to computational learning theory, we prove the ascending and converging properties of the directional mean shift algorithm (Theorems 8 and 11).

- In this section, we present our experimental results of the directional mean shift algorithm on both simulated and real-world data sets.




# [Learning Whenever Learning is Possible: Universal Learning under General Stochastic Processes](https://jmlr.org/papers/volume22/17-298/17-298.pdf)
- We begin the discussion of universally consistent online learning with the subject of concisely characterizing the family of processes SUOL.

- In contrast, we find that the family of processes admitting the existence of universally consistent online learning rules forms a strict superset of these other two families.

- As for the problem of concisely characterizing the family of processes that admit strong universal online learning, again the present work only makes partial progress.

- In particular, this implies that for unbounded losses, there exist optimistically universal (inductive/self-adaptive/online) learning rules, so that Theorem 51 immediately follows.




# [Generalization Properties of hyper-RKHS and its Applications](https://jmlr.org/papers/volume22/19-482/19-482.pdf)
- Hence, we characterize a kernel learning framework in this space for kernel learning and out-of-sample extensions.

- In this paper, we generalize two regularized regression problems in hyper-RKHS, illustrates its utility for kernel learning and out-of-sample extensions, and proves asymptotic convergence results for the introduced regression models in an approximation theory.

- In particular, we make the following contributions: Algorithmically, in Section 2, motivated by Ong et al. (2005), we consider regularized regression problems with squared loss and Œµ-insensitive loss (i.e., KRR and SVR) in hpyer-RKHS for kernel learning and out-of-sample extensions.

- In this paper, we have studied the generalization properties of regularized regression models in hyper-RKHS.




# [Entangled Kernels -Beyond Separability](https://jmlr.org/papers/volume22/19-665/19-665.pdf)
- We now define the two novel classes of operator-valued kernels.

- Some well-known classes of operator-valued kernels include separable and transformable kernels.

- Finally we compare the running times of learning with various classes of operator-valued kernels.

- In Section 3, we describe some known classes of operator-valued kernels and review previous work on learning separable operator-valued kernels.




# [Consistent estimation of small masses in feature sampling](https://jmlr.org/papers/volume22/18-534/18-534.pdf)
- That is, there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass P n,0 .

- In Section 3 we prove that, under the Bernoulli product model, there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass M n,0 .

- We now introduce a nonparametric estimator of the small mass M n,r in a quite natural way, i.e., by comparing expectations.

- As an extension of the main result of Mossel and Ohannessian (2019) to the feature sampling framework, we first show that there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass M n,0 .




# [A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning](https://jmlr.org/papers/volume22/21-0112/21-0112.pdf)
- This section introduces the necessary notation for both self-paced and reinforcement learning.

- We have presented an interpretation of self-paced learning as inducing a sampling distribution over tasks in a reinforcement learning setting when using the KL divergence w.r.t. a target distribution ¬µ(c) as a self-paced regularizer.

- Contribution We propose an interpretation of the self-paced learning algorithm from a probabilistic perspective, in which the weighting of training samples corresponds to a sampling distribution (Section 4).

- In this paper, we develop an interpretation of self-paced learning as the process of generating a sequence of distributions over samples.




# [An algorithmic view of 2 regularization and some path-following algorithms](https://jmlr.org/papers/volume22/19-477/19-477.pdf)
- Moreover, we proposed various new path-following algorithms to approximate the 2 -regularized solution path.

- We first establish an equivalence between 2 -regularized solution path for a convex loss function, and the solution of an ODE.

- In this article, we established a formal connection between 2 -regularized solution path and the solution of an ODE.

- Section 2 discusses the properties of the solution path, and provides a proof of the equivalence to the ODE solution.




# [Approximate Newton Methods](https://jmlr.org/papers/volume22/19-870/19-870.pdf)
- Accordingly, we propose a general result for analysis of both local and global convergence properties of second order methods.

- In Section 3 we present a unifying framework for local and global convergence analysis of second order methods.

- We summarize our contribution as follows: ‚Ä¢ We propose a unifying framework (Theorem 3 and Theorem 5) to analyze local and global convergence properties of second order methods including stochastic and deterministic versions.

- In this paper we have proposed a framework to analyze both local and global convergence properties of second order methods including stochastic and deterministic versions.




# [Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks *](https://jmlr.org/papers/volume22/20-1329/20-1329.pdf)
- We studied a security threat to reinforcement learning (RL) where an attacker poisons the environment, thereby forcing the agent into executing a target policy.

- We instantiate our attacks in both the offline and online settings with appropriate notions of attack cost.

- We propose a general optimization framework for environment poisoning; our theoretical analysis provides technical conditions which ensures the attacker's success and gives lower/upper bounds on the attack cost.

- (5) Cost of the attack.




# [Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks](https://jmlr.org/papers/volume22/21-0366/21-0366.pdf)
- Particularly common in deep learning

- Model sparsity is thus often trained with a pruning schedule.

- In biological brains, model sparsity is one important component.

- 5. Combined ephemeral and model sparsity Any sparse deep neural network should combine both ephemeral and model sparsity.




# [FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection](https://jmlr.org/papers/volume22/20-815/20-815.pdf)
- In this paper, we introduced the first industrial-strength federated learning platform FATE.

- At its core, FATE is built on a library of federated and privacy-preserving machine learning algorithms, called Federat-edML.

- FATE also supports cross-cloud deployment and management through FATE-cloud.

- FATE (Federated AI Technology Enabler) is the first production-oriented platform developed by Webank's AI Department.




# [Statistical Query Lower Bounds for Tensor PCA](https://jmlr.org/papers/volume22/20-837/20-837.pdf)
- Our analysis revealed that the optimal sample complexity in the SQ model depends on whether ET is symmetric or not.

- In this paper, we studied the Tensor PCA problem in the Statistical Query model.

- To implement these estimators in the SQ model, we leverage on the following result.

- In this section we describe SQ procedures that achieve the optimal sample complexity for the Tensor PCA testing and estimation problems.




# [Benchmarking Unsupervised Object Representations for Video Sequences](https://jmlr.org/papers/volume22/21-0199/21-0199.pdf)
- Hence, in this work, we propose a benchmark based on procedurally generated video sequences to test basic perceptual abilities of object-centric video models under various challenging tracking scenarios.

- The MOT challenge uses the CLEAR MOT metrics (Bernardin and Stiefelhagen, 2008), which quantitatively evaluate different performance aspects of object detection, tracking and segmentation.

- Moving forward, occlusion handling is a key component that object-centric video models need to master.

- Recently, there has been an increased interest in unsupervised learning of object-centric representations.




# [Analysis of high-dimensional Continuous Time Markov Chains using the Local Bouncy Particle Sampler](https://jmlr.org/papers/volume22/18-651/18-651.pdf)
- The key contributions of this paper are as follows: ‚Ä¢ Efficient algorithms to simulate the bouncing time for each factor of the factorized posterior density of CTMCs to boost the computational efficiency.

- ‚Ä¢ A proof-of-concept application on protein evolution to demonstrate on real data the computational efficiency of LBPS compared to state-of-the-art HMC algorithms.

- efficiency of the algorithm.

- We provide a general characterization of the running time analysis for both arbitrary factor graphs and factor graphs with sparse structure in Section 4.2.




# [Some Theoretical Insights into Wasserstein GANs](https://jmlr.org/papers/volume22/20-553/20-553.pdf)
- This, along with our observations, is an interesting avenue to choose the architecture of the discriminator.

- In the present paper, we make one step further in the analysis of mathematical forces driving WGANs and contribute to the literature in the following ways: (i) We properly define the architecture of WGANs parameterized by neural networks.

- The present section is devoted to the presentation of the WGANs framework.

- Lemma 11 stresses the importance of T P (Lip 1 , D) in the performance of WGANs.




# [Communication-Efficient Distributed Covariance Sketch, with Application to Distributed PCA](https://jmlr.org/papers/volume22/20-705/20-705.pdf)
- We also improve the communication cost for the distributed PCA problem.

- We exhibit new algorithms for distributed covariance sketch with improved communication costs.

- Based on our new distributed covariance sketch algorithm, we give an improved communication bound in this setting as well.

- For covariance sketch, we give efficient one pass algorithms with improved communication costs, and prove a tight deterministic lower bound.




# [Neighborhood Structure Assisted Non-negative Matrix Factorization and Its Application in Unsupervised Point-wise Anomaly Detection](https://jmlr.org/papers/volume22/19-924/19-924.pdf)
- In this paper, we propose a neighborhood structure-assisted non-negative matrix factorization method and demonstrate its application in anomaly detection.

- We demonstrate the benefit of the neighborhood structure-assisted NMF in the mission of anomaly detection.

- Section 4 presents the proposed NS-NMF algorithm in a structured way for both offline and online versions.

- We refer to the resulting method in this paper as the neighborhood structure-assisted NMF (NS-NMF).




# [High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm](https://jmlr.org/papers/volume22/20-576/20-576.pdf)
- However, after discretization using the Euler scheme, the resulting algorithm-a discrete-time stochastic process-has a mixing rate that scales as O(d)

- For the second step, we constructed a third-order Langevin dynamics that has smoother trajectories, and for which the integration of ‚àáU can be separated from the Brownian motion part.

- In particular, while the mixing time scales as O( ‚àö d), as with lower-order methods, we show that the Œµ-dependency term can be adaptive to the degree of smoothness of the function U .

- We proved a mixing time of order O d 1/4 /Œµ 1/2 for ridge-separable potentials, which cover a large class of machine learning models.




# [Hybrid Predictive Models: When an Interpretable Model Collaborates with a Black-box Model](https://jmlr.org/papers/volume22/19-325/19-325.pdf)
- Pareto Frontier To evaluate the trade-off between transparency and predictive performance, such as accuracy, we use Pareto frontiers.

- We instantiate this framework with the two most popular forms of interpretable models in this paper, association rules and linear models, to build two hybrid models with customized training algorithms.

- Thus a Pareto frontier captures the natural trade-off obtained by hybrid models between transparency and accuracy.

- To evaluate the models, we use pareto frontiers that characterize the trade-off between transparency and accuracy.




# [Probabilistic Iterative Methods for Linear Systems](https://jmlr.org/papers/volume22/21-0031/21-0031.pdf)
- ‚Ä¢ We study application of probabilistic iterative methods to a toy regression problem.

- It is unknown whether probabilistic iterative methods based on CG are strongly or weakly calibrated.

- Fig. 3 displays samples from the output of the probabilistic iterative methods just described.

- The contributions of this paper are therefore as follows: ‚Ä¢ We introduce probabilistic iterative methods, a class of PNM derived from iterative methods for solving linear systems such as Eq. 1.




# [Single and Multiple Change-Point Detection with Differential Privacy](https://jmlr.org/papers/volume22/19-770/19-770.pdf)
- This paper gives private algorithms for both online and offline change-point detection, including the problem of detecting multiple change-points.

- We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and then provide empirical validation of these results.

- In this section, we give new differentially private algorithms for change-point detection in the online setting.

- In this work we study the statistical problem of change-point detection through the lens of differential privacy.




# [Bayesian Distance Clustering](https://jmlr.org/papers/volume22/20-688/20-688.pdf)
- In the literature, there are two primary approaches -distance-and model-based clustering.

- There is an order difference of O(n h ) between distance-based and model-based divergences.

- In contrast, Bayesian distance clustering maintains high clustering accuracy.

- Our proposed Bayesian distance clustering approach gains some of the advantages of modelbased clustering, such as uncertainty quantification and flexibility, while significantly simplifying the model specification task.




# [GIBBON: General-purpose Information-Based Bayesian OptimisatioN](https://jmlr.org/papers/volume22/21-0120/21-0120.pdf)
- Our primary contributions are as follows: 1. We propose an approximation for a general-propose extension of MES named General-purpose Information-Based Bayesian Optimisation (GIBBON).

- Therefore, we provide the first high-performing yet computationally light-weight framework for synchronous batch BO suitable for search spaces consisting of discrete structures.

- We have presented GIBBON, a general-purpose acquisition function that extends max-value entropy search to provide computationally light-weight yet high performing optimisation for a wide range of BO problems.

- In section 3, we propose the GIBBON acquisition function, before examining GIBBON in the context of existing heuristics for batch BO (Section 4).




# [Dynamic Tensor Recommender Systems](https://jmlr.org/papers/volume22/19-792/19-792.pdf)
- In this article, we propose a new dynamic tensor recommender system which incorporates time information through a tensor-valued function.

- The following theorem establishes the convergence rate for the proposed tensor factorization.

- Theorem 2 provides the convergence rate of the proposed method given trend functions.

- Specifically, we establish the convergence rate of the proposed tensor factorization and the asymptotic normality of the spline coefficient estimator.




# [A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration](https://jmlr.org/papers/volume22/20-919/20-919.pdf)
- In addition, we also provide the perturbation bounds for tensor reconstruction.

- ‚Ä¢ In addition, we apply the new perturbation bounds of HOOI in two modern applications, tensor denoising and tensor co-clustering, from machine learning and statistics.

- Furthermore, we show both HOOI and one-step HOOI with good initialization is optimal in terms of tensor reconstruction by providing rate matching lower bound.

- Next we demonstrate the unilateral perturbation bounds for mode-k singular subspace estimation.




# [MetaGrad: Adaptation using Multiple Learning Rates in Online Learning * Tim van Erven](https://jmlr.org/papers/volume22/20-1444/20-1444.pdf)
- Then, in Section 8, we compare all versions of MetaGrad to OGD and to AdaGrad in experiments with several benchmark classification and regression data sets.

- Section 7 extends this analysis to the two other versions of MetaGrad.

- In this section analyse the sketched and coordinate-wise versions of MetaGrad.

- There are two further mechanisms to deal with extreme changes in the size of the gradients.




# [Black-Box Reductions for Zeroth-Order Gradient Algorithms to Achieve Lower Query Complexity](https://jmlr.org/papers/volume22/20-611/20-611.pdf)
- In this paper, we develop two reduction frameworks for ZO algorithms under convex and non-convex setting, respectively.

- Moreover, our frameworks can directly derive convergence results of ZO algorithms under convex and non-convex settings without extra analyses, as long as convergence results under strongly convex setting are given.

- To the best of our knowledge, we are the first to propose black-box reduction frameworks for zeroth-order algorithms and apply them to zeroth-order optimization.

- Thus, whether the function query complexities of ZO algorithms can be improved further




# [Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits](https://jmlr.org/papers/volume22/19-753/19-753.pdf)
- We propose an algorithm that achieves logarithmic pseudoregret guarantee in the adversarial regime with a self-bounding constraint simultaneously with the adversarial regret guarantee.

- The algorithm is based on online mirror descent with regularization by Tsallis entropy with power Œ±.

- To the best of our knowledge, this is also the first evidence that Thompson Sampling is vulnerable in adversarial environments.

- We show that Tsallis-INF achieves logarithmic regret in the new regime.




# [Optimal Rates of Distributed Regression with Imperfect Kernels](https://jmlr.org/papers/volume22/20-627/20-627.pdf)
- Then we conducted leave one analyses of KRR and BCKRR, which lead to sharp error bounds and capacity independent optimal rates for both approaches.

- The error bounds in Theorem 1 are sharp and the rates are capacity independent optimal.

- In Section 3 we propose a general framework for the analysis of response weighted distributed regression algorithms.

- The primary goal of this study is to verify the capacity independent optimality of distributed kernel regression algorithms when the kernel is imperfect.




# [Banach Space Representer Theorems for Neural Networks and Ridge Splines](https://jmlr.org/papers/volume22/20-583/20-583.pdf)
- Our result says that single-hidden layer neural networks are solutions to continuous-domain linear inverse problems with TV regularization in the Radon domain.

- In other words, our main result is the derivation of a neural network representer theorem.

- Single-hidden layer neural networks are superpositions of ridge functions.

- We show that continuous-domain linear inverse problems with total variation regularization in the Radon domain admit sparse atomic solutions, with the atoms being the familiar neurons of a neural network.




# [Oblivious Data for Fairness with Kernels](https://jmlr.org/papers/volume22/20-1311/20-1311.pdf)
- thanks to the plug-in approach.

- We further show how estimation errors can be controlled.

- In Section 4 we study the relation between H-independence and bounds on the dependencies between oblivious and sensitive features.

- We measure the dependencies between the predicted values and the sensitive features, and compare against a standard SVM and to FERM.




# [Soft Tensor Regression](https://jmlr.org/papers/volume22/20-476/20-476.pdf)
- We refer to the tensor regression model that uses the soft PARAFAC for estimation of the coefficient tensor as Soft Tensor Regression (Softer).

- In this paper, we propose a soft tensor regression (Softer) framework for estimating a high-dimensional linear regression model with a tensor predictor and scalar outcome.

- Softer accommodates the predictor's tensor structure by basing the coefficient tensor estimation on the parallel factors approximation, similarly to other approaches in the literature.

- Low-rank approximations to the coefficient tensor of a linear tensor regression model provide a supervised approach to estimating the relationship between a tensor predictor and a scalar outcome.




# [Guided Visual Exploration of Relations in Data Sets](https://jmlr.org/papers/volume22/19-364/19-364.pdf)
- We provide an efficient implementation of this method using constrained randomisation.

- Contributions In summary, our contributions are: (1) a computationally efficient formulation and implementation of the user's background knowledge of the data and objectives (which we here call hypotheses) using constrained randomisation, (2) a dimensionality reduction method for finding the view most informative to the user, and (3) an experimental evaluation that supports that our approach is fast, robust, and produces easily understandable results.

- We use tile constraints to describe the user's knowledge concerning relations in the data.

- In this paper we propose an interactive visual data exploration framework integrating the user's background knowledge (increasing iteratively during the exploration) and the user's current exploration interests in a principled way.




# [OpenML-Python: an extensible Python API for OpenML](https://jmlr.org/papers/volume22/19-920/19-920.pdf)
- While OpenML is an online platform, we facilitate offline usage as well.

- OpenML-Python allows easy interaction with OpenML from within Python.

- The OpenML platform is organized around several entity types which describe different aspects of a machine learning study.

- OpenML is a collaborative online machine learning (ML) platform, meant for sharing and building on prior empirical machine learning research (Vanschoren et al., 2014).




# [Implicit Langevin Algorithms for Sampling From Log-concave Densities](https://jmlr.org/papers/volume22/19-292/19-292.pdf)
- To establish geometric ergodicity, we prove the stronger Proposition 8 below.

- In this section, we establish sufficient conditions for the geometric ergodicity of the sequence of iterates generated from Algorithm 1.

- While Theorem 1 establishes the geometric ergodicity of the chain towards some stationary distribution, in general, that distribution need not necessarily be œÄ.

- Under Assumptions 1 and 2, we can now establish geometric ergodicity of the Œ∏-method scheme under certain conditions on Œ∏ and h (Theorem 1).




# [Risk-Averse Learning by Temporal Difference Methods with Markov Risk Measures](https://jmlr.org/papers/volume22/20-168/20-168.pdf)
- ‚Ä¢ A stochastic risk-averse method of temporal differences with linear function approximation (section 3) and proof of its convergence with probability one in a simulation setting (section 4).

- ‚Ä¢ A novel stochastic multistep risk-averse method of temporal differences with linear function approximation, the new projected multistep dynamic programming equation, and the analysis of its properties (section 5).

- Our contributions can be summarized as follows: ‚Ä¢ A projected risk-averse dynamic programming equation and its analysis (section 2).

- The objective of this paper is to propose and analyze new risk-averse reinforcement learning methods for Markov Decision Processes (MDPs).




# [Attention is Turing Complete](https://jmlr.org/papers/volume22/20-302/20-302.pdf)
- Our study also reveals some minimal sets of elements needed to obtain these completeness results.

- In Section 4 we prove our main result on the Turing completeness of the Transformer (Theorem 6).

- The main contribution of our paper is to show that the Transformer is Turing complete √† la Siegelmann and Sontag, that is, based on its capacity to compute and access internal dense representations of the data it processes and produces.

- This is based on two conditions: (1) the ability of RNNs to compute internal dense representations of the data, and (2) the mechanisms they use for accessing such representations.




# [Doubly infinite residual neural networks: a diffusion process approach](https://jmlr.org/papers/volume22/20-706/20-706.pdf)
- Fully i.i.d. initializations are commonly used in training of neural networks.

- Hereafter, we consider the setting of Corollary 5, i.e. fully i.i.d. network's parameters.

- Regarding convolutional networks, we consider the fully i.i.d. parameterization of Assumption 10.

- Regarding fully-connected networks, we consider the fully i.i.d. parametrization of Assumption 7.




# [Non-parametric Quantile Regression via the K-NN Fused Lasso](https://jmlr.org/papers/volume22/20-1462/20-1462.pdf)
- We show that under mild conditions, (6) attains a convergence rate of n ‚àí1/d for d-dimensional data in terms of ‚àÜ 2 n , ignoring the logarithmic factor.

- The theorems demonstrate that under general assumptions, both estimators converge at a rate of n ‚àí1/d , up to a logarithmic factor, for estimating d-dimensional data under the loss function ‚àÜ 2 n defined above.

- The experiments show that the proposed estimator outperform state-of-the-art methods on both simulated and real datasets.

- The second theorem states that, under certain choice of the tuning parameter, the penalized estimator achieves the convergence rate of n ‚àí1/d , similar to the constrained estimator, ignoring the logarithmic factor.




# [Improved Shrinkage Prediction under a Spiked Covariance Structure](https://jmlr.org/papers/volume22/21-0006/21-0006.pdf)
- We propose CASP -a Coordinate-wise Adaptive Shrinkage Prediction rule for shrinkage prediction in high-dimensional Gaussian models with an unknown mean and covariance.

- Definition 2 (Class of coordinate-wise shrinkage predictive rules).

- In Section 4, we further develop the CASP method for prediction in aggregated models.

- Consider a class of coordinate-wise shrinkage predictive rules Q cs




# [A Generalised Linear Model Framework for Œ≤-Variational Autoencoders based on Exponential Dispersion Families](https://jmlr.org/papers/volume22/21-0037/21-0037.pdf)
- ‚Ä¢ find an analytical description of the auto-pruning property of Œ≤-VAE, a reason for posterior collapse.

- Further, we provide an analytical description of the auto-pruning of Œ≤-VAE .

- For this, we establish a connection between Œ≤-VAE and generalized linear models (GLM) and provide a framework for analysing Œ≤-VAE based on the observation model distribution.

- In this work, we answer the following research question: Is there a way to generalize the loss analysis of Œ≤-VAE based on the observation model distribution?




# [From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction](https://jmlr.org/papers/volume22/20-406/20-406.pdf)
- By making the above mentioned assumptions, we arrive at an interpretation of Koopman theory that bears resemblance to the Fourier transform.

- Because of a symmetry relationship, we can leverage the Fast Fourier Transform to obtain model parameters in a fast and scalable way.

- Note that the inability of the trivial solution of Koopman's theorem to generalize stems from implicit periodicity assumptions of the discrete Fourier transform.

- Because the resulting algorithm is closely linked to the Fourier transform, we will show how the Fast Fourier Transform can be leveraged to obtain model parameters in a computationally efficient and scalable manner similar to approach introduced earlier, in spite of non-convexities and nonlinearities.




# [Integrative High Dimensional Multiple Testing with Heterogeneity under Data Sharing Constraints](https://jmlr.org/papers/volume22/20-774/20-774.pdf)
- In this paper, we propose a data shielding integrative large-scale testing (DSILT) procedure to fill this gap.

- In this paper, we propose a DSILT method for simultaneous inference of high dimensional covariate effects in the presence of between-study heterogeneity under the DataSHIELD framework.

- Algorithm 4 Individual-level meta-analysis (ILMA).

- 3. Perform multiple testing procedure in Section 2.5.




# [Geometric structure of graph Laplacian embeddings](https://jmlr.org/papers/volume22/19-683/19-683.pdf)
- Let us now introduce the notion of a well-separated mixture model.

- We are now ready to introduce the notion of a well-separated mixture model.

- Discrete and continuum limit graph Laplacian operators are introduced in Subsection 2.1; the notion of a well-separated mixture model is defined in Subsection 2.2.

- 1. We prove that the measure F ŒΩ has an orthogonal cone structure provided the model is well-separated.




# [Hoeffding's Inequality for General Markov Chains and Its Applications to Statistical Learning](https://jmlr.org/papers/volume22/19-479/19-479.pdf)
- Hoeffding's lemma asserts

- Letting Œ≥ = 0 recovers Hoeffding's lemma for a single random variable.

- To our best knowledge, there are few results on upper bound of similar form for reversible Markov chains on general state spaces.

- To the best of our knowledge, this extension of Hoeffding's lemma has never been discovered before.




# [RaSE: Random Subspace Ensemble Classification](https://jmlr.org/papers/volume22/20-600/20-600.pdf)
- At the end of Section 2, an iterative version of the RaSE algorithm is presented.

- Next, we provide a similar upper bound for the MC variance of the RaSE classifier.

- Here we propose a new criterion, which enjoys the weak consistency under the general setting of (1), based on Kullback-Leibler divergence (Kullback and Leibler, 1951).

- Third, we propose a new information criterion RIC with its theoretical properties analyzed under the high-dimensional setting.




# [PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings](https://jmlr.org/papers/volume22/20-825/20-825.pdf)
- In PyKEEN 1.0

- Knowledge graph embedding models (KGEMs) learn representations for entities and relations of KGs in vector spaces while preserving the graph structure.

- PyKEEN 1.0 currently supports 23 interaction models, seven loss functions, four regularizers, two training approaches, HPO, six evaluation metrics, and 21 built-in benchmarking datasets.

- Here, we present PyKEEN (Python KnowlEdge EmbeddiNgs) 1.0, a community effort in which PyKEEN has been re-designed and re-implemented from scratch to overcome the mentioned limitations, to make models entirely configurable, and to extend it with more interaction models and other components.




# [Refined approachability algorithms and application to regret minimization with global costs](https://jmlr.org/papers/volume22/20-1019/20-1019.pdf)
- ‚Ä¢ We consider a class of Follow the Regularized Leader algorithms (FTRL) which we convert from regret minimization to approachability.

- However, one drawback of using Blackwell's approachability is that algorithms then usually minimize the Euclidean distance of the average payoffs to the target set, which is seldom the exact quantity of interest in online learning applications.

- One of the main objectives of the present work is to provide a flexible class of algorithms which are able to minimize various distance-like quantities, and not only the Euclidean distance.

- This conversion is an extension of the scheme introduced in Abernethy et al. ( 2011), which gives approachability algorithms which minimize the Euclidean distance of the average payoff to the target set.




# [A Bayes-Optimal View on Adversarial Examples](https://jmlr.org/papers/volume22/20-567/20-567.pdf)
- This section describes our method for generating realistic image datasets with computable Bayes-Optimal classifiers that are provably robust to all adversarial attacks.

- We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and analyze the vulnerability of the optimal classifier.

- We now show under what conditions these optimal classifiers are provably robust to all adversarial attacks.

- compared to the Bayes-Optimal and the RBF SVM classifiers




# [Pathwise Conditioning of Gaussian Processes](https://jmlr.org/papers/volume22/20-1260/20-1260.pdf)
- We study the broader implications of this paradigm shift to develop a general framework for conditioning Gaussian processes at the level of random functions.

- In Section 4.5, we discussed finite-dimensional approximations of Gaussian process posteriors.

- We provide a general framework for pathwise conditioning of Gaussian processes based on GPflow (Matthews et al., 2017).

- Here, we overview the precise formalism that gives rise to the pathwise approach to conditioning Gaussian random variables and show how to derive this result from first principles.




# [Learning interaction kernels in heterogeneous systems of agents from multiple trajectories](https://jmlr.org/papers/volume22/19-861/19-861.pdf)
- We exhibit an efficient algorithm to compute the estimators based on the regularized least-squares problem (1.3), and demonstrate the learnability of interaction kernels on various systems, including opinion dynamics, predator-swarm dynamics and heterogeneous particle dynamics.

- Section 5 presents some theoretical results for the coercivity condition, a key condition for achieving the optimal convergence rate of interaction kernels.

- Next, we study the learnability of the estimated interaction kernels in this system.

- Note that the true interaction kernels are not compactly supported.




# [Optimal Minimax Variable Selection for Large-Scale Matrix Linear Regression Model](https://jmlr.org/papers/volume22/19-969/19-969.pdf)
- For implementation, an iterative hard-thresholding (IHT) algorithm is developed for matrix response linear regression models.

- Therefore, Theorem 5 generalizes their results to the matrix linear regression model (1).

- Conditions (C1) and (C2) state that both the coefficient matrix size (pq) and the number of predictors d n are allowed to grow at the exponential rate of the sample size n.

- We conduct simulation studies to examine the finite sample performance of the proposed method.




# [Learning partial correlation graphs and graphical models by covariance queries](https://jmlr.org/papers/volume22/20-1137/20-1137.pdf)
- We propose randomized procedures that recover the correct graph and have low query and computational complexity with high probability.

- Our work also provides a new way of performing constraint-based inference in Gaussian graphical models and partial correlation graphs.

- The computational complexity of the proposed algorithms is also quasi-linear.

- The rest of the paper is devoted to a careful analysis of three main cases: trees, tree-like graphs, and graphs with small treewidth.




# [Classification vs regression in overparameterized regimes: Does the loss function matter?](https://jmlr.org/papers/volume22/20-603/20-603.pdf)
- Our study investigates differences and commonalities between classification and regression, using the overparameterized linear model with Gaussian features.

- We theoretically characterize the performance of solutions for classification and regression tasks using two representative ensembles, defined below.

- In this regime, the generalization is poor for both classification and regression.

- On the other hand, we show that the choice of test loss function results in a significant asymptotic difference between classification and regression tasks.




# [Sparse Popularity Adjusted Stochastic Block Model](https://jmlr.org/papers/volume22/19-835/19-835.pdf)
- As a result, sparsity describes only the behavior of network as a whole, without distinguishing between the block-dependent sparsity patterns.

- In summary, to the best of our knowledge, our paper is the first paper that studies structural sparsity in stochastic block models and the PABM is the only block model that allows the treatment.

- We are particularly interested in the PABM since, to the best of our knowledge, it is the only block model that allows to model structural sparsity in the connections between the nodes in the network.

- The Popularity Adjusted Block Model (PABM), introduced by Sengupta and Chen (2018) and subsequently studied in Noroozi et al. (2021), provides a generalization of both the SBM and the DCBM.




# [Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms](https://jmlr.org/papers/volume22/20-147/20-147.pdf)
- More specifically, the main contributions of this paper are as follows: (i) We provide a unified convergence analysis for the cooperative SGD class of algorithms (i.e., distributed SGD algorithms with local updates).

- Elastic Averaging SGD (EASGD).

- Elastic Averaging.

- The main advantage of having a single local-update SGD update rule is that we can give a unified convergence analysis, as presented in Section 4.




# [Inference In High-dimensional Single-Index Models Under Symmetric Designs](https://jmlr.org/papers/volume22/19-744/19-744.pdf)
- Dudeja and Hsu (2018) and Pananjady and Foster (2019) consider estimation in single-and multi-index models by expanding the unknown link function in the Hermite polynomial basis.

- In this paper, we develop an inference scheme for the regression coefficients of a highdimensional single-index model with minimal restrictions on the (potentially random) link function-indeed, even discontinuous link functions are allowed-that completely bypasses the estimation of the link.

- However, their method critically uses the form of the link function and also requires it to be adequately differentiable.

- Luo and Ghosal (2016) proposed a penalized forward selection technique for high-dimensional single index models with a monotone link function.




# [A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms](https://jmlr.org/papers/volume22/19-804/19-804.pdf)
- First, we describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework.

- Second, we aim to describe a representative subset of the research that has so far been carried out on robot learning for manipulation.

- of the plane

- In so doing, we highlight the diversity of the manipulation learning problems that these methods have been applied to as well as identify the many research opportunities and challenges that remain.




# [Incorporating Unlabeled Data into Distributionally-Robust Learning](https://jmlr.org/papers/volume22/19-1023/19-1023.pdf)
- In this section, we demonstrate another application, to active learning.

- We have demonstrated an application to the problem of active learning that yields a distributionally-robust model change heuristic that empirically often outperforms the existing model change heuristics.

- We have explored an alternative to Wasserstein distributionally robust learning that incorporates unlabeled data to restrict the adversary's decision set.

- We use this same mechanism to formulate a novel distributionally-robust method for active learning; this method frequently outperforms both uniform random sampling and standard methods for active learning.




# [Beyond English-Centric Multilingual Machine Translation](https://jmlr.org/papers/volume22/20-1307/20-1307.pdf)
- We introduced m2m-100, a new Many-to-Many multilingual translation model that can translate between the 9,900 directions of 100 languages.

- In this work, we create more diverse multilingual machine translation models by building a large-scale Many-to-Many data set for 100 languages.

- Overall, we build the first true Many-to-Many data set comprising 7.5B training sentences for 100 languages, providing direct training data for thousands of translation directions.

- Figure 6 shows that backtranslation performs comparably to the Many-to-English approach.




# [Normalizing Flows for Probabilistic Modeling and Inference](https://jmlr.org/papers/volume22/19-1028/19-1028.pdf)
- Below we summarize the use of flows for sampling, variational inference, and likelihood-free inference.

- We have described normalizing flows and their use for probabilistic modeling and inference.

- Normalizing flows provide a general way of constructing flexible probability distributions over continuous random variables.

- We begin by outlining basic definitions and properties of normalizing flows.




# [Residual Energy-Based Models for Text](https://jmlr.org/papers/volume22/20-326/20-326.pdf)
- In the next sections, we show that generations from even large models can be discriminated from real text by such classifiers.

- In the previous sections, we checked empirically that machine-generated text by current stateof-the-art locally normalized and auto-regressive language models can be easily discriminated, albeit to a lesser extent in extreme generalization conditions.

- A typical artifact of auto-regressive language models is their tendency to repeat phrases (Holtzman et al., 2020;Welleck et al., 2020).

- The goal of this work is to improve upon strong language models on large scale datasets.




# [Bandit Learning in Decentralized Matching Markets](https://jmlr.org/papers/volume22/20-1429/20-1429.pdf)
- We have shown that Algorithm 1 is not incentive compatible in the fully general setting.

- We propose a solution for the decentralized version of the two-sided matching bandit problem.

- When the arms have the same preferences over players we offer a better guarantee.

- 2. Incentive compatibility in the decentralized setting.




# [Optimal Feedback Law Recovery by Gradient-Augmented Sparse Polynomial Regression](https://jmlr.org/papers/volume22/20-755/20-755.pdf)
- In the second column gradient information is added and the errors decrease for the same number of training samples (N d = 40).

- We have presented a sparse polynomial regression framework for the approximation of feedback laws arising in nonlinear optimal control.

- Similarly to Figures 3 and 4, H 1 and L 2 validation errors decrease as the number of training samples increases.

- The reduction of the number of samples due to the inclusion of gradient information is particularly relevant for high-dimensional nonlinear optimal control problems, as sampling generation can be particularly costly.




# [Counterfactual Mean Embeddings](https://jmlr.org/papers/volume22/20-185/20-185.pdf)
- Treatment effect estimation.

- In this work, we focus on the distributional treatment effect (DTE), which involves the entire outcome distributions.

- Finally, we introduce the kernel treatment effect (KTE) as a way to evaluate the distributional treatment effect in Section 3.5.

- Finally, we demonstrate the effectiveness of the proposed estimator on simulated data as well as real-world policy evaluation tasks in Section 7.




# [Universal consistency and rates of convergence of multiclass prototype algorithms in metric spaces](https://jmlr.org/papers/volume22/20-1081/20-1081.pdf)
- Rates for the Proto-k-NN rule.

- Rates for the k-NN rule.

- We now introduce a second prototype rule, termed Proto-k-NN, that hybridizes between k-NN and Proto-NN.

- Here, we instead derive rates for a second novel prototype rule, Proto-k-NN, that hybridizes between the Proto-NN and k-NN rules (Theorem 7).




# [On Efficient Multilevel Clustering via Wasserstein Distances](https://jmlr.org/papers/volume22/19-782/19-782.pdf)
- We have proposed an optimization-based approach to multilevel clustering using Wasserstein metrics.

- Therefore, the simultaneous local and global clusterings in the joint optimization formulation of MWM enable the discovery of nested multilevel structures hidden in grouped data.

- Secondly, our method requires knowledge of the numbers of clusters both in local and global clustering.

- We call the above optimization the problem of Multilevel Wasserstein Means (MWM).




# [Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning](https://jmlr.org/papers/volume22/21-0259/21-0259.pdf)
- We presented the numerical implementation of a new consensus-based model for global optimization on the sphere, which is inspired by the kinetic Kolmogorov-Kuramoto-Vicsek equation.

- The algorithm is able to perform essentially as good as ad hoc state of the art methods and in some instances it obtains quantitatively better results.

- The algorithm is able to perform essentially as good as ad hoc state of the art methods and in some instances it obtains quantifiably better results.

- These experiments show that the algorithm proposed in the present paper scales well with the dimension and is very versatile (one just needs to modify the definition of the function E and the rest goes with the same code 5 !).




# [Structure Learning of Undirected Graphical Models for Count Data](https://jmlr.org/papers/volume22/18-401/18-401.pdf)
- We aim to measure the ability of PC-LPGM to recover the true structure of the graphs, also in situations where relatively moderate sample sizes are available.

- A validation of the algorithm on two real cases is given in Section 7.

- Thus, the maximum likelihood estimator still exhibits a desirable form of robustness to model misspecification.

- The main contribution of this paper is a careful analysis of the numerical and statistical efficiency of PC-LPGM, a simple method for structure learning of undirected graphical models for count data.




# [Alibi Explain: Algorithms for Explaining Machine Learning Models](https://jmlr.org/papers/volume22/21-0017/21-0017.pdf)
- Extensive testing of code correctness and algorithm convergence is done using pytest under various Python versions.

- The library features comprehensive documentation and extensive in-depth examples of use cases 1 .

- Code Snippet 1: Demo of the Alibi API with the AnchorTabular explanation algorithm.

- The first phase of the development of Alibi has focused on creating a curated set of reference explanation algorithms with comprehensive guidance on typical use cases.




# [Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be](https://jmlr.org/papers/volume22/21-0019/21-0019.pdf)
- Two common misunderstanding have contributed to view that CNNs are architecturally invariant to translation.

- The network seem to be perfectly invariant to translation for some classes.

- We have shown that, even though most CNNs are not architecturally invariant to translation, they can learn to be by training on a data set that contains this regularity.

- Such networks would achieve translation invariance through the mechanism of being invariant to anything.




# [Integrated Principal Components Analysis](https://jmlr.org/papers/volume22/20-084/20-084.pdf)
- To this end, we propose Integrated Principal Components Analysis (iPCA), which extends a model-based framework of the classical Principal Components Analysis (PCA) to integrated data.

- Since the multiplicative Frobenius iPCA estimator is geodesically convex, then all local optima are global optima, and the Flip-Flop algorithm for the multiplicative Frobenius iPCA estimator converges to the global solution of (36).

- In particular, we show that our non-sparse iPCA estimator converges to the global solution of a non-convex problem using geodesic convexity, and in the Appendix, we show that our sparse iPCA estimator consistently estimates the underlying joint subspace.

- iPCA is also a useful and effective tool in practice to discover interesting joint patterns that are shared across multiple data sets.




# [Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data](https://jmlr.org/papers/volume22/19-1012/19-1012.pdf)
- In this paper, we develop a convex formulation of integrative clustering for high-dimensional mixed multi-view data.

- By construction, iGecco+ directly clusters mixed multi-view data and selects features from each data view simultaneously.

- Next we compare iGecco+ with other methods on mixed multi-view data.

- Specifically, we show that clustering for mixed, multi-view data can be achieved using different data-specific convex losses with a joint fusion penalty.




# [Continuous Time Analysis of Momentum Methods](https://jmlr.org/papers/volume22/19-466/19-466.pdf)
- This demonstrates that introduction of momentum in the form used within both HB and NAG results in numerical methods that do not differ substantially from gradient descent.

- We model these oscillations in the next section via use of a modified equation.

- To the best of our knowledge, the first application of HB to neural network training appears in Rumelhart et al. (1986).

- We study momentum-based optimization algorithms for the minimization task (1), with learning rate independent momentum, fixed at every iteration step, focusing on deterministic methods for clarity of exposition.




# [How to Gain on Power: Novel Conditional Independence Tests Based on Short Expansion of Conditional Mutual Information](https://jmlr.org/papers/volume22/19-600/19-600.pdf)
- We define Short Expansion of Conditional Mutual Information (SECM I) as the truncated M√∂bius expansion (17) which incorporates the leading term I(X, Y ) and interactions of order 2 i.e. the terms II(X, Z k , Y ).

- The first exhibits the dichotomous behaviour of SECM I showing that its limit can be either normal or may have a distribution of quadratic form in normal random variables.

- In this work, we discuss drawbacks of existing CM I-based procedures when the conditioning set consists of a large number of variables and in the view of them we propose a novel test procedure based on a sample analogue of SECM I (Short Expansion of Conditional Mutual Information), obtained from M√∂bius representation of CM I by truncation.

- Moreover, it is empirically confirmed that, under null hypothesis, the distribution of a quadratic form is close to the chi square distribution.




# [Context-dependent Networks in Multivariate Time Series: Models, Methods, and Risk Bounds in High Dimensions](https://jmlr.org/papers/volume22/20-244/20-244.pdf)
- In this paper, we develop two procedures that estimate context-dependent networks from autoregressive time series of annotated event data.

- The contribution of this paper focuses on estimation methods and theoretical guarantees for context-dependent network structures which exploit features associated with events.

- Our logistic-normal approach ( 7), (8) combines ideas from compositional time series and autoregressive process framework.

- Developing a statistical model for autoregressive time series of annotated event data is a non-trivial task.




# [PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review](https://jmlr.org/papers/volume22/20-190/20-190.pdf)
- In contrast, our goal is to design an assignment algorithm that can simultaneously achieve both the desired objectives of fairness and statistical accuracy.

- We provide a sharp minimax analysis under this subjective setting and prove that our assignment algorithm PeerReview4All is also near-optimal for this subjective-score setting.

- Having defined the model and estimator, we now provide a sharp minimax analysis for the subjective-score model.

- We provide a sharp analysis of the minimax risk in terms of "incorrect" accept/reject decisions, and show that our PeerReview4All algorithm leads to a near-optimal solution.




# [A Lyapunov Analysis of Accelerated Methods in Optimization](https://jmlr.org/papers/volume22/20-195/20-195.pdf)
- We provide a brief review of the technique of estimate sequences (Nesterov, 2004).

- It also makes the connection between estimate sequences and Lyapunov functions explicit.

- We derive continuous-time estimate sequences directly from our Lyapunov function arguments and show that these two techniques are equivalent.

- In particular, we present Lyapunov functions for both the continuousand discrete-time settings, and we show how to move between these Lyapunov functions.




# [Limit theorems for out-of-sample extensions of the adjacency and Laplacian spectral embeddings](https://jmlr.org/papers/volume22/19-852/19-852.pdf)
- Two natural approaches to the out-of-sample extension of ASE suggest themselves.

- To compare the out-of-sample extension to its in-sample counterpart, we consider the following set-up.

- We leave a more thorough exploration of adversarial variants of the out-of-sample extension problem for future work.

- This problem is well-studied in the dimensionality reduction literature, where it is known as the out-of-sample extension problem.




# [Unlinked Monotone Regression](https://jmlr.org/papers/volume22/20-689/20-689.pdf)
- Thus, it seems that a transitional regime occurs in the rate of convergence in case the noise distribution is known.

- This means that the rate of convergence of the estimator is driven by the integral in the first summand.

- 2. Finding (minimax) lower bounds for the rate of convergence seems to be hard to obtain in our setting.

- In this section, we will give upper bounds on the convergence rate of the minimum contrast estimator defined above.




# [Subspace Clustering through Sub-Clusters](https://jmlr.org/papers/volume22/18-780/18-780.pdf)
- On a high level, we transfer the problem from "clustering points" to "clustering sub-clusters".

- In this section, we introduce our sampling based algorithm for subspace clustering (SBSC).

- We show that the clustering through sub-clusters algorithm is highly scalable and can significantly boost the clustering accuracy on both the subset and whole dataset.

- We provide theoretical guarantees for our procedure in Section 3.




# [Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations](https://jmlr.org/papers/volume22/21-0453/21-0453.pdf)
- 3. A general result that allows to obtain bounds for the 2-Wasserstein distance between the target distribution and its numerical approximations for general SDEs.

- As an application of the suggested framework, we study two numerical methods for the underdamped Langevin dynamics.

- Section 6 applies the general result to investigate two discretizations of the underdamped Langevin dynamics.

- 2. A study of the contractivity of integrators for the underdamped Langevin dynamics that takes into account the possible impact of increasing condition numbers.




# [A flexible model-free prediction-based framework for feature ranking](https://jmlr.org/papers/volume22/20-673/20-673.pdf)
- This work introduces model-free objective-based marginal feature ranking criteria-s-CC and s-NPC-for the purpose of binary decision-making.

- This objective-based feature ranking perspective is extendable to ranking feature subsets (e.g., feature pairs).

- thanks to the robustness of s-NPC to sampling bias.

- In this section, we introduce two objective-based marginal feature ranking criteria, on the population level, under the classical paradigm and the Neyman-Pearson (NP) paradigm.




# [From Low Probability to High Confidence in Stochastic Convex Optimization](https://jmlr.org/papers/volume22/20-821/20-821.pdf)
- Our paper rests on two pillars: the proximal point method and robust distance estimation.

- Section 2 presents the problem setting and robust distance estimation.

- The current paper extends the proxBoost algorithm to constrained and regularized settings, develops consequences for both streaming and offline algorithms, and develops a smoothing technique that enables application of proxBoost for nonsmooth problems.

- In this section, we explore the consequences of the proxBoost algorithm for empirical risk minimization.




# [Langevin Monte Carlo: random coordinate descent and variance reduction](https://jmlr.org/papers/volume22/20-1205/20-1205.pdf)
- ‚Ä¢ We propose and rigorously analyze a new variance reduction method SVRG-O/U-LMC.

- The idea of RCD is to surrogate the full gradient in Gradient Descent by a randomly selected partial derivative in each iteration (Nesterov, 2012).

- More specifically, we explore how to incorporate random coordinate descent (RCD) in LMC.

- It essentially surrogates the full gradient in the gradient descent method by one directional derivative in each iteration, and thus it naturally reduces the computational cost by d folds per iteration.




# [COKE: Communication-Censored Decentralized Kernel Learning](https://jmlr.org/papers/volume22/20-070/20-070.pdf)
- ‚Ä¢ To increase the communication efficiency, we further develop a COmmunicationcensored KErnel learning (COKE) algorithm, which achieves desired learning performance given limited communication resources and energy supply.

- Decentralized kernel learning.

- Centralized kernel learning.

- This paper studies the decentralized kernel learning problem under privacy concern and communication constraints for multi-agent systems.




# [FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference](https://jmlr.org/papers/volume22/19-853/19-853.pdf)
- FLAME produces interpretable, high-quality matches.

- The main benefits of FLAME are: ‚Ä¢ It learns a weighted Hamming distance for matching based on a hold-out training set (rather than using a pre-specified distance).

- It learns a distance for matching from a hold-out training set.

- Our method (FLAME -Fast, Large-scale, Almost Matching Exactly) creates matches that are almost-exact, meaning that it tries to match treatment and control units exactly on important covariates.




# [A Two-Level Decomposition Framework Exploiting First and Second Order Information for SVM Training Problems](https://jmlr.org/papers/volume22/19-632/19-632.pdf)
- In this work we proposed a novel way to deal with sub-problems of more than two variables.

- Numerical evidence is provided to show that employing SMO to solve the sub-problems compares favorably, for up to 50 variables, with other state-of-the-art solvers designed to solve sub-problems of more than two variables, like GVPM and the Dai-Fletcher method.

- Moreover, the computational effort required to solve the subproblem becomes greater as the size of the working set grows.

- Future work will be devoted to investigate the use of sub-problems of more than 50 variables, where the benefits of the current approach start to wear off.




# [Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace](https://jmlr.org/papers/volume22/19-558/19-558.pdf)
- The COSIE model presents a flexible, adaptable model for multiple graphs, one that encompasses a rich collection of independent-edge random graphs and yet retains identifiability and model parsimony.

- We first apply our multiple adjacency spectral embedding method to the HNU1 data.

- In this paper, we resolve the following questions: first, can we construct a simple, flexible multiple random network model that can be used to approximate real-world data?

- Because the invariant subspaces defined by V are common to all the graphs, we call this model the common subspace independent edge (COSIE) random graph model.




# [On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift](https://jmlr.org/papers/volume22/19-736/19-736.pdf)
- We characterize the computational, approximation, and sample size properties of these methods in the context of a discounted Markov Decision Process (MDP).

- Bhandari and Russo (2019) also provide global convergence results beyond MDPs.

- In particular, we pay close attention to both log-linear policy classes and neural policy classes (see Section 6).

- Overall, the results of this work place policy gradient methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms.




# [Gradient Methods Never Overfit On Separable Data](https://jmlr.org/papers/volume22/20-997/20-997.pdf)
- Since there are standard generalization bounds for predictors which achieve a large margin over the dataset, we get that asymptotically, gradient descent does not overfit, even if we just run it on the empirical risk function without any explicit regularization, and even if the number of iterations T diverges to infinity.

- In other words, at no point does gradient descent significantly overfit, regardless of the number of iterations.

- Could it be that gradient methods do not overfit only after so many iterations?

- Moreover, the bounds in both papers have a worse polynomial dependence on the margin Œ≥,




# [Online stochastic gradient descent on non-convex losses from high-dimensional inference](https://jmlr.org/papers/volume22/20-1288/20-1288.pdf)
- Then Assumptions A and B hold and the population loss has information exponent 1.

- In this paper, we show that a key quantity governing the performance of online SGD is the following, which we call the information exponent for a population loss.

- Suppose that Assumptions A and B hold and that the population loss has information exponent k.

- We study the dependence of the amount of data needed (i.e., the sample complexity) for recovery of the parameter using online SGD on the information exponent.




# [On the Estimation of Network Complexity: Dimension of Graphons](https://jmlr.org/papers/volume22/19-747/19-747.pdf)
- In this paper, we develop a statistical theory of graph complexity in a universal model of random graphs.

- In particular, our error bounds for the neighborhood distance and the covering number hold for any graphon.

- Unfortunately, the covering number and the Minkowski dimension of a graphon are not identifiable from the data distribution P (‚Ñ¶,¬µ,W ) .

- Our complexity index is then defined as the covering number and the Minkowski dimension of a purified version of the (pseudo-) metric space (‚Ñ¶, r W ).




# [Is SGD a Bayesian sampler? Well, almost](https://jmlr.org/papers/volume22/20-676/20-676.pdf)
- One surprising property is that they typically perform best in the overparameterised regime, with many more parameters than data points.

- Thus, the bias in the prior is essentially translated over to the posterior.

- In Section 7.3, we discussed work showing that DNNs may have an inductive bias towards simple functions in their parameter-function map.

- We argue here that the inductive bias found in DNNs trained by SGD or related optimisers, is, to first order, determined by the parameter-function map of an untrained DNN.




# [Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime](https://jmlr.org/papers/volume22/20-974/20-974.pdf)
- pro-vided upper bounds on the population risk for the least-norm interpolant applied to a class of kernels including the Neural Tangent Kernel.

- Our main result is a finite-sample bound on the misclassification error of the maximum margin classifier.

- In some cases where s, p and n are polynomially related, the risk of the maximum-margin algorithm approaches the Bayes-optimal risk as e ‚àín œÑ , for œÑ > 0.

- This simplifies the proofs without materially affecting the analysis, since rescaling the data does not affect the accuracy of the maximum margin algorithm.




# [An Inertial Newton Algorithm for Deep Learning](https://jmlr.org/papers/volume22/19-1024/19-1024.pdf)
- Convergence rates of the underlying continuoustime differential inclusion are obtained in Section 4.

- We now study the convergence of INNA.

- We also provided new general results to study differential inclusions on Clarke subdifferential and obtain convergence rates for the continuous-time counterpart of our algorithm.

- We introduced a novel stochastic optimization algorithm featuring inertial and Newtonian behavior motivated by applications to deep learning.




# [River: machine learning for streaming data in Python](https://jmlr.org/papers/volume22/20-1380/20-1380.pdf)
- River is a machine learning package for data streams in Python.

- River's architecture is the result from the lessons learned during the development of its parent packages Creme and scikit-multiflow.

- Creme (Halford et al., 2019) and scikit-multiflow (Montiel et al., 2018) are two opensource libraries to perform machine learning in the stream setting.

- More than a simple merge of code, River includes a revamped architecture and expands functionality, e.g. support for mini-batches, processing time improvements, more metrics for classification, regression and clustering, more clustering methods, etc.




# [Achieving Fairness in the Stochastic Multi-Armed Bandit Problem](https://jmlr.org/papers/volume22/20-704/20-704.pdf)
- In this section, we provide the framework of our proposed class of Fair-MAB algorithms.

- This leads us to evaluate the cost of fairness in terms of the conventional notion of regret.

- As our primary contribution, in Section 4, we define a class of Fair-MAB algorithms, called Fair-Learn, characterized by two parameters: the unfairness tolerance and the learning algorithm used as a black-box.

- We then evaluate the cost of fairness in Fair-MAB with respect to the conventional notion of regret in Section 5.




# [Factorization Machines with Regularization for Sparse Feature Interactions](https://jmlr.org/papers/volume22/20-1170/20-1170.pdf)
- In this paper, we present a new regularization scheme for feature interaction selection in FMs.

- Formula Feature Feature interaction selection selection

- We here introduce a preferable but hard to optimize regularizer for feature interaction selection in FMs.

- We next introduce a preferable but hard to optimize regularizer ‚Ñ¶ * for feature interaction selection in FMs.




# [Learning with semi-definite programming: statistical bounds based on fixed point analysis and excess risk curvature](https://jmlr.org/papers/volume22/21-0021/21-0021.pdf)
- Goal of the paper.

- This section encompasses the main contributions of our paper for the three problems we study, namely signed clustering, angular synchronization, and MAX-CUT.

- The goal of the present paper is to introduce a new fixed point approach to the statistical analysis of SDP-based estimators, and illustrate our method on four current problems of interest, namely community detection, signed clustering, angular group synchronization, and Max-Cut.

- The aim of this paper is to put forward a methodology developed in Learning Theory for the study of SDP estimators.




# [Shape-Enforcing Operators for Generic Point and Interval Estimators of Functions](https://jmlr.org/papers/volume22/20-513/20-513.pdf)
- (1) We introduce an operator to enforce quasi-convexity and deliver improved point and interval estimates of general multivariate quasi-convex functions.

- We propose a method to enforce shape restrictions ex post on any initial generic point and interval estimates of functions by applying functional operators.

- Section 3 discusses the use of shape-enforcing operators to obtain improved point and interval estimates of functions that satisfy shape restrictions.

- We show how to use shape-enforcing operators to improve point and interval estimators of a shape-constrained function.




# [A Unified Framework for Random Forest Prediction Error Estimation](https://jmlr.org/papers/volume22/18-558/18-558.pdf)
- We propose a unified framework for random forest prediction error estimation based on a novel estimator for the conditional prediction error distribution.

- To our knowledge, we are the first to propose a method of estimating the conditional prediction error distribution of random forests.

- This paper proposes a method of estimating the conditional prediction error distribution F E (e | x) of random forests.

- Our method of prediction interval estimation avoids both of these pitfalls.




# [On Solving Probabilistic Linear Diophantine Equations](https://jmlr.org/papers/volume22/17-474/17-474.pdf)
- Trimmed p-convolution trees are benchmarked versus untrimmed p-convolution trees (Table 2).

- Here, we present the lazily trimmed p-convolution tree.

- Figure 2: Lazily, trimmed p-convolution tree.

- With these constraints in mind, the trimmed p-convolution tree method is always within a log-factor of the specialized methods.




# [Expanding Boundaries of Gap Safe Screening](https://jmlr.org/papers/volume22/21-0179/21-0179.pdf)
- Here the proposed approach reduces to the standard Gap Safe screening.

- In this paper, we proposed a safe screening framework that improves upon the existing Gap Safe screening approach, while extending its application to a wider range of problems-in particular, problems whose associated dual problem is not globally strongly concave.

- The standard Gap Safe screening approach is not applicable while the proposed extension is.

- We thus expect the proposed approach to improve over the standard Gap Safe screening.




# [The ensmallen library for flexible numerical optimization](https://jmlr.org/papers/volume22/20-416/20-416.pdf)
- The ensmallen numerical optimization provides a flexible framework for optimization of user-supplied objective functions in C++.

- A large set of pre-built optimizers is provided; at the time of writing, 46 optimizers are available.

- Unlike other frameworks, ensmallen supports many types of objective functions, provides a diverse set of pre-built optimizers, supports custom behavior via callback functions, and handles various element and matrix types used by objective functions.

- These shortcomings have motivated us to create the ensmallen library, which explicitly supports numerous types of user-defined objective functions, including general, differentiable, separable, categorical, and constrained objective functions, as well as semidefinite programs.




# [What Causes the Test Error? Going Beyond Bias-Variance via ANOVA](https://jmlr.org/papers/volume22/20-1211/20-1211.pdf)
- They further also decomposed the variance in a specific order into that stemming from label noise, initialization, and training features.

- While studied the unimodality of the overall variance, we study the properties the components individually.

- Moreover, we study the monotonicity and unimodality of certain variance components in that setting.

- Moreover, we study the monotonicity and unimodality of MSE, bias, variance, and the various variance components in a specific variance decomposition.




# [Locally Differentially-Private Randomized Response for Discrete Distribution Learning](https://jmlr.org/papers/volume22/18-726/18-726.pdf)
- Finally, we have derived inner and outer bounds to some specific instances of the fundamental privacy-fidelity trade-off curve, all of which depend on the random mechanism via Œ¶(W ) or œï(W ).

- We now introduce the metrics for characterizing this privacy-fidelity trade-off.

- Much like for the feasibility problem in Section 7.1, specializing W to the step mechanism W , allows us to derive upper bounds on the fundamental privacy-fidelity trade-off curves Œ± ( ; P) for the minimax problem.

- For a better understanding of the fundamental privacy-fidelity trade-off problems, it would be desirable to tighten the gap between inner and outer bounds much further.




# [Failures of Model-dependent Generalization Bounds for Least-norm Interpolation](https://jmlr.org/papers/volume22/20-1164/20-1164.pdf)
- If k * < n/c 1 , then, with probability at least 1 ‚àí Œ¥, the least-norm interpolant h satisfies .

- We study the minimum norm linear interpolant.

- Aside from this constraint, our result applies for any bound that is determined as a function of the output of the learning algorithm, the number of training examples, and the confidence parameter.

- The distribution Q n is defined so that the least-norm interpolant performs poorly on Q n .




# [Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs](https://jmlr.org/papers/volume22/18-489/18-489.pdf)
- The following theorem introduces a risk bound for unsupervised cross-domain mapping.

- We derive risk bounds for unsupervised cross-domain mapping, comparing alternative hyperparameters œâ ‚àà ‚Ñ¶.

- In Section 5, we derived a novel risk bound for the unsupervised learning of mappings between domains.

- 1. Theorem 1 provides a rigorous statement of a risk bound for unsupervised cross-domain mapping with IPMs, which is the basis of this work.




# [Multi-class Gaussian Process Classification with Noisy Inputs](https://jmlr.org/papers/volume22/20-107/20-107.pdf)
- In this paper we have proposed several multi-class GP classifiers that can account for input noise.

- The experiments show that the predictive distribution of the proposed methods is significantly better in terms of the test log-likelihood.

- Section 3 describes the proposed models and methods to account for input noise in the observed attributes.

- To account for input noise in the context of multi-class GP classification, we describe three different methods.




# [Convex Geometry and Duality of Over-parameterized Neural Networks](https://jmlr.org/papers/volume22/20-1447/20-1447.pdf)
- We studied two-layer ReLU networks and introduced a convex analytic framework based on duality to characterize a set of optimal solutions to the regularized training problem.

- Here, we provide the closed-form formulations for the optimal solutions to the regularized training problem (23)

- Our analysis showed that optimal solutions can be exactly characterized as the extreme points of a convex set.

- Next, we provide the closed-form formulations for the optimal solutions to the regularized training problem (16) as in Lemma 9.




# [A Bayesian Contiguous Partitioning Method for Learning Clustered Latent Variables](https://jmlr.org/papers/volume22/20-136/20-136.pdf)
- Finally, the Bayesian spanning tree partitioning prior model introduced in Section 3.1 is adopted to model œÄ.

- The preceding Bayesian spanning tree partitioning prior model can be extended to other hierarchical model settings.

- Our goal is to develop a partition model for a given spatial graph.

- For computation, we propose an RJ-MCMC algorithm to sample spanning trees and partitions from their posterior distributions.




# [sklvq: Scikit Learning Vector Quantization Rick van Veen a](https://jmlr.org/papers/volume22/21-0029/21-0029.pdf)
- Learning vector quantization (LVQ) has, since its introduction by Kohonen (1990), become an important family of supervised learning algorithms.

- In the previous section, we have shown the theory behind the design and implementation of sklvq and the resulting advantages.

- Here we present "sklvq" 1 , an open-source, Python based, and "scikit-learn" (Pedregosa et al., 2011) compatible 2 LVQ framework, including the following three variants: Generalized LVQ (GLVQ) by Sato and Yamada (1995), generalized matrix LVQ (GMLVQ), and localized GMLVQ (LGMLVQ) by Schneider et al. (2009); .

- A comprehensive review of the most relevant LVQ algorithms is given by Nova and Est√©vez (2014).




# [Explaining Explanations: Axiomatic Feature Interactions for Deep Networks](https://jmlr.org/papers/volume22/20-1223/20-1223.pdf)
- We proposed a novel method called Integrated Hessians to explain feature interactions in neural networks.

- Several existing methods explain feature interactions in neural networks.

- First, we propose an approach, Integrated Hessians, to quantify pairwise feature interactions that can be applied to any neural network architecture.

- Finally, we demonstrate the utility of Integrated Hessians in a variety of applications where identifying feature interactions in neural networks is useful.




# [Quasi-Monte Carlo Quasi-Newton in Variational Bayes](https://jmlr.org/papers/volume22/21-0498/21-0498.pdf)
- We propose to combine the stochastic quasi-Newton method with RQMC samples to create a randomized quasi-stochastic quasi-Newton (RQSQN) algorithm.

- This method is called SQN (stochastic quasi-Newton).

- Stochastic quasi-Newton algorithms like the one we study also require randomized Hessian information.

- In this section we investigate quasi-Newton quasi-Monte Carlo optimization for some VB problems.




# [Thompson Sampling Algorithms for Cascading Bandits](https://jmlr.org/papers/volume22/20-447/20-447.pdf)
- We derive a tighter bound on the regret than theirs.

- Finally, we derive a problem-independent lower bound on the regret incurred by any online algorithm for the standard cascading bandit problem.

- In Section 6, we provide a lower bound on the regret of any algorithm in the cascading bandits with its proof sketch.

- We are the first to theoretically establish a regret bound for Thompson sampling algorithm on the linear cascading bandit problem.




# [One-Shot Federated Learning: Theoretical Limits and Algorithms to Achieve Them *](https://jmlr.org/papers/volume22/19-1048/19-1048.pdf)
- We also derive a lower bound on the estimation error of any algorithm.

- We also addressed the problem of distributed learning under tiny (constant) communication budget.

- Order optimal one-shot distributed learning.

- In particular, ‚Ä¢ Under general communication budget with B ‚â• d log mn bits per signal, we present a tight lower bound and an order-optimal estimator that achieves this bounds up to poly-logarithmic factors.




# [Wasserstein Barycenters can be Computed in Polynomial Time in Fixed Dimension](https://jmlr.org/papers/volume22/20-588/20-588.pdf)
- The literature on computing Wasserstein barycenters is extensive and rapidly growing.

- We use tools from computational geometry to solve the separation oracle for (MOT-D) in polynomial time.

- We now conclude the desired efficient algorithm for the separation oracle.

- That is, we can efficiently solve (MOT-D) so long as we can efficiently implement the corresponding separation oracle.




# [First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems](https://jmlr.org/papers/volume22/20-533/20-533.pdf)
- The method we propose is called the inexact proximal point (IPP) method.

- An inexact proximal point method is presented with different variations that employ different algorithms for solving the constructed strongly monotone variational inequalities.

- provided near-optimal algorithms for strongly-convex-strongly-concave min-max problems.

- To the best of our knowledge, this is the first work that proves the non-asymptotic convergence of first-order methods to a nearly stationary solution of a class of non-smooth non-convex non-concave min-max problems.




# [Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms](https://jmlr.org/papers/volume22/20-625/20-625.pdf)
- Finally, we presented a complete weak convergence proof of the IRL algorithm using martingale averaging methods.

- This paper has presented and analyzed the convergence of passive Langevin dynamics algorithms for adaptive inverse reinforcement learning (IRL).

- Finally, this paper analyzed the weak convergence and tracking properties of passive Langevin dynamic algorithms.

- 3. Sec.4 gives a complete weak convergence proof of IRL algorithm (2) using martingale averaging methods.




# [A Contextual Bandit Bake-off](https://jmlr.org/papers/volume22/18-863/18-863.pdf)
- To our knowledge, this is the first evaluation of contextual bandit algorithms on such a large and diverse corpus of datasets.

- Simulated contextual bandit setting.

- In this section, we present our evaluation of the contextual bandit algorithms described in Section 3.

- In this paper, we presented an evaluation of practical contextual bandit algorithms on a large collection of supervised learning datasets with simulated bandit feedback.




# [Path Length Bounds for Gradient Descent and Flow](https://jmlr.org/papers/volume22/19-979/19-979.pdf)
- In this work, we study the path length of GD and gradient flow (GF) curves as an independent object of interest.

- In this section we provide lower bounds on the path length for quadratic functions, PKL functions, and separable quasiconvex functions.

- For separable quasiconvex objectives, we give matching (up to constants) upper and lower bounds on the path length.

- One such property is an upper bound on the path length of the GD curve.




# [ChainerRL: A Deep Reinforcement Learning Library](https://jmlr.org/papers/volume22/20-376/20-376.pdf)
- ChainerRL offers a comprehensive set of algorithms and abstractions, a set of "reproducibility scripts" that replicate research papers, and a companion visualizer to inspect agents.

- In this paper, we introduce ChainerRL, an open-source Python DRL library supporting both CPU and GPU training, built off of the Chainer (Tokui et al., 2019) deep learning framework.

- ChainerRL's comprehensive suite of algorithms, flexible APIs, visualization tools, and faithful reproductions can accelerate the research and application of DRL algorithms.

- Since its resurgence in 2013 (Mnih et al., 2013), deep reinforcement learning (DRL) has undergone tremendous progress, and has enabled significant advances in numerous complex sequential decision-making problems




# [Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo](https://jmlr.org/papers/volume22/21-0307/21-0307.pdf)
- These results illustrate our theoretical results and show the performance of our methods for decentralized Bayesian logistic regression problems.

- Our results are non-asymptotic and provide performance bounds for any finite k. We also illustrated the efficiency of our methods on the Bayesian linear regression and Bayesian logistic regression problems.

- We focus on applying our methods to Bayesian linear regression and Bayesian logistic regression problems.

- We recall from (2) that decentralized stochastic gradient Langevin dynamics (DE-SGLD) are based on stochastic estimates ‚àáf i (x) of the actual gradients ‚àáf i (x).




# [Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models](https://jmlr.org/papers/volume22/20-1162/20-1162.pdf)
- We have presented a fairly general method to establish the hardness of identity testing from the hardness of approximate counting.

- For this, we establish rst the hardness of the identity testing problem for antiferromagnetic Ising models with bounded edge interactions.

- Speci cally, we devise a methodology to reduce the problem of approximate counting (i.e., approximating partition functions) to identity testing.

- We explore a more re ned picture of hardness of identity testing vs. polynomial-time algorithms.




# [Method of Contraction-Expansion (MOCE) for Simultaneous Inference in Linear Models](https://jmlr.org/papers/volume22/19-776/19-776.pdf)
- We have developed a new method of contraction and expansion (MOCE) for simultaneous inference in high-dimensional linear models.

- To address such challenges, we propose a new approach, with some mild regularity conditions, termed as Method of Contraction and Expansion (MOCE).

- Thus, MOCE provides a realistic solution to valid simultaneous post-model selection inferences.

- To demonstrate the feasibility of the proposed MOCE framework for post-model selection inference, in this section we introduce a model expansion strategy based on the forward screening method proposed by Wang (2009).




# [Estimation and Optimization of Composite Outcomes](https://jmlr.org/papers/volume22/20-429/20-429.pdf)
- 1 We propose a new paradigm for estimating optimal individualized treatment rules from observational data without eliciting patient preferences.

- We propose a novel framework for using observational data to estimate a composite outcome and the corresponding optimal individualized treatment rule.

- An individualized treatment rule formalizes precision medicine as a map from the space of patient covariates into the space of allowable treatments (Murphy, 2003;Robins, 2004).

- However, in the context of precision medicine, both the utility function and the probability of optimal treatment may vary across patients.




# [Determining the Number of Communities in Degree-corrected Stochastic Block Models](https://jmlr.org/papers/volume22/20-037/20-037.pdf)
- Based on these properties, we show the consistency of our estimator for the true number of communities.

- This estimation approach enables us to establish the limiting distribution of the pseudo likelihood ratio when the model is under-fitted, and derive the upper bound for it when the model is over-fitted.

- Lastly, we obtain the estimator of the true number of communities based on the change of the pseudo-LR.

- We propose a new pseudo conditional likelihood ratio method for selecting the number of communities in DCSBMs.




# [Regulating Greed Over Time in Multi-Armed Bandits](https://jmlr.org/papers/volume22/17-720/17-720.pdf)
- This section illustrates the problem, the proposed algorithms to regulate greed over time, and theoretical results on the bound on the expected regret of each policy.

- Some multiplier functions will work better than others for regulating greed over time,

- This is the simplest method we know that would allow regulating greed over time.

- However, the "smarter" algorithms do not regulate greed over time and their performance is worse than the algorithms that do this regulation.




# [Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis](https://jmlr.org/papers/volume22/16-179/16-179.pdf)
- Therefore, we show how inter-battery factor analysis naturally manifests itself as a Gaussian process latent variable model.

- In summary, the main contributions of our paper are the following: ‚Ä¢ a probabilistic, nonlinear, nonparametric formulation of inter-battery factor analysis, ‚Ä¢ a variational framework for approximate, data-efficient Bayesian learning, ‚Ä¢ a framework for introducing priors that encourage specific latent space configurations, ‚Ä¢ a multi-view latent consolidation model that naturally extends beyond two views; to our knowledge, we present the first work which shows results of a data-efficient, factorized generative model with truly large number of views.

- A large portion of the multi-view learning literature has been motivated CCA.

- We build on our previous work (Damianou et al., 2012) and further we reinterpret that model as inter-battery factor analysis and consider extensions to more than two views.




# [A general linear-time inference method for Gaussian Processes on one dimension](https://jmlr.org/papers/volume22/21-0072/21-0072.pdf)
- To prove this theorem, we develop a convenient family of Gaussian hidden Markov models on one dimension: the Latent Exponentially Generated (LEG) process.

- We here provide several advances for Gaussian Processes on one dimension.

- However, no general proof of this conjecture exists, to our knowledge.

- Second, we develop a new unconstrained parameterization of continuous-time state-space models, making it easy to use simple gradient-descent methods.




# [Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives](https://jmlr.org/papers/volume22/20-207/20-207.pdf)
- This article characterizes the convergence rate of momentum-based optimization algorithms by taking fundamental topological properties into account.

- We have presented a convergence-rate analysis of momentum-based optimization algorithms from a dynamical systems point of view.

- The subsequent result derived in Section 2.4 highlights our assertion that fundamental topological properties can be exploited for characterizing the convergence rate of momentum-based optimization algorithms.

- ‚Ä¢ Accelerated convergence is generic to momentum-based optimization algorithms, provided that the damping scales with 1/ ‚àö Œ∫ for large Œ∫.




# [An Empirical Study of Bayesian Optimization: Acquisition Versus Partition](https://jmlr.org/papers/volume22/18-220/18-220.pdf)
- We presented experimental results comparing PGO, PBO, and ABO methods within a common open-source evaluation framework.

- To perform these experiments, we built a custom, extendable C++ framework that executes easily-repeatable evaluations of arbitrary black-box optimization algorithms.

- We conduct this investigation using a common software framework, which will be publicly available and allow for complete reproducibility.

- We first consider the performance of the algorithms when given the same experimental budget.




# [MushroomRL: Simplifying Reinforcement Learning Research](https://jmlr.org/papers/volume22/18-056/18-056.pdf)
- We developed the MushroomRL Benchmarking Suite, a framework based on MushroomRL for running large-scale benchmarking experiments on the already provided algorithms, or new ones implemented by users.

- Our results are comparable with the ones in literature assert the quality of the implementation of the algorithms in MushroomRL.

- The number of open-source RL libraries has significantly increased with the success of deep RL.

- Monitoring the execution of RL experiments, especially in deep RL, is crucial to properly assess the performance of the algorithms and the quality of the learned policies.




# [Empirical Bayes Matrix Factorization](https://jmlr.org/papers/volume22/20-589/20-589.pdf)
- Our primary contribution here is to develop and implement a more general EB approach to matrix factorization (EBMF).

- Here we apply them to matrix factorization problems.

- We demonstrate the utility of these methods through both numerical comparisons with competing methods and through a scientific application: analysis of data from the GTEx (Genotype Tissue Expression) project on genetic associations across 44 human tissues.

- Focusing on the parts of F that depend on œÑ gives: where ƒé R 2 is defined by: Empirical Bayes Matrix Factorization




# [Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls](https://jmlr.org/papers/volume22/20-1235/20-1235.pdf)
- In Section 3, we propose a Q-learning method based on the semi-discrete HJB equation and analyze its convergence properties.

- Applying the dynamic programming principle to the continuous-time Q-function, we derive a novel class of HJB equations.

- A novel class of HJB equations for Q-functions has been derived and used to construct a Q-learning method for continuous-time control.

- A Q-learning algorithm and its DQN variant are newly designed in a principled manner to use transition data collected in discrete time with a theoretically consistent target.




# [Locally Private k-Means Clustering](https://jmlr.org/papers/volume22/20-721/20-721.pdf)
- In this work, we design a new locally-private algorithm for the Euclidean k-means and kmedian problems.

- In this work we study the Euclidean k-means problem in the local model of differential privacy (LDP).

- Specifically, we present a lower bound showing that every LDP algorithm for the k-means must have additive error ‚Ñ¶( ‚àö n).

- Indeed, in Section 5 we show that every locally-private algorithm for the k-means must have additive error ‚Ñ¶( ‚àö n).




# [A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective](https://jmlr.org/papers/volume22/19-498/19-498.pdf)
- We propose a unified framework of optimal sample selection (OSS) for effectiveness determination and optimal sample selection in output noise filtering from the perspective of GE bound.

- In another word, they provide a unified framework, named as the optimal sample selection (OSS) framework, for the output noise filtering in regression.

- A noise estimator and a novel filter are presented to deal with noisy outputs in regression and ordinal classification problems.

- The covering distance is proposed for estimating the output noise, and it is integrated with the OSS framework, generating the CDF filtering algorithm.




# [NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization](https://jmlr.org/papers/volume22/20-255/20-255.pdf)
- ‚Ä¢ We propose a non-uniform gradient quantization method and establish strong theoretical guarantees for its excess variance and communication costs.

- Furthermore, NUQSGD enjoys strong theoretical guarantees.

- In this paper, we propose and study a new scheme to quantize normalized gradient vectors.

- We study data-parallel and communication-efficient version of stochastic gradient descent.




# [How Well Generative Adversarial Networks Learn Distributions](https://jmlr.org/papers/volume22/20-911/20-911.pdf)
- The main technical contributions are the development of the oracle inequalities for analyzing GANs, and the formulation of the new generator-discriminator-pair regularization.

- We develop the oracle inequalities, which are the main tool for analyzing the rates of GANs.

- Further discussions on the generator-discriminator-pair regularization and connections to the regularity theory in optimal transport is deferred to Section 4.

- We consider two special cases of neural network generator and discriminator and derive the rates for implicitly estimating certain parametric distributions.




# [Prediction against a limited adversary](https://jmlr.org/papers/volume22/20-1234/20-1234.pdf)
- Using viscosity theory tools in the field of partial differential equation, we provided the growth rate of regret for the forecaster.

- Viscosity solution theory provides formidable tools to rigorously show this convergence and study the properties of the long-time behavior of the value function.

- Using these tools, we show that the long-time behavior of the regret have different regimes

- (ii) Balanced strategies exist, Œ¶ satisfies Assumption 2 (iii): the growth rate of regret is given by the solution of ( 24); see Theorem 12.




# [A Greedy Algorithm for Quantizing Neural Networks](https://jmlr.org/papers/volume22/20-1233/20-1233.pdf)
- Specifically, ‚Ä¢ We propose a novel algorithm in (2) and (3) for sequentially quantizing layers of a pre-trained neural network in a data-dependent manner.

- ‚Ä¢ We establish upper bounds on the relative training error in Theorem 2 and the generalization error in Theorem 3 when quantizing the first layer of a neural network that hold with high probability when the training data are Gaussian.

- Our next result shows that the quantization error is well-controlled in the span of the training data so that the quantized weights generalize to new data.

- These complexities are already quite restrictive and only give the run-time for quantizing a single neuron.




# [Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?](https://jmlr.org/papers/volume22/20-1422/20-1422.pdf)
- In this paper, we presented an in-depth study of acquisition function maximisation in Bayesian optimisation.

- Compositional acquisition function maximisation requires considerably larger memory relative to ERM.

- In order to both improve and analyse the optimisation performance on the compositional form of the acquisition function, we introduce several algorithmic adaptations.

- We provide a general overview of compositional optimisation and derive compositional forms for the four most popular myopic acquisition functions.




# [Projection-free Decentralized Online Learning for Submodular Maximization over Time-Varying Networks](https://jmlr.org/papers/volume22/18-407/18-407.pdf)
- ‚Ä¢ We propose a decentralized one-shot Frank-Wolfe online learning method over timevarying networks for submodular maximization in the stochastic online setting, where each agent uses local communication and local computation.

- Furthermore, we have also showed that the proposed algorithm can achieve an expected regret bound of O(T 2/3 ) with (1 ‚àí 1/e) approximation guarantee, where T is a time horizon.

- ‚Ä¢ We also show that the decentralized one-shot Frank-Wolfe online learning method can achieve (1 ‚àí 1/e)-regret with a bound O(T 2/3 ).

- Furthermore, Chen et al. (2018b) also proposed stochastic conditional gradient online optimization algorithms and showed that the regret bound O( ‚àö T ) is achieved with (1 ‚àí 1/e) approximation guarantee.




# [Representer Theorems in Banach Spaces: Minimum Norm Interpolation, Regularized Learning and Semi-Discrete Inverse Problems](https://jmlr.org/papers/volume22/20-751/20-751.pdf)
- We also present the infimum of the MNI problem in a Banach space.

- In the literature there are a few existing representer theorems for regularized learning problems in a Banach space.

- We first describe the MNI problem in a Banach space.

- In section 5, we propose fixed-point equations for the MNI problem in a Banach space.




# [Stochastic Proximal AUC Maximization](https://jmlr.org/papers/volume22/19-418/19-418.pdf)
- ‚Ä¢ We propose a novel stochastic proximal algorithm for AUC maximization which accommodates general convex regularizers with favorable O(d) space and per-iteration time complexities.

- In this paper, we presented a new stochastic gradient descent method for AUC maximization which can accommodate general penalty terms.

- Our objective is to develop efficient SGD-type algorithms for AUC maximization scalable to large-scale streaming data.

- Our algorithm can update the model parameter upon receiving individual data with favorable O(d) space and per-iteration time complexity, making it amenable for streaming data analysis.




# [Unfolding-Model-Based Visualization: Theory, Method and Applications](https://jmlr.org/papers/volume22/18-846/18-846.pdf)
- Second, an estimator is proposed for the ideal points and an asymptotic theory is provided for this estimator, shedding lights on the validity of model-based visualization.

- Under reasonable conditions, we provide asymptotic results for the recovery of ideal-point configuration.

- We call (3) the average loss for the recovery of ideal point configuration.

- Little statistical theory has been developed for the recovery of configuration based on MDS models.




# [The Decoupled Extended Kalman Filter for Dynamic Exponential-Family Factorization Models](https://jmlr.org/papers/volume22/18-417/18-417.pdf)
- A reader familiar with the extended Kalman filter may find it difficult to map the above expressions onto the standard EKF expressions.

- Algorithm 2: The DEKF optimized for dynamic factorization models.

- This derivation directly illustrates why the DEKF is particularly appropriate for factorization models.

- In factorization models, typically d k.




# [Conditional independences and causal relations implied by sets of equations](https://jmlr.org/papers/volume22/20-863/20-863.pdf)
- We then prove that Simon's causal ordering algorithm is well-defined and has a unique output.

- We showed that the causal ordering graph, on the other hand, does encode the effects of soft and certain perfect interventions.

- In Section 4 we will show how the Markov ordering graph can be constructed from a causal ordering graph.

- 17 We prove that the causal ordering graph represents the effects of both soft interventions on equations and perfect interventions on 17.




# [Bandit Convex Optimization in Non-stationary Environments](https://jmlr.org/papers/volume22/20-763/20-763.pdf)
- In the experiments, we choose the cumulative loss as the performance measure of different learning algorithms.

- As a result, our proposed algorithm is more adaptive to the non-stationary environments than BCO algorithms designed for minimizing static regret or worse-case dynamic regret.

- ‚Ä¢ We establish the first minimax lower bound of the universal dynamic regret for bandit convex optimization problems.

- We extend the algorithm to an anytime version.




# [Simple and Fast Algorithms for Interactive Machine Learning with Random Counter-examples](https://jmlr.org/papers/volume22/19-372/19-372.pdf)
- In this work we provided simple and efficient algorithms for interactively learning nonbinary concepts in the recently proposed setting of exact learning from random counterexamples (LRC).

- We also provided an analysis that shows that interactive LRC learning, regardless of the learning algorithm, is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning.

- It also establishes that LRC learning, regardless of the learning algorithm, is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning (Kearns and Vazirani, 1994).

- We also define the PAC-LRC model, an extension of LRC, for approximate learning with random counter-examples.




# [mvlearn: Multiview Machine Learning in Python](https://jmlr.org/papers/volume22/20-1370/20-1370.pdf)
- Compose: Several functions for integrating single-view and multiview methods are implemented, facilitating operations such as preprocessing, merging, or creating multiview data sets.

- Also, plotting tools extend matplotlib and seaborn to facilitate visualizing multiview data.

- Cluster: mvlearn contains multiple algorithms for multiview clustering, which can better take advantage of multiview data by using unsupervised adaptations of cotraining.

- With these methods accessible to non-specialists, multiview learning algorithms will be able to improve results in academic and industry applications of machine learning.




# [Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions](https://jmlr.org/papers/volume22/21-0287/21-0287.pdf)
- We undertake a novel perturbation analysis for low multilinear rank tensor CUR approximations, and prove error bounds for the two primary tensor CUR decompositions discussed here.

- There are very few approximation bounds for any tensor CUR decompositions.

- First, let us state the formation of both Chidori and Fiber CUR decompositions via random sampling in algorithmic form here.

- The runtime and approximation performance of the tensor CUR decompositions are compared against the other state-of-the-art methods.




# [A Unified Analysis of First-Order Methods for Smooth Games via Integral Quadratic Constraints](https://jmlr.org/papers/volume22/20-1068/20-1068.pdf)
- Table 1: Global convergence rates of algorithms for smooth and strongly-monotone games.

- ‚Ä¢ Similarly, we derive an analytical convergence bound for the proximal point method that is sharper than the best available result (Mokhtari et al., 2020a, Theorem 2).

- ‚Ä¢ We derive a slightly improved convergence rate for the optimistic gradient method (even though the existing analysis (Gidel et al., 2018) is fairly involved).

- ‚Ä¢ We also show that the optimistic gradient method achieves the optimal convergence rate provable in our framework among algorithms with one step of memory (6).




# [A Distributed Method for Fitting Laplacian Regularized Stratified Models](https://jmlr.org/papers/volume22/19-345/19-345.pdf)
- Algorithm 4.1 Distributed method for fitting stratified models with Laplacian regularization.

- Stratified model fitting with Laplacian regularization is simply a convex optimization problem with Laplacian regularization.

- We now explore the idea of fitting stratified models with nonconvex local loss and regularization functions.

- First, we provide a simple, cohesive discussion of stratified model fitting with Laplacian regularization, which previously was not unified.




# [Prediction Under Latent Factor Regression: Adaptive PCR, Interpolating Predictors and Beyond](https://jmlr.org/papers/volume22/20-768/20-768.pdf)
- 1. General finite sample risk bounds for linear predictors, under factor regression models.

- 2. Finite sample risk bounds for PCRs, with data-adaptive s principal components.

- Our main applications will be to the finite sample risk bounds of the three classes of predictors discussed in the previous section.

- We use Theorem 3 to analyze the prediction risk of PCR-s under the factor regression model, for two choices of the number of principal components s.




# [Testing Conditional Independence via Quantile Regression Based Partial Copulas](https://jmlr.org/papers/volume22/20-1074/20-1074.pdf)
- The second main contribution of this paper is an analysis of a nonparametric test for conditional independence based on the partial copula construction.

- based on quantile regression.

- In this work we take a novel approach to testing conditional independence using the partial copula by using quantile regression for estimating the conditional distribution functions.

- We now compare the partial copula based test Œ®n with other nonparametric tests.





# [Strong Consistency, Graph Laplacians, and the Stochastic Block Model](https://jmlr.org/papers/volume22/20-391/20-391.pdf)
- In this work, we will study the performance of spectral clustering in community detection for the stochastic block model.

- We prove that spectral clustering is able to achieve strong consistency when the triple (n, p, q) satisfies the information-theoretic limits Abbe et al. (2016); Mossel et al. (2016) where p = αn −1 log n and q = βn −1 log n. In particular, our analysis of the normalized Laplacian is new and should be of independent interest.

- In this work, we establish an ∞ -norm perturbation bound for the Fiedler eigenvector of both the unnormalized Laplacian and the normalized Laplacian associated with the stochastic block model.

- We will present the main results, including the strong consistency of spectral clustering, in Section 3.




# [Towards a Unified Analysis of Random Fourier Features](https://jmlr.org/papers/volume22/20-1369/20-1369.pdf)
- Moreover, the number of features can be reduced even further if a data-dependent sampling distribution is employed.

- The Ω(n) requirement on the number of features is too restrictive and precludes any computational savings.

- The required number of features can be Ω(log n log log n)

- In particular, we have given generic bounds on the number of features required for consistency of learning with two sampling strategies: leverage weighted and plain random Fourier features.




# [Edge Sampling Using Local Network Information](https://jmlr.org/papers/volume22/18-240/18-240.pdf)
- Parameter α measures the average strength of local network connectivity.

- Measuring the strength of local connectivity through local networks is more challenging, and we leave it for future work.

- This simple statistic provides an easy way to measure the strength of network local connectivity through parameter α, which directly controls the accuracy of the sampling method.

- In contrast, if the local connectivity is weak (for example, when t ij = O(1))




# [When Does Gradient Descent with Logistic Loss Find Interpolating Two-layer Networks?](https://jmlr.org/papers/volume22/20-1372/20-1372.pdf)
- We demonstrated that gradient descent drives the logistic loss of finite-width two-layer Huberized ReLU networks to zero if the initial loss is small enough.

- However, they do not prove that the training loss converges to zero.

- It guarantees that if the initial loss is small then gradient descent drives the logistic loss to zero.

- In this paper, we show that, under two sets of conditions, training fixed-width two-layer networks with gradient descent drives the logistic loss to zero.




# [Non-attracting Regions of Local Minima in Deep and Wide Neural Networks](https://jmlr.org/papers/volume22/19-586/19-586.pdf)
- We have proved the existence of suboptimal local minima for regression neural networks with sigmoid activation functions of arbitrary width.

- It can be shown that such generalized local minima at infinity also exist for deep neural networks.

- We theoretically proved the possibility of suboptimal local minima in deep and wide networks and empirically validated their existence.

- Therefore, the critical points can only be saddle points or local minima on a non-attracting region of local minima.




# [Information criteria for non-normalized models](https://jmlr.org/papers/volume22/20-1366/20-1366.pdf)
- The proposed criteria are approximately unbiased estimators of discrepancy measures for non-normalized models.

- In this study, we develop information criteria for non-normalized models estimated by NCE or score matching.

- Experimental results show that these procedures successfully select the appropriate non-normalized model in a data-driven manner.

- In Sections 5 and 6, we develop information criteria for non-normalized models estimated by NCE and score matching, respectively.




# [Preference-based Online Learning with Dueling Bandits: A Survey](https://jmlr.org/papers/volume22/18-546/18-546.pdf)
- In the following, we review all known methods of the preference-based multi-armed bandit literature for the axiomatic approaches.

- The aim of this paper is to provide a survey of the state of the art in the field of preference-based multi-armed bandits (PB-MAB); it updates and significantly extends an earlier review by Busa-Fekete and Hüllermeier (2014).

- Similar questions can be addressed in the preference-based setup.

- In this paper, we surveyed the state of the art in preference-based online learning with bandit algorithms, a relatively recent research field that we referred to as preference-based multi-armed bandits (PB-MAB), and which is also known under the notion of "dueling bandits".




# [Ranking and synchronization from pairwise measurements via SVD](https://jmlr.org/papers/volume22/19-542/19-542.pdf)
- This paper considered the problems of ranking and time synchronization given a subset of noisy pairwise comparisons, and proposed an SVD-based algorithmic pipeline to solve both tasks.

- We provide a detailed theoretical consistency analysis for both algorithms for a random measurement model (see Section 3) in terms of robustness against sampling sparsity of the measurement graph and noise level.

- Additionally, we provide extensive numerical experiments on both synthetic and real data, showing that in certain noise and sparsity regimes, our proposed algorithms perform comparable or better than state-of-the-art methods.

- We consider a variety of performance metrics summarized further below, altogether highlighting the competitiveness of our proposed SVD-based algorithms with that of state-of-the-art methods.




# [Phase Diagram for Two-layer ReLU Neural Networks at Infinite-width Limit](https://jmlr.org/papers/volume22/20-1123/20-1123.pdf)
- In this paper, we characterized the linear, critical, and condensed regimes with distinctive features and draw the phase diagram for the two-layer ReLU NN at the infinite-width limit.

- However, its exact range in the phase diagram remains unclear.

- We experimentally demonstrate and theoretically prove the transition across the boundary (critical regime) in the phase diagram.

- Identification of the coordinates is important for drawing the phase diagram.




# [As You Like It: Localization via Paired Comparisons](https://jmlr.org/papers/volume22/18-105/18-105.pdf)
- 2 )︀ = Θ(ℓ 2 ) possible paired comparisons

- Here we aim to understand how the paired comparisons change with the introduction of "pre-quantization" Gaussian noise.

- The results of this section apply without considering any particular recovery method or algorithm and relate to the number of paired comparisons which may be flipped due to the noise, not from reconstruction.

- There is a large body of work that studies the problem of learning to rank items from various sources of data, including paired comparisons of the sort we consider in this paper.




# [Nonparametric Modeling of Higher-Order Interactions via Hypergraphons](https://jmlr.org/papers/volume22/19-941/19-941.pdf)
- In order to facilitate efficient estimation and computation, we restrict ourself to a class of Simple Lipschitz Hypergraphons (SLH).

- We provided rates of convergence in expectation for estimating a class of Smooth Lipschitz Hypergraphons (SLH) and provided practical algorithms for implementing the estimators.

- For this class of hypergraphons, we propose an estimator along with its rates of convergence.

- We now introduce our estimator and establish rates of convergence for estimating the probability tensor, under the hypergraphon model.




# [Explaining by Removing: A Unified Framework for Model Explanation](https://jmlr.org/papers/volume22/20-1316/20-1316.pdf)
- Our contributions include: 1. We present a unified framework that characterizes 26 existing explanation methods and formalizes a new class of removal-based explanations.

- Removal-based explanations perform a specific type of counterfactual reasoning.

- We now examine how removal-based explanations are connected to information theory.

- This perspective shows the broad potential applications for removal-based explanations.




# [Global and Quadratic Convergence of Newton Hard-Thresholding Pursuit](https://jmlr.org/papers/volume22/19-026/19-026.pdf)
- We will establish its global convergence to an η stationary point under the restricted strong smoothness of f .

- In this way, we establish the global convergence to the η-stationarity.

- Suppose NHTP: Newton Hard-Thresholding Pursuit Step 0 Initialize x 0 .

- In this main section, we present our Newton Hard-Thresholding Pursuit (NHTP) algorithm, which largely follows the general framework (3), but with distinctive features.




# [Aggregated Hold-Out](https://jmlr.org/papers/volume22/19-624/19-624.pdf)
- This enables us to prove an oracle inequality for the hold-out and agghoo in regularized kernel regression with a general Lipschitz loss (Theorem 11).

- This section recalls the definition of cross-validation for estimator selection, and introduces a new procedure called aggregated hold-out (agghoo).

- The first theorem is a general oracle inequality for the hold-out.

- To the best of our knowledge, all these oracle inequalities are new, even for the hold-out.




# [Sparse and Smooth Signal Estimation: Convexification of 0 -Formulations](https://jmlr.org/papers/volume22/18-745/18-745.pdf)
- The proposed iterative convexification approach substantially closes the gap between the 0 -"norm" and its 1 surrogate and results in significantly better estimators than the standard approaches using 1 surrogates.

- In this paper, we derived strong iterative convex relaxations for quadratic optimization problems with M-matrices and indicators, of which signal estimation with smoothness and sparsity is a special case.

- In Section 4 we give conic quadratic extended reformulation of the model and describe a scalable Lagrangian decomposition method to solve it.

- For additional scalability, we give an easy-to-parallelize tailored Lagrangian decomposition method that solves instances with n = 100,000 under one minute.




# [Generalization Performance of Multi-pass Stochastic Gradient Descent with Convex Loss Functions](https://jmlr.org/papers/volume22/19-716/19-716.pdf)
- We develop both capacity-independent and capacity-dependent learning rates with high probability.

- In this paper, we investigate both capacity-independent and capacity-dependent learning rates for multi-pass SGD with general convex loss functions.

- We now develop capacity-dependent learning rates.

- Optimal capacity-dependent learning rates O(n for the case α + ζ/2 < 1.




# [Flexible Signal Denoising via Flexible Empirical Bayes Shrinkage](https://jmlr.org/papers/volume22/19-042/19-042.pdf)
- Our methods are implemented in the R package smashr ("SMoothing by Adaptive SHrinkage in R"), available on GitHub (https://www. github.com/stephenslab/smashr).

- We have introduced "SMoothing by Adaptive SHrinkage" (SMASH) for smoothing Gaussian and Poisson data using multi-scale methods.

- We also compared against the empirical Bayes shrinkage procedure, "EbayesThresh" (Johnstone and Silverman, 2005a).

- These wavelet coefficients μj will be estimated using empirical Bayes shrinkage (14).




# [Particle-Gibbs Sampling for Bayesian Feature Allocation Models](https://jmlr.org/papers/volume22/20-082/20-082.pdf)
- Our PG sampling approach has computational complexity that scales linearly with the number of features.

- We overcome this limitation by using the PG methodology to develop an algorithm which scales linearly in the number of features.

- An alternative approach to speeding up sampling for feature allocation models was proposed by Doshi-Velez and Ghahramani (2009).

- Taken together our results suggest the DPF algorithm is a computationally efficient and generally applicable approach for performing Bayesian inference for feature allocation models.




# [Optimal Bounds between f-Divergences and Integral Probability Metrics *](https://jmlr.org/papers/volume22/20-867/20-867.pdf)
- we define a generalization of the log moment-generating function and show that it exactly characterizes the best lower bound of a ϕ-divergence by a given IPM.

- For example, the best lower bound of the Kullback-Leibler divergence by a quadratic function of the total variation distance is known as Pinsker's inequality.

- Throughout this paper, the ϕ-cumulant generating function has proved central in explicitating the relationship between ϕ-divergences and integral probability metrics.

- This establishes a "correspondence principle" by which properties of the relationship between ϕ-divergences and integral probability metrics translate by duality into properties of the cumulant generating function, and vice versa.




# [Replica Exchange for Non-Convex Optimization](https://jmlr.org/papers/volume22/20-697/20-697.pdf)
- After exchange, GD can converge to the global minimum, and LD keeps exploring.

- When γ = 1 (Figure 6 (a)), we do not see convergence to the global minimum at all.

- We then establish convergence to the global minimum conditional on being in the properly defined small set.

- The LD explores the state space and visits the neighborhood of the global minimum.




# [Finite Time LTI System Identification](https://jmlr.org/papers/volume22/19-725/19-725.pdf)
- We show that it is always possible to learn the parameters of a lower-order approximation of the underlying system.

- In this paper we provide a purely data-driven approach to system identification from a single time-series of finite noisy data.

- We propose a new approach to system identification when we observe only finite noisy data.

- This leads to the question of model order selection from data.




# [Predictive Learning on Hidden Tree-Structured Ising Models](https://jmlr.org/papers/volume22/19-149/19-149.pdf)
- In particular, we derived sample complexity guarantees for exact structure learning and marginal distributions estimation.

- Our main results Theorem 5 and Theorem 7 provide the amount of finite samples needed for exact structure recovery and accurate predictive learning with high probability.

- We first find upper (Theorem 5) and lower bounds (Theorem 6) on the sample complexity for exact hidden structure recovery using the Chow-Liu algorithm on noisy observations.

- • Determination of the sufficient and necessary number of samples for accurate predictive learning.




# [Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent](https://jmlr.org/papers/volume22/20-1067/20-1067.pdf)
- We consider four low-rank matrix estimation tasks: • Low-rank matrix sensing.

- Our analysis is also applicable to general loss functions that are restricted strongly convex and smooth over low-rank matrices.

- Section 4 illustrates the excellent empirical performance of ScaledGD in a variety of low-rank matrix estimation problems.

- Section 2 describes the proposed ScaledGD method and details its application to low-rank matrix sensing, robust PCA and matrix completion with theoretical guarantees in terms of both statistical and computational complexities, highlighting the role of a new distance metric.




# [Adaptive Estimation of Nonparametric Functionals](https://jmlr.org/papers/volume22/19-892/19-892.pdf)
- As mentioned above, such dependence can even further relaxed by considering estimators based on even higher-order U-statistics.

- In particular, we summarize our results on upper and lower bounds on the adaptive minimax estimation rate of φ(P ) in the following theorem.

- However, since our estimators are based on second-order U-statistics, we can only perform a second-order bias correction (see e.g. Robins et al. (2008Robins et al. ( , 2017) and this results in the requirement of sufficient large smoothness of the marginal density of the covariates.

- First we discuss a general recipe for producing a data-adaptive "best" estimator from a sequence of estimators based on second-order U-statistics -which in turn are constructed from compactly supported wavelet based projection kernels (defined in Section 5).




# [Learning a High-dimensional Linear Structural Equation Model via 1 -Regularized Regression](https://jmlr.org/papers/volume22/20-1005/20-1005.pdf)
- In Section 3, we introduce the new algorithm for high-dimensional linear SEM learning with sub-Gaussian and bounded-moment error distributions.

- This paper provides a statistically consistent and computational feasible algorithm for learning high-dimensional linear SEMs via 1 -regularized regression.

- This section introduces a new regression-based algorithm for high-dimensional linear SEMs.

- This section provides the statistical guarantees on Algorithm 1 for learning high-dimensional linear SEMs (2).




# [Consistent Semi-Supervised Graph Regularization for High Dimensional Data](https://jmlr.org/papers/volume22/19-081/19-081.pdf)
- This result is fundamentally responsible for the failure of semi-supervised Laplacian regularization on large dimensional data.

- These semi-supervised learning algorithms of Laplacian regularization are presented in Section 2.1.

- The focus of this article is to promote the usage of centered similarities in graph regularization for semi-supervised learning.

- We review first some competitive variants of Laplacian regularization before presenting another major approach of graph-based semi-supervised learning which relies explicitly on the spectral decomposition of Laplacian matrices.




# [Estimation and Inference for High Dimensional Generalized Linear Models: A Splitting and Smoothing Approach](https://jmlr.org/papers/volume22/19-132/19-132.pdf)
- We propose a new approach for drawing inference with high dimensional GLMs.

- Indeed, the de-biased methods may be sensitive to tuning parameters, which could explain the gap in the finite sample performance.

- Section 6 provides simulations and comparisons with the existing methods.

- Unlike many of the existing methods (Zhang and Zhang, 2014;Javanmard and Montanari, 2018), our method is more computationally feasible as it does not require estimating high dimensional precision matrices.




# [Convergence Guarantees for Gaussian Process Means With Misspecified Likelihoods and Smoothness](https://jmlr.org/papers/volume22/20-662/20-662.pdf)
- In each setting, our results demonstrate the impact of the choice of hyperparameters and the experimental design.

- In particular, the impact of the experimental design and covariance function is made clear in the bounds.

- This often occurs because conditioning of Gaussian process means on data is only possible in closed-form if assuming the data is noiseless, or contains independently and identically distributed Gaussian noise with known variance.

- • Experimental design: The terms h Xn and ρ Xn quantify the impact of the experimental design.




# [POT: Python Optimal Transport](https://jmlr.org/papers/volume22/20-451/20-451.pdf)
- The Python Optimal Transport (POT) library takes advantage of Python to make Optimal Transport accessible to the machine learning community.

- We have presented POT, an optimal transport toolbox written in Python.

- Kantorovich optimal transport problems.

- The library provides recent state-of-the-art solvers for various optimal transport problems related to statistics and machine learning.




# [Optimal Structured Principal Subspace Estimation: Metric Entropy and Minimax Rates](https://jmlr.org/papers/volume22/20-610/20-610.pdf)
- However, the minimax rates of convergence for subspace constrained PCA/SVD remain unknown.

- Minimax optimal rates of convergence for a collection of structured PCA/SVD problems are established.

- The results yield interesting phase transition phenomena concerning the rates of convergence as functions of the SNRs and the fundamental limit for consistent estimation.

- The general lower and upper bounds reduce determination of the minimax optimal rates for many interesting problems to mere calculations of certain information-geometric quantities.




# [A General Framework for Adversarial Label Learning](https://jmlr.org/papers/volume22/20-537/20-537.pdf)
- This paper introduces adversarial label learning (ALL), a method for training classifiers without labels by making use of weak supervision.

- We test adversarial label learning on a variety of datasets, comparing it with other approaches for weak supervision.

- We introduced adversarial label learning (ALL), a method to train robust classifiers when access to labeled training data is limited.

- Subsequently, we introduced Multi-ALL, a generalized adversarial label learning framework that enables users to encode information about the data as a set of linear constraints.




# [Matrix Product States for Inference in Discrete Probabilistic Models](https://jmlr.org/papers/volume22/18-431/18-431.pdf)
- To get a sense for the low fidelity of the mean-field approximation, we will draw a connection between multivariate discrete distributions and tensors.

- The rank of the tensor train is a fundamental hyperparameter which will effectively correspond to the expressiveness of the approximation.

- The matrix product state (MPS) literature contains a number of results not present in the tensor train literature, which motivates its introduction here.

- The rank of the matrix product state is the most significant tuning parameter, both in terms of modeling capacity and computational complexity.




# [Pseudo-Marginal Hamiltonian Monte Carlo](https://jmlr.org/papers/volume22/19-486/19-486.pdf)
- (15 ) The pseudo-marginal MH algorithm can mix very poorly if the relative variance of the likelihood estimator is large; e.g. if N = 1 in (13).

- In this article, we propose a novel HMC scheme, termed pseudo-marginal HMC (PM-HMC), which mimics the HMC algorithm targeting the marginal posterior (1) while integrating out numerically the auxiliary variables.

- Results for the remaining methods/settings, including the pseudo-marginal MH method and the Particle Gibbs sampler which both performed very poorly, are given in the appendix.

- The three plots correspond to the pseudo-marginal HMC sampler, the pseudo-marginal slice sampler, and the Gibbs sampler with conditional importance sampling kernels, respectively.




# [Learning Laplacian Matrix from Graph Signals with Sparse Spectral Representation](https://jmlr.org/papers/volume22/19-944/19-944.pdf)
- • A factor analysis model for smooth graph signals with sparse spectral representation is introduced (Section 5).

- Interpretation of the terms.

- We proposed two algorithms to solve the corresponding optimization problem.

- This model provides a probabilistic interpretation of our optimization program and links its objective function to a maximum a posteriori estimation.




# [Kernel Operations on the GPU, with Autodiff, without Memory Overflows](https://jmlr.org/papers/volume22/20-275/20-275.pdf)
- As showcased on our website and at the end of this paper, KeOps scripts for kernel and geometric applications generally outperform their Numpy and PyTorch counterparts by several orders of magnitude while keeping a linear memory footprint.

- These high-level Python frameworks unlock the use of massively parallel hardware for machine learning research.

- Our focus on the simple yet powerful concept of symbolic matrices allows us to keep a transparent interface, while being more efficient than the generalist PyTorch and XLA frameworks on a wide range of computations.

- The workhorse of the KeOps library is a C++ engine for generic reductions on sampled data.




# [An Importance Weighted Feature Selection Stability Measure](https://jmlr.org/papers/volume22/20-366/20-366.pdf)
- The choice of the stability measure also influences the predictive performance of the chosen compromise.

- We show here that decision-making is heavily influenced by the choice of the stability measure.

- The proposed measure is also shown to correct for under-or over-estimation of stability in feature spaces with groups of highly correlated variables.

- While most measures proposed in the literature study the stability of feature subsets, we incorporate into the stability value the selected features importance in predictive models.




# [LassoNet: A Neural Network with Feature Sparsity](https://jmlr.org/papers/volume22/20-848/20-848.pdf)
- We test LassoNet on a variety of datasets, and find that it generally outperforms state-of-the-art methods for feature selection and regression.

- We now describe the problem of global feature selection.

- We propose a new approach that extends Lasso regression and its feature sparsity to feedforward neural networks.

- The method can be implemented by adding just a few lines of code to a standard neural network.




# [Sparse Tensor Additive Regression](https://jmlr.org/papers/volume22/19-769/19-769.pdf)
- Section 2 introduces our sparse tensor additive regression model.

- In this section, we apply the STAR model to click-through rate (CTR) prediction in online advertising.

- Next, we derive a non-asymptotic error bound for the estimator from each iteration, which demonstrates the improvement of the estimation error in each update.

- Section 3 develops an efficient penalized alternating minimization algorithm for model estimation.




# [Asymptotic Normality, Concentration, and Coverage of Generalized Posteriors](https://jmlr.org/papers/volume22/20-469/20-469.pdf)
- Here, we provide sufficient conditions for π n to exhibit concentration, asymptotic normality, and an asymptotically correct Laplace approximation.

- Thus, in Theorem 8, we provide simple conditions under which a generalized posterior has correct frequentist coverage, asymptotically.

- In this section, we provide sufficient conditions for concentration, asymptotic normality, and the Laplace approximation for a large class of pseudolikelihood-based posteriors.

- In this article, we provide new theoretical results on the asymptotic validity of generalized posteriors.




# [Statistical guarantees for local graph clustering](https://jmlr.org/papers/volume22/20-029/20-029.pdf)
- We compare the performance of 1 -regularized PageRank with state-of-the-art local graph clustering algorithms.

- In this paper, we have examined the 1 -regularized PageRank optimization problem for local graph clustering.

- A.3 1 -regularized PageRank restricted to target cluster

- First, we give a guarantee on the recovery of the target cluster for the optimal solution (12).




# [Convex Clustering: Model, Theoretical Guarantee and Efficient Algorithm](https://jmlr.org/papers/volume22/18-694/18-694.pdf)
- 1. We prove the perfect recovery guarantee of the general weighted convex clustering model (2) under mild sufficient conditions.

- However, to the best of our knowledge, no theoretical recovery guarantee has been established for the general weighted convex clustering model (2).

- In this paper, we established the theoretical recovery guarantee for the general weighted convex clustering model, which includes many popular setting as special cases.

- The above theorem has established the theoretical recovery guarantee for the general weighted convex clustering model (2).




# [NEU: A Meta-Algorithm for Universal UAP-Invariant Feature Representation](https://jmlr.org/papers/volume22/18-803/18-803.pdf)
- Many feature maps can impede the universal approximation property (UAP) of F .

- This paper introduces Non-Euclidean Upgrading (NEU), a meta-algorithm that incorporates a linearizing preprocessing step into (L) as summarized in Meta-Algorithm 1.

- NEU does this by training a new deep neural model type, called the reconfiguration network and denoted by Φ :d , whose members form a universal class of regular feature maps.

- Namely, the latter always represents the input space as an embedded topological submanifold of the feature space R F whereas the former typically does not.




# [LocalGAN: Modeling Local Distributions for Adversarial Response Generation](https://jmlr.org/papers/volume22/20-052/20-052.pdf)
- This paper aims at presenting a specific adversarial training schema for neural response generation.

- This observation indicates that modeling the local distribution of response clusters is potentially valuable.

- To address this issue, we proposed to model the local distribution of the queries and its responses in the semantic space by adopting energy-based function, and found the approximation of this function.

- Therefore, reasonably describing the local distribution of the responses to a given query is highly necessary.




# [GemBag: Group Estimation of Multiple Bayesian Graphical Models](https://jmlr.org/papers/volume22/19-1006/19-1006.pdf)
- Moreover, implementations of existing Bayesian methods for multiple graphical models have severe computational limitations.

- In contrast, the theoretical properties we establish for our prior specification help us show that GemBag procedure enjoys optimal theoretical properties in terms of 8 norm estimation accuracy and correct recovery of the graphical structure (see Section 3).

- In the proposed hierarchical model, we allow the GGMs to have similar but different sparsity structures and heterogeneous signal magnitudes.

- Our theoretical results show that the MAP estimators have an optimal rate of convergence in 8 norm under a general setting where the graphical models may have different sparsity structures and signal strength.




# [A Unified Framework for Spectral Clustering in Sparse Graphs](https://jmlr.org/papers/volume22/20-261/20-261.pdf)
- This article provides a critical analysis of the current state-of-the-art spectral algorithms for community detection in sparse graphs.

- The problem of estimating the number of communities in an unsupervised manner is in general non-trivial.

- Let us now discuss how Claim 1 can be exploited in practice to obtain an efficient spectral clustering algorithm for sparse graphs with a heterogeneous degree distribution.

- Several contributions have thus tackled the challenging problem of devising efficient spectral clustering in the sparse regime and for an arbitrary degree distribution.




# [When random initializations help: a study of variational inference for community detection](https://jmlr.org/papers/volume22/19-630/19-630.pdf)
- When the parameters are known, we show conditions under which random initializations can converge to the ground truth.

- In this paper, we work with the BCAVI mean field variational algorithm for a simple two class stochastic blockmodel with equal sized classes.

- For simplicity, we work with equal-sized two-class stochastic blockmodels.

- In this paper, we will focus on the community detection problem in networks under SBM.




# [Homogeneity Structure Learning in Large-scale Panel Data with Heavy-tailed Errors](https://jmlr.org/papers/volume22/19-1018/19-1018.pdf)
- We propose a data-driven procedure to robustly estimate the interactive effects model and learn the homogeneity structure.

- We model large-scale panel data with an interactive effects model where both covariates and errors are influenced by some latent factors.

- In this paper, we propose to relax the sub-Gaussian assumption in panel data analysis.

- Instead of assuming a global attribute that may lead to model misspecification or individual attributes that create too many free parameters, we propose to learn a parsimonious yet flexible homogeneity structure in coefficients.




# [Understanding Recurrent Neural Networks Using Nonequilibrium Response Theory Soon Hoe Lim](https://jmlr.org/papers/volume22/20-620/20-620.pdf)
- (2) Building on our understanding in (1), we identify a universal feature, called the response feature, potentially useful for learning temporal series.

- In particular, we have shown, via a representer theorem, that SRNNs can be viewed as kernel machines operating on a reproducing kernel Hilbert space associated with the response feature.

- We then show that SRNNs, with only the weights in the readout layer optimized and the weights in the hidden layer kept fixed and not optimized, are essentially kernel machines operating in a reproducing kernel Hilbert space associated with the response feature (see Theorem 4.4).

- One can view SRNNs, with only the weights in the readout layer optimized and the weights in the hidden layer kept fixed and not optimized, as kernel machines operating on a RKHS associated with the response feature.




# [Stochastic Proximal Methods for Non-Smooth Non-Convex Constrained Sparse Optimization *](https://jmlr.org/papers/volume22/20-287/20-287.pdf)
- To the best of our knowledge, we have presented the first non-asymptotic convergence bounds for this class of objective function.

- We proposed a new measure of convergence, the subdifferential mapping, and presented two stochastic proximal gradient algorithms.

- Non-asymptotic convergence bounds were first achieved in (Ghadimi et al., 2016).

- We are not aware of any other works proving non-asymptotic convergence bounds for our general problem setting.




# [TensorHive: Management of Exclusive GPU Access for Distributed Machine Learning Workloads Pawe l Rościszewski](https://jmlr.org/papers/volume22/20-225/20-225.pdf)
- TensorHive is an open-source tool that facilitates efficient coordination of exclusive access to computing resources for machine learning workloads by introducing a hybrid mechanism of reservations and job queuing.

- In the face of the spectacular improvements in many practical applications introduced by deep learning (LeCun et al., 2015), both research and engineering teams all over the world are equipped with computing accelerators such as GPUs for neural network training.

- In this paper we propose TensorHive, an open-source GPU management tool that combines exclusive GPU reservations with job execution and monitoring, while focusing on user-friendliness, simple configuration and support for frameworks and scenarios typical for high performance machine learning.

- Supported platforms: TensorHive is a package for Python v3.5+, developed and tested on various distributions of GNU/Linux including Debian Buster and Ubuntu 18.04, both on the hosting and the monitored nodes.




# [Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)](https://jmlr.org/papers/volume22/20-303/20-303.pdf)
- The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process.

- In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research.

- It seems a code submission policy based on volun-tary participation is sufficient at this time.

- More recently, ICML 2019, the second largest international conference in machine learning has also rolled-out an explicit code submission policy (ICML, 2019).




# [Variance Reduced Median-of-Means Estimator for Byzantine-Robust Distributed Inference](https://jmlr.org/papers/volume22/20-950/20-950.pdf)
- The MOM estimator is computationally efficient and robust against Byzantine failures.

- The goal of this paper is to propose a communication-efficient statistical inference method, which is robustly against Byzantine failures.

- To the best of our knowledge, this is the first asymptotic normality result in the setting of Byzantine-robust distributed learning.

- Our estimator is called variance reduced median-of-means (VRMOM) estimator.




# [Analyzing the discrepancy principle for kernelized spectral filter learning algorithms](https://jmlr.org/papers/volume22/20-358/20-358.pdf)
- We now turn to a modification of the discrepancy principle based on the smoothing of the residuals that is, on the smoothed empirical risk.

- In this section, we analyze the discrepancy principle conditional on the design.

- On the other hand, these finite-sample bounds from the random design case lead to establish that: (i) the classical discrepancy principle is statistically adaptive for slow rates occurring in the hard learning scenario (called outer case), and (ii) the smoothing-based discrepancy principles are adaptive over ranges of higher smoothness parameters (called inner case).

- The early stopping rule based on the discrepancy principle (DP) is then introduced and motivated in Section 2.4.




# [On ADMM in Deep Learning: Convergence and Saturation-Avoidance](https://jmlr.org/papers/volume22/20-1006/20-1006.pdf)
- • Methodology Novelty: We develop a novel sigmoid-ADMM pair for deep learning.

- Specifically, the performance of the proposed ADMM is slightly better than SGD type methods.

- In Theorem 5, we only give the convergence guarantee for the proposed ADMM.

- This demonstrates that the proposed ADMM is stable to the initial scheme.




# [Tighter Risk Certificates for Neural Networks](https://jmlr.org/papers/volume22/20-879/20-879.pdf)
- We elaborate more on the use of PAC-Bayes bounds for model selection in the next subsection.

- In this paper we experiment with probabilistic neural networks from a PAC-Bayes approach.

- We show now that PAC-Bayes bounds can be used not only as training objectives to guide the optimisation algorithm but also for model selection.

- 1. We rigorously study and illustrate 'PAC-Bayes with Backprop' (PBB), a generic strategy to derive (probabilistic) neural network training methods from PAC-Bayes bounds.




# [Hyperparameter Optimization via Sequential Uniform Designs](https://jmlr.org/papers/volume22/20-058/20-058.pdf)
- In Figure 1, a sequential uniform design (SeqUD) approach is proposed for such HPO-type of computer experiment.

- In summary, this paper contributes to the HPO and AutoML literature in the following three aspects: • We develop a novel AugUD algorithm for the efficient augmentation of uniform design points.

- A real-time AugUD algorithm is introduced for fast construction of augmented uniform designs.

- • The SeqUD strategy generalizes Bayesian optimization from one-point-at-a-time to batch-by-batch.




# [Learning and Planning for Time-Varying MDPs Using Maximum Likelihood Estimation](https://jmlr.org/papers/volume22/20-006/20-006.pdf)
- • Section 5 uses the proposed notion of uncertainty to propose optimal learning and control policies for an agent operating in an unknown, time-varying environment.

- -Theorem 10 relates the proposed measure to measures of uncertainty used in learning and planning techniques for time-invariant MDPs.

- Learning and planning for agents operating in the last two frameworks have been discussed at length (Sutton et al., 1999;Szita et al., 2002).

- The CCMLE problem for general online learning and planning, with all the questions opened in this paper, thus remains largely open.




# [giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration](https://jmlr.org/papers/volume22/20-325/20-325.pdf)
- This makes giotto-tda the most comprehensive Python library for topological machine learning and data exploration to date.

- Topological data analysis (TDA) uses tools from algebraic and combinatorial topology to extract features that capture the shape of data (Carlsson, 2009).

- Facilitating these processes is one of the reasons why giotto-tda maintains and extends compatibility with the scikit-learn API.

- The library appears in scikit-learn's curated list of related projects.




# [On Multi-Armed Bandit Designs for Dose-Finding Clinical Trials](https://jmlr.org/papers/volume22/19-228/19-228.pdf)
- Hence finite-time upper bounds on the number of sub-optimal selection lead to non-asymptotic upper bound on the error probability of the design.

- Our second contribution is to show that Thompson Sampling using more sophisticated prior distributions can compete with state-of-the art dose-finding algorithms.

- In Section 3, we propose an analysis of Thompson Sampling with independent Beta priors on the toxicity of each dose: We provide finite-time upper-bounds on the number of sub-optimal selections, which match an (asymptotic) lower bound on those quantities.

- In this paper, we investigate the use of Thompson Sampling (Thompson, 1933) for dose-finding clinical trials.




# [L-SVRG and L-Katyusha with Arbitrary Sampling](https://jmlr.org/papers/volume22/20-156/20-156.pdf)
- We establish the connection between these expected smoothness parameters and ESO , which allows us the explore the sparsity of data as well.

- It is possible to estimate these expected smoothness parameters under weaker conditions.

- In this subsection, we give the estimations of these expected smoothness parameters under the ESO inequality.

- (28 ) First we give the estimations of these expected smoothness parameters for arbitrary set sampling S.




# [Asynchronous Online Testing of Multiple Hypotheses](https://jmlr.org/papers/volume22/19-910/19-910.pdf)
- This immediately gives two procedures for asynchronous online testing as special cases of LORD* and SAFFRON*. From here forward we will refer to these methods as LORD async and SAFFRON async , respectively.

- In this work, we reinforce this connection between asynchronous online testing and dependence by developing a general abstract framework in which, from an algorithmic point of view, these two issues are treated Figure 1: Testing five hypotheses synchronously (top) and asynchronously (bottom).

- We have presented a unified framework for the design and analysis of online FDR procedures for asynchronous testing, as well as testing locally dependent p-values.

- Substituting X t for X t async in Algorithms 1-4 yields procedures for asynchronous online FDR control.




# [Stochastic Online Optimization using Kalman Recursion](https://jmlr.org/papers/volume22/20-618/20-618.pdf)
- We derive local bounds on the cumulative risk with high probability from a martingale analysis.

- S(δ) is the cumulative excess risk of the convergence phase.

- In Section 3.2 we compare the excess risk with its second-order expansion thanks to Assumption 5, and we use a martingale analysis to obtain a bound on the cumulative excess risk.

- Note that the dependence of the cumulative excess risk of the convergence phase in terms of δ is O(log(δ −1 ) 3 ).




# [Consistency of Gaussian Process Regression in Metric Spaces](https://jmlr.org/papers/volume22/21-0853/21-0853.pdf)
- In this paper, we made a crucial first step towards the theoretical legitimization of using GP regression on non-Euclidean manifolds and other metric domain spaces, as has already been done by various authors.

- More precisely, we provide a proof for the consistency of GP regression of an unknown, real-valued function f 0 , whose domain is a separable metric space (T, ρ).

- Gaussian Process (GP) regression (Rasmussen and Williams, 2006, Chapter 2) is an established tool for nonparametric modelling of real-world phenomena.

- Finally, this work aimed to establish asymptotic consistency of GP regression under as weak as possible assumptions on the observed function and the GP prior, allowing us to provide results for a large class of domains and kernels.




# [Knowing what You Know: valid and validated confidence sets in multiclass and multilabel prediction](https://jmlr.org/papers/volume22/20-753/20-753.pdf)
- We refer to the procedure (3) as the Marginal conformal prediction method.

- Our main motivation is to design methods with more robust conditional coverage than the "marginal" split-conformal method (3).

- The randomized oracle method provides exact 1−α conditional coverage, but CQC appears to provide more robust coverage for individual classes than the marginal method.

- CQC is similar to Romano et al. (2019)'s Conformalized Quantile Regression (CQR): we estimate a quantile function of the scores, which we use to construct valid confidence sets after conformalization.




# [DIG: A Turnkey Library for Diving into Graph Deep Learning Research](https://jmlr.org/papers/volume22/21-0343/21-0343.pdf)
- For each direction, DIG provides unified and extensible implementations of data interfaces, common algorithms, and evaluation metrics.

- Deep learning on 3D graphs.

- We currently consider several research directions in graph deep learning.

- These are graph generation, selfsupervised learning on graphs, explainability of graph neural networks, and deep learning on 3D graphs.




# [DeEPCA: Decentralized Exact PCA with Linear Convergence Rate](https://jmlr.org/papers/volume22/21-0298/21-0298.pdf)
- (15 ) Thus, DeEPCA achieves the best communication complexity of decentralized PCA algorithms.

- We summarize our contributions as follows: 1. We propose a novel power-iteration based decentralized PCA called DeEPCA, which can achieve the best known communication complexity.

- In this way, DeEPCA can achieve the best known communication complexity for decentralized PCA.

- The subspace tracking technique in our algorithm is the key to achieving the advantages of DeEPCA.




# [Differentially Private Regression and Classification with Sparse Gaussian Processes](https://jmlr.org/papers/volume22/19-017/19-017.pdf)
- We will first introduce Gaussian processes and differential privacy.

- In summary, we have presented three tools to extend the use of differential privacy for Gaussian process models.

- The paper is organised as follows: In Section 2 we introduce Gaussian processes for regression, differential privacy and the cloaking method, before briefly discussing the wider context of differential privacy and non-parametric methods.

- In the current paper we suggest solutions to some of that method's shortcomings, specifically we propose non-stationary covariance functions to reduce the impact of outliers, and develop a method for using the Laplace approximation to perform differentially private classification.




# [Pykg2vec: A Python Library for Knowledge Graph Embedding](https://jmlr.org/papers/volume22/19-433/19-433.pdf)
- Pykg2vec is a Python library with extensive documentation that includes the implementations of a variety of state-of-the-art Knowledge Graph Embedding methods and modular building blocks of the embedding pipeline.

- (a) Provide access to the latest and state-of-the-art KGE implementations.

- In recent years, Knowledge Graph Embedding (KGE) has become an active research area and many authors have provided reference software implementations.

- (c) Deliver a modular and flexible software architecture and KGE pipeline that is both educational and of practical use for researchers.




# [Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives](https://jmlr.org/papers/volume22/19-1049/19-1049.pdf)
- We also established new estimation error bounds for a class of 0 -regularized classification problems and showed that these bounds compare favorably with the best-known bounds for 1 regularization.

- Our approximate algorithms are based on coordinate descent and local combinatorial search.

- To this end, we propose a new MIP-based algorithm that we call "integrality generation", which allows for solving instances of Problem (2) with p ≈ 50, 000 (where n is small) to optimality within a few minutes.

- Compared to regression, there has been limited work in deriving estimation error bounds for classification tasks.




# [A General Framework for Empirical Bayes Estimation in Discrete Linear Exponential Family](https://jmlr.org/papers/volume22/19-873/19-873.pdf)
- In this paper we propose a Nonparametric Empirical Bayes framework for compound estimation in the discrete linear exponential family.

- This article develops a general non-parametric empirical Bayes (NEB) framework for compound estimation in discrete models.

- This section describes the proposed NEB framework for compound estimation in discrete models.

- This article develops a general framework for empirical Bayes estimation for the discrete linear exponential (DLE) family, also known as the family of discrete power series distributions (Noack, 1950), under both regular and scaled squared error losses.




# [On the Optimality of Kernel-Embedding Based Goodness-of-Fit Tests](https://jmlr.org/papers/volume22/17-570/17-570.pdf)
- In this paper, we investigated the performance of kernel embedding based approaches to goodness-of-fit testing from a minimax perspective.

- In particular, we focus on kernel embedding based goodness-of-fit tests and investigate their power under a general composite alternative.

- To complement the earlier theoretical development, we also performed several sets of simulation experiments to demonstrate the merits of the proposed adaptive test.

- A particularly attractive approach to goodness-of-fit testing problems in general domains is through RKHS embedding of distributions.




# [Interpretable Deep Generative Recommendation Models](https://jmlr.org/papers/volume22/20-1098/20-1098.pdf)
- InDGRM preference similarity and intra-user preference diversity has ability to significantly improve model performance.

- In this paper, we propose an interpretable deep generative method (InDGRM) for recommendation by modeling both inter-user preference similarity and intra-user preference diversity to achieve disentanglement from both observed-level and latent-level.

- The second module is to characterize the intra-user preference diversity and achieve observed-level disentanglement on items via item prototype learning technique.

- In this section, an Interpretable Deep Generative Recommendation Model (InDGRM) will be presented by investigating user behaviors from the views of inter-user preference similarity and intra-user preference diversity.




# [Bayesian Text Classification and Summarization via A Class-Specified Topic Model](https://jmlr.org/papers/volume22/18-332/18-332.pdf)
- In this paper, we propose the class-specified topic model (CSTM) as another extension of LDA.

- A two-stage approach can also be used to obtain class-specific text summarization.

- Topics extracted under each class can be used to obtain class-specific text summarization.

- Section 4 develops Bayesian inference of CSTM in the semisupervised scenario, with the supervised scenario as a special case.




# [The Ridgelet Prior: A Covariance Function Approach to Prior Specification for Bayesian Neural Networks](https://jmlr.org/papers/volume22/20-1300/20-1300.pdf)
- • Finite Sample-Size Error Bounds: Approximation error bounds are obtained which are valid for a finite-dimensional parametrisation of the network, as opposed to relying on asymptotic results such as a central limit theorem.

- Consider the neural network in (1).

- A proof-of-concept empirical assessment is contained Section 6.

- It is clear that (7) closely resembles one layer of a neural network; c.f. (1).




# [Domain Generalization by Marginal Transfer Learning](https://jmlr.org/papers/volume22/17-679/17-679.pdf)
- We propose two data generation models.

- In this section we formally define domain generalization via two possible data generation models together with associated notions of risk.

- We also provide a basic generalization error bound for the first of these data generation models.

- 1 To our knowledge, the problem of domain generalization was first proposed and studied by our earlier conference publication (Blanchard et al., 2011) which this work extends in several ways.




# [A Unified Convergence Analysis for Shuffling-Type Gradient Methods](https://jmlr.org/papers/volume22/20-1238/20-1238.pdf)
- We have conducted an intensive convergence analysis for a wide class of shuffling-type gradient methods for solving a finite-sum minimization problem.

- In this paper, we develop a new and unified convergence analysis framework for general shuffling-type gradient methods to solve (P) and apply it to different shuffling variants in both nonconvex and strongly convex settings under standard assumptions.

- There exists no unified analysis that can cover a wide class of shuffling-type gradient algorithms under different assumptions ranging from strongly convex to nonconvex cases.

- • Finally, prior to our work, convergence analysis of shuffling-type gradient schemes has not been rigorously investigated for the nonconvex setting of (P).




# [Collusion Detection and Ground Truth Inference in Crowdsourcing for Labeling Tasks](https://jmlr.org/papers/volume22/19-373/19-373.pdf)
- To address these issues, in the next subsection, we propose a penalized pairwise profile likelihood method for collusion detection.

- The next theorem verifies the consistency of the penalized pairwise profile likelihood method.

- Next, we consider the asymptotic properties of the penalized pairwise profile likelihood estimation method.

- To derive the pairwise profile likelihood




# [Histogram Transform Ensembles for Large-scale Regression](https://jmlr.org/papers/volume22/19-1004/19-1004.pdf)
- In this paper, we propose a randomized ensemble algorithm named histogram transform ensembles (HTE) for large-scale regression problems.

- By conducting a statistical learning treatment, this paper studies the large-scale regression problem with histogram transform estimators.

- Thus, we turn to apply the kernel histogram transform (KHT) which is verified to have almost optimal convergence rates.

- Secondly, almost optimal convergence rates are verified within the regularized empirical risk minimization framework for our histogram transform estimators in the sense of different space C k,α .




# [Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning](https://jmlr.org/papers/volume22/20-410/20-410.pdf)
- • 5+1 Phases of Regularization.

- Based on this, in Section 5, we present our main theory of 5+1 Phases of Training.

- These observations demonstrate that modern, state-of-the-art DNNs exhibit a new form of Heavy-Tailed Self-Regularization.

- • Exhibiting the 5+1 Phases.




# [mlr3pipelines -Flexible Machine Learning Pipelines in R](https://jmlr.org/papers/volume22/21-0281/21-0281.pdf)
- But real-world applications often require complex combinations of ML (pre-) processing steps, which can be expressed as a directed acyclic graph (DAG); we will call such graphs ML pipelines or ML workflows.

- The DAGs in mlr3pipelines go beyond simple combinations of preprocessing and ML models.

- mlr3pipelines and the mlr3 ecosystem are integrated with each other, so that mlr3's Learners can be used as PipeOps and Graphs adhere to the same interface as mlr3 learners and can be, for example, resampled and tuned just like any other Learner.

- They support ensemble models and conditional branching that can be represented explicitly as part of the graph structure.




# [dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python](https://jmlr.org/papers/volume22/20-1473/20-1473.pdf)
- In Figure 1, we present the architecture of a unified interface for model-agnostic responsible machine learning with interactive explainability and fairness.

- This contribution builds upon the software for explainable machine learning presented by us in "DALEX: Explainers for Complex Predictive Models in R" (Biecek, 2018).

- Based on these experiences, we implemented a Python package.

- In this article, we present dalex, which builds upon and extends the DALEX R package to bring a unified interface for responsible machine learning into Python.




# [Understanding How Dimension Reduction Tools Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data Visualization](https://jmlr.org/papers/volume22/20-1061/20-1061.pdf)
- We now formally introduce the Pairwise Controlled Manifold Approximation Projection (PaCMAP) method.

- This new loss function is used in an algorithm called Pairwise Controlled Manifold Approximation Projection (PaCMAP) introduced in this work.

- First, one could consider the possibility of a continuum between local and global structure.

- PaCMAP preserves both local and global structure, owing to a dynamic choice of graph components over the course of the algorithm.




# [Sparse Convex Optimization via Adaptively Regularized Hard Thresholding](https://jmlr.org/papers/volume22/20-661/20-661.pdf)
- In this work we present a new algorithm called Adaptively Regularized Hard Thresholding (ARHT), that closes the longstanding gap between the O s * κ f (x 0 )−f (x * ) and O s * κ 2 bounds by getting a sparsity of O(s * κ) and thus achieving the best of both worlds.

- Our algorithm is essentially a hard thresholding algorithm (and more specifically OMPR, also known as PHT(1)) with the crucial novelty that it is applied on an adaptively regularized objective function.

- The algorithms that we will consider are LASSO, Orthogonal Matching Pursuit (OMP), Orthogonal Matching Pursuit with Replacement (OMPR), Adaptively Regularized Hard Thresholding (ARHT) (Algorithm 6), and Exhaustive Local Search (Algorithm 4).

- Importantly, previous results imply that our bound is tight up to constants for a general class of algorithms, including Greedy-type algorithms and LASSO (Foster et al., 2015).




# [Mixing Time of Metropolis-Hastings for Bayesian Community Detection](https://jmlr.org/papers/volume22/18-770/18-770.pdf)
- To the best of our knowledge, ( 22) is the first explicit upper bound on the mixing time of the Markov chain for Bayesian community detection.

- Then, we present the posterior strong consistency result.

- (14 ) We perform the analysis of mixing time for the Markov chain {Γ t } t≥0 .

- There exist several obstacles associated with the mixing time analysis of MCMC in Bayesian high-dimensional models.




# [Individual Fairness in Hindsight](https://jmlr.org/papers/volume22/19-658/19-658.pdf)
- Formally, we design an algorithm that we call Cautious Fair Exploration (CaFE), which is individually fair in hindsight and attains sub-linear regret guarantees as compared to the optimal individually-fair benchmark in a wide range of settings.

- Individual Fairness in Hindsight

- Formally, we propose an online learning algorithm that we call Cautious Fair Exploration or CaFE in Algorithm 1.

- This ensures fairness-in-hindsight.




# [Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach](https://jmlr.org/papers/volume22/19-707/19-707.pdf)
- of the Safe Policy Iteration (SPI) methods

- 5. SSPI was called Multiple-parameter Safe Policy Improvement (MSPI) in Pirotta et al. (2013b).

- Following the approach proposed in CPI, USPI iteratively updates the current policy using a safe policy improvement.

- , we define the update rule of the policy improvement step as: π = απ + (1 − α)π, where α ∈ [0, 1] is the scalar trade-off coefficient.




# [Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data](https://jmlr.org/papers/volume22/20-1194/20-1194.pdf)
- 6. We demonstrate the applicability of the directional mean shift algorithm by using it as a clustering method on both simulated and real-world data sets (Section 6).

- Section 5 considers the computational learning theory of the directional mean shift algorithm; we study the ascending and converging properties of the algorithm.

- In this section, we present our experimental results of the directional mean shift algorithm on both simulated and real-world data sets.

- 4. With regard to computational learning theory, we prove the ascending and converging properties of the directional mean shift algorithm (Theorems 8 and 11).




# [Learning Whenever Learning is Possible: Universal Learning under General Stochastic Processes](https://jmlr.org/papers/volume22/17-298/17-298.pdf)
- We begin the discussion of universally consistent online learning with the subject of concisely characterizing the family of processes SUOL.

- In contrast, we find that the family of processes admitting the existence of universally consistent online learning rules forms a strict superset of these other two families.

- As for the problem of concisely characterizing the family of processes that admit strong universal online learning, again the present work only makes partial progress.

- In particular, this implies that for unbounded losses, there exist optimistically universal (inductive/self-adaptive/online) learning rules, so that Theorem 51 immediately follows.




# [Generalization Properties of hyper-RKHS and its Applications](https://jmlr.org/papers/volume22/19-482/19-482.pdf)
- Hence, we characterize a kernel learning framework in this space for kernel learning and out-of-sample extensions.

- In this paper, we generalize two regularized regression problems in hyper-RKHS, illustrates its utility for kernel learning and out-of-sample extensions, and proves asymptotic convergence results for the introduced regression models in an approximation theory.

- In this paper, we have studied the generalization properties of regularized regression models in hyper-RKHS.

- In particular, we make the following contributions: Algorithmically, in Section 2, motivated by Ong et al. (2005), we consider regularized regression problems with squared loss and ε-insensitive loss (i.e., KRR and SVR) in hpyer-RKHS for kernel learning and out-of-sample extensions.




# [Entangled Kernels -Beyond Separability](https://jmlr.org/papers/volume22/19-665/19-665.pdf)
- We now define the two novel classes of operator-valued kernels.

- Some well-known classes of operator-valued kernels include separable and transformable kernels.

- For the first step of kernel learning, we propose a novel definition of alignment between an operator-valued kernel and labels of a multi-output learning problem.

- Finally we compare the running times of learning with various classes of operator-valued kernels.




# [Consistent estimation of small masses in feature sampling](https://jmlr.org/papers/volume22/18-534/18-534.pdf)
- That is, there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass P n,0 .

- In Section 3 we prove that, under the Bernoulli product model, there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass M n,0 .

- That is, Pn,0 is not a universally consistent estimator, in the multiplicative sense, of the missing mass P n,0 .

- As an extension of the main result of Mossel and Ohannessian (2019) to the feature sampling framework, we first show that there do not exist universally consistent estimators, in the multiplicative sense, of the missing mass M n,0 .




# [A Probabilistic Interpretation of Self-Paced Learning with Applications to Reinforcement Learning](https://jmlr.org/papers/volume22/21-0112/21-0112.pdf)
- We have presented an interpretation of self-paced learning as inducing a sampling distribution over tasks in a reinforcement learning setting when using the KL divergence w.r.t. a target distribution µ(c) as a self-paced regularizer.

- This section introduces the necessary notation for both self-paced and reinforcement learning.

- In this paper, we develop an interpretation of self-paced learning as the process of generating a sequence of distributions over samples.

- To make this intuition rigorous, we now introduce a probabilistic view on self-paced learning that views the weights ν in the SPL objective (1) as probabilities of a distribution over samples.




# [An algorithmic view of 2 regularization and some path-following algorithms](https://jmlr.org/papers/volume22/19-477/19-477.pdf)
- Moreover, we proposed various new path-following algorithms to approximate the 2 -regularized solution path.

- We first establish an equivalence between 2 -regularized solution path for a convex loss function, and the solution of an ODE.

- In this subsection, we propose a path following algorithm based on Newton update over a set of grid points.

- Typically, Newton method is used as the "working horse" for the modern path-following interior point methods.




# [Approximate Newton Methods](https://jmlr.org/papers/volume22/19-870/19-870.pdf)
- Accordingly, we propose a general result for analysis of both local and global convergence properties of second order methods.

- In Section 3 we present a unifying framework for local and global convergence analysis of second order methods.

- In this paper we have proposed a framework to analyze both local and global convergence properties of second order methods including stochastic and deterministic versions.

- We summarize our contribution as follows: • We propose a unifying framework (Theorem 3 and Theorem 5) to analyze local and global convergence properties of second order methods including stochastic and deterministic versions.




# [Policy Teaching in Reinforcement Learning via Environment Poisoning Attacks *](https://jmlr.org/papers/volume22/20-1329/20-1329.pdf)
- We studied a security threat to reinforcement learning (RL) where an attacker poisons the environment, thereby forcing the agent into executing a target policy.

- We propose a general optimization framework for environment poisoning; our theoretical analysis provides technical conditions which ensures the attacker's success and gives lower/upper bounds on the attack cost.

- We instantiate our attacks in both the offline and online settings with appropriate notions of attack cost.

- (5) Cost of the attack.




# [Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks](https://jmlr.org/papers/volume22/21-0366/21-0366.pdf)
- Model sparsity is thus often trained with a pruning schedule.

- In biological brains, model sparsity is one important component.

- 5. Combined ephemeral and model sparsity Any sparse deep neural network should combine both ephemeral and model sparsity.

- All those architectures are designed for the relatively modest sparsity in today's deep neural networks.




# [FATE: An Industrial Grade Platform for Collaborative Learning With Data Protection](https://jmlr.org/papers/volume22/20-815/20-815.pdf)
- In this paper, we introduced the first industrial-strength federated learning platform FATE.

- At its core, FATE is built on a library of federated and privacy-preserving machine learning algorithms, called Federat-edML.

- FATE (Federated AI Technology Enabler) is the first production-oriented platform developed by Webank's AI Department.

- FATE also supports cross-cloud deployment and management through FATE-cloud.




# [Statistical Query Lower Bounds for Tensor PCA](https://jmlr.org/papers/volume22/20-837/20-837.pdf)
- Our analysis revealed that the optimal sample complexity in the SQ model depends on whether ET is symmetric or not.

- In this paper, we studied the Tensor PCA problem in the Statistical Query model.

- In this section we describe SQ procedures that achieve the optimal sample complexity for the Tensor PCA testing and estimation problems.

- To implement these estimators in the SQ model, we leverage on the following result.




# [Benchmarking Unsupervised Object Representations for Video Sequences](https://jmlr.org/papers/volume22/21-0199/21-0199.pdf)
- Hence, in this work, we propose a benchmark based on procedurally generated video sequences to test basic perceptual abilities of object-centric video models under various challenging tracking scenarios.

- Spatial attention models with unconstrained latent representations use perobject variational autoencoders, as introduced by .

- Recently, there has been an increased interest in unsupervised learning of object-centric representations.

- Moving forward, occlusion handling is a key component that object-centric video models need to master.




# [Analysis of high-dimensional Continuous Time Markov Chains using the Local Bouncy Particle Sampler](https://jmlr.org/papers/volume22/18-651/18-651.pdf)
- The key contributions of this paper are as follows: • Efficient algorithms to simulate the bouncing time for each factor of the factorized posterior density of CTMCs to boost the computational efficiency.

- efficiency of the algorithm.

- • A proof-of-concept application on protein evolution to demonstrate on real data the computational efficiency of LBPS compared to state-of-the-art HMC algorithms.

- Conservative bounds can lead to computational inefficiency of the algorithm.




# [Some Theoretical Insights into Wasserstein GANs](https://jmlr.org/papers/volume22/20-553/20-553.pdf)
- In the present paper, we make one step further in the analysis of mathematical forces driving WGANs and contribute to the literature in the following ways: (i) We properly define the architecture of WGANs parameterized by neural networks.

- (iv) Building upon the above, we clarify the adversarial effects of the generator and the discriminator by underlining some trade-off properties.

- The present section is devoted to the presentation of the WGANs framework.

- Therefore, the proposal of is to replace the 1-Lipschitz functions with a discriminator parameterized by neural networks.




# [Communication-Efficient Distributed Covariance Sketch, with Application to Distributed PCA](https://jmlr.org/papers/volume22/20-705/20-705.pdf)
- We also improve the communication cost for the distributed PCA problem.

- We exhibit new algorithms for distributed covariance sketch with improved communication costs.

- Based on our new distributed covariance sketch algorithm, we give an improved communication bound in this setting as well.

- In this section, we show how to use our distributed sketching algorithm to obtain improved communication bounds for distributed PCA.




# [Neighborhood Structure Assisted Non-negative Matrix Factorization and Its Application in Unsupervised Point-wise Anomaly Detection](https://jmlr.org/papers/volume22/19-924/19-924.pdf)
- In this paper, we propose a neighborhood structure-assisted non-negative matrix factorization method and demonstrate its application in anomaly detection.

- We demonstrate the benefit of the neighborhood structure-assisted NMF in the mission of anomaly detection.

- We refer to the resulting method in this paper as the neighborhood structure-assisted NMF (NS-NMF).

- Section 4 presents the proposed NS-NMF algorithm in a structured way for both offline and online versions.




# [High-Order Langevin Diffusion Yields an Accelerated MCMC Algorithm](https://jmlr.org/papers/volume22/20-576/20-576.pdf)
- In particular, while the mixing time scales as O( √ d), as with lower-order methods, we show that the ε-dependency term can be adaptive to the degree of smoothness of the function U .

- For the second step, we constructed a third-order Langevin dynamics that has smoother trajectories, and for which the integration of ∇U can be separated from the Brownian motion part.

- We proved a mixing time of order O d 1/4 /ε 1/2 for ridge-separable potentials, which cover a large class of machine learning models.

- However, after discretization using the Euler scheme, the resulting algorithm-a discrete-time stochastic process-has a mixing rate that scales as O(d)




# [Hybrid Predictive Models: When an Interpretable Model Collaborates with a Black-box Model](https://jmlr.org/papers/volume22/19-325/19-325.pdf)
- Finally, the current models only work on structured data and text data.

- We proposed a novel Hybrid Predictive Model that integrates an interpretable model with any black-box model for classification.

- We apply both models to structured datasets and text data, where the interpretable models partially substitute state-of-the-art black-box models, including ensembles and neural networks.

- Thus a Pareto frontier captures the natural trade-off obtained by hybrid models between transparency and accuracy.




# [Probabilistic Iterative Methods for Linear Systems](https://jmlr.org/papers/volume22/21-0031/21-0031.pdf)
- • We study application of probabilistic iterative methods to a toy regression problem.

- It is unknown whether probabilistic iterative methods based on CG are strongly or weakly calibrated.

- The contributions of this paper are therefore as follows: • We introduce probabilistic iterative methods, a class of PNM derived from iterative methods for solving linear systems such as Eq. 1.

- The aim of this section is to empirically assess our proposed probabilistic iterative methods.




# [Single and Multiple Change-Point Detection with Differential Privacy](https://jmlr.org/papers/volume22/19-770/19-770.pdf)
- In this section, we give new differentially private algorithms for change-point detection in the online setting.

- This paper gives private algorithms for both online and offline change-point detection, including the problem of detecting multiple change-points.

- Our work considers the statistical problem of change-point detection through the lens of differential privacy.

- We give private algorithms for both online and offline change-point detection, analyze these algorithms theoretically, and then provide empirical validation of these results.




# [Bayesian Distance Clustering](https://jmlr.org/papers/volume22/20-688/20-688.pdf)
- In the literature, there are two primary approaches -distance-and model-based clustering.

- There is an order difference of O(n h ) between distance-based and model-based divergences.

- Our proposed Bayesian distance clustering approach gains some of the advantages of modelbased clustering, such as uncertainty quantification and flexibility, while significantly simplifying the model specification task.

- In contrast, Bayesian distance clustering maintains high clustering accuracy.




# [GIBBON: General-purpose Information-Based Bayesian OptimisatioN](https://jmlr.org/papers/volume22/21-0120/21-0120.pdf)
- Our primary contributions are as follows: 1. We propose an approximation for a general-propose extension of MES named General-purpose Information-Based Bayesian Optimisation (GIBBON).

- Therefore, we provide the first high-performing yet computationally light-weight framework for synchronous batch BO suitable for search spaces consisting of discrete structures.

- We have presented GIBBON, a general-purpose acquisition function that extends max-value entropy search to provide computationally light-weight yet high performing optimisation for a wide range of BO problems.

- In this work, we propose a novel approximation strategy for (1) completely free from numerical integrations, thus providing the first computationally light-weight information-theoretic acquisition function for synchronous batch BO.




# [Dynamic Tensor Recommender Systems](https://jmlr.org/papers/volume22/19-792/19-792.pdf)
- In this article, we propose a new dynamic tensor recommender system which incorporates time information through a tensor-valued function.

- Specifically, we establish the convergence rate of the proposed tensor factorization and the asymptotic normality of the spline coefficient estimator.

- The following theorem establishes the convergence rate for the proposed tensor factorization.

- Theorem 2 provides the convergence rate of the proposed method given trend functions.




# [A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration](https://jmlr.org/papers/volume22/20-919/20-919.pdf)
- In addition, we also provide the perturbation bounds for tensor reconstruction.

- Next we demonstrate the unilateral perturbation bounds for mode-k singular subspace estimation.

- • In addition, we apply the new perturbation bounds of HOOI in two modern applications, tensor denoising and tensor co-clustering, from machine learning and statistics.

- In this paper, we provide the first sharp blockwise perturbation bounds of HOOI for tensors with guarantees for both tensor reconstruction and mode-k singular subspace estimation.




# [MetaGrad: Adaptation using Multiple Learning Rates in Online Learning * Tim van Erven](https://jmlr.org/papers/volume22/20-1444/20-1444.pdf)
- Then, in Section 8, we compare all versions of MetaGrad to OGD and to AdaGrad in experiments with several benchmark classification and regression data sets.

- Section 7 extends this analysis to the two other versions of MetaGrad.

- This is a standard online convex optimization task with a quadratic loss function and time-varying domain.

- We provide a new adaptive method, MetaGrad, which is robust to general convex losses but simultaneously can take advantage of special structure in the losses, like curvature in the loss function or if the data come from a fixed distribution.




# [Black-Box Reductions for Zeroth-Order Gradient Algorithms to Achieve Lower Query Complexity](https://jmlr.org/papers/volume22/20-611/20-611.pdf)
- In this paper, we develop two reduction frameworks for ZO algorithms under convex and non-convex setting, respectively.

- Moreover, our frameworks can directly derive convergence results of ZO algorithms under convex and non-convex settings without extra analyses, as long as convergence results under strongly convex setting are given.

- Thus, whether the function query complexities of ZO algorithms can be improved further

- To the best of our knowledge, we are the first to propose black-box reduction frameworks for zeroth-order algorithms and apply them to zeroth-order optimization.




# [Tsallis-INF: An Optimal Algorithm for Stochastic and Adversarial Bandits](https://jmlr.org/papers/volume22/19-753/19-753.pdf)
- We propose an algorithm that achieves logarithmic pseudoregret guarantee in the adversarial regime with a self-bounding constraint simultaneously with the adversarial regret guarantee.

- To the best of our knowledge, this is also the first evidence that Thompson Sampling is vulnerable in adversarial environments.

- The algorithm is based on online mirror descent with regularization by Tsallis entropy with power α.

- We show that it also achieves a logarithmic regret guarantee in the more general adversarial regime with a self-bounding constraint.




# [Optimal Rates of Distributed Regression with Imperfect Kernels](https://jmlr.org/papers/volume22/20-627/20-627.pdf)
- Then we conducted leave one analyses of KRR and BCKRR, which lead to sharp error bounds and capacity independent optimal rates for both approaches.

- The error bounds in Theorem 1 are sharp and the rates are capacity independent optimal.

- The primary goal of this study is to verify the capacity independent optimality of distributed kernel regression algorithms when the kernel is imperfect.

- In Section 3 we propose a general framework for the analysis of response weighted distributed regression algorithms.




# [Banach Space Representer Theorems for Neural Networks and Ridge Splines](https://jmlr.org/papers/volume22/20-583/20-583.pdf)
- Our result says that single-hidden layer neural networks are solutions to continuous-domain linear inverse problems with TV regularization in the Radon domain.

- Single-hidden layer neural networks are superpositions of ridge functions.

- We show that continuous-domain linear inverse problems with total variation regularization in the Radon domain admit sparse atomic solutions, with the atoms being the familiar neurons of a neural network.

- In other words, our main result is the derivation of a neural network representer theorem.




# [Oblivious Data for Fairness with Kernels](https://jmlr.org/papers/volume22/20-1311/20-1311.pdf)
- thanks to the plug-in approach.

- In Section 4 we study the relation between H-independence and bounds on the dependencies between oblivious and sensitive features.

- We further show how estimation errors can be controlled.

- To this end, we devise a plugin-approach.




# [Soft Tensor Regression](https://jmlr.org/papers/volume22/20-476/20-476.pdf)
- We refer to the tensor regression model that uses the soft PARAFAC for estimation of the coefficient tensor as Soft Tensor Regression (Softer).

- In this paper, we propose a soft tensor regression (Softer) framework for estimating a high-dimensional linear regression model with a tensor predictor and scalar outcome.

- Low-rank approximations to the coefficient tensor of a linear tensor regression model provide a supervised approach to estimating the relationship between a tensor predictor and a scalar outcome.

- Softer accommodates the predictor's tensor structure by basing the coefficient tensor estimation on the parallel factors approximation, similarly to other approaches in the literature.




# [Guided Visual Exploration of Relations in Data Sets](https://jmlr.org/papers/volume22/19-364/19-364.pdf)
- Contributions In summary, our contributions are: (1) a computationally efficient formulation and implementation of the user's background knowledge of the data and objectives (which we here call hypotheses) using constrained randomisation, (2) a dimensionality reduction method for finding the view most informative to the user, and (3) an experimental evaluation that supports that our approach is fast, robust, and produces easily understandable results.

- We provide an efficient implementation of this method using constrained randomisation.

- Specifically, our goal and main contribution is to devise a framework for human-guided data exploration by modelling the user's background knowledge and objectives, and using these to provide the user with the most informative views of the data.

- We next discuss the relation of our present work to existing literature on exploratory data analysis.




# [OpenML-Python: an extensible Python API for OpenML](https://jmlr.org/papers/volume22/19-920/19-920.pdf)
- While OpenML is an online platform, we facilitate offline usage as well.

- OpenML-Python allows easy interaction with OpenML from within Python.

- The OpenML platform is organized around several entity types which describe different aspects of a machine learning study.

- OpenML is a collaborative online machine learning (ML) platform, meant for sharing and building on prior empirical machine learning research (Vanschoren et al., 2014).




# [Implicit Langevin Algorithms for Sampling From Log-concave Densities](https://jmlr.org/papers/volume22/19-292/19-292.pdf)
- To establish geometric ergodicity, we prove the stronger Proposition 8 below.

- Under Assumptions 1 and 2, we can now establish geometric ergodicity of the θ-method scheme under certain conditions on θ and h (Theorem 1).

- Then, using this, we establish conditions for geometric ergodicity, in terms of θ, the step size h, Lipschitz continuity, tail behaviour, and semi-convexity of f (Theorem 1).

- For this, we established non-asymptotic convergence of the sample empirical distribution to the target as measured by 2-Wasserstein metric, finding again that for θ > 1/2, the resulting scheme is unconditionally stable for all step sizes.




# [Risk-Averse Learning by Temporal Difference Methods with Markov Risk Measures](https://jmlr.org/papers/volume22/20-168/20-168.pdf)
- • A stochastic risk-averse method of temporal differences with linear function approximation (section 3) and proof of its convergence with probability one in a simulation setting (section 4).

- The objective of this paper is to propose and analyze new risk-averse reinforcement learning methods for Markov Decision Processes (MDPs).

- • A novel stochastic multistep risk-averse method of temporal differences with linear function approximation, the new projected multistep dynamic programming equation, and the analysis of its properties (section 5).

- In this paper, we use Markov risk measures in conjunction with linear approximations of the risk-averse policy evaluation and temporal difference learning in the basic and multi-step settings.




# [Attention is Turing Complete](https://jmlr.org/papers/volume22/20-302/20-302.pdf)
- Our study also reveals some minimal sets of elements needed to obtain these completeness results.

- The main contribution of our paper is to show that the Transformer is Turing complete à la Siegelmann and Sontag, that is, based on its capacity to compute and access internal dense representations of the data it processes and produces.

- In Section 4 we prove our main result on the Turing completeness of the Transformer (Theorem 6).

- This is based on two conditions: (1) the ability of RNNs to compute internal dense representations of the data, and (2) the mechanisms they use for accessing such representations.




# [Doubly infinite residual neural networks: a diffusion process approach](https://jmlr.org/papers/volume22/20-706/20-706.pdf)
- Building upon our connection between infinitely deep fully-connected ResNets and diffusion processes, we showed that both forward and backward dynamics are wellbehaved.

- Limited to fully i.i.d. network's parameters and fully-connected neural networks without the second activation ψ, we investigated the case of the doubly infinite ResNets where both the network's depth and the network's width grow unboundedly.

- Under suitable assumptions for our class of doubly infinite ResNets, this result implies that weakly and fully trained neural networks with a large depth and width collapse to linear regression.

- Then, we study analogous backward-propagation results, which relate to the fundamental problem of training ResNets.




# [Non-parametric Quantile Regression via the K-NN Fused Lasso](https://jmlr.org/papers/volume22/20-1462/20-1462.pdf)
- We show that under mild conditions, (6) attains a convergence rate of n −1/d for d-dimensional data in terms of ∆ 2 n , ignoring the logarithmic factor.

- The theorems demonstrate that under general assumptions, both estimators converge at a rate of n −1/d , up to a logarithmic factor, for estimating d-dimensional data under the loss function ∆ 2 n defined above.

- Specifically, we have shown that the quantile K-NN fused lasso estimator achieves an optimal convergence rate of n −1/d for estimating a d-dimensional piecewise Lipschitz quantile function.

- The experiments show that the proposed estimator outperform state-of-the-art methods on both simulated and real datasets.




# [Improved Shrinkage Prediction under a Spiked Covariance Structure](https://jmlr.org/papers/volume22/21-0006/21-0006.pdf)
- We propose CASP -a Coordinate-wise Adaptive Shrinkage Prediction rule for shrinkage prediction in high-dimensional Gaussian models with an unknown mean and covariance.

- In Section 4, we further develop the CASP method for prediction in aggregated models.

- Prediction in aggregated models is more complicated than prediction in disaggregate models.

- Finally, we extend our methodology for prediction to aggregated models.




# [A Generalised Linear Model Framework for β-Variational Autoencoders based on Exponential Dispersion Families](https://jmlr.org/papers/volume22/21-0037/21-0037.pdf)
- • find an analytical description of the auto-pruning property of β-VAE, a reason for posterior collapse.

- Further, we provide an analytical description of the auto-pruning of β-VAE .

- Therefore, we are able to provide a systematic generalization of the loss analysis for VAE based on the assumption that the observation model belongs to an EDF (see Section 3.3).

- In this work, we answer the following research question: Is there a way to generalize the loss analysis of β-VAE based on the observation model distribution?




# [From Fourier to Koopman: Spectral Methods for Long-term Time Series Prediction](https://jmlr.org/papers/volume22/20-406/20-406.pdf)
- Figure 13 exemplifies the pitfalls of time-stepping methods for long-term time series prediction.

- Because of a symmetry relationship, we can leverage the Fast Fourier Transform to obtain model parameters in a fast and scalable way.

- When systems fulfill these properties, we show empirically that stable long-term forecasting can be accomplished.

- In order to probe the performance of the resulting algorithms in long-term forecasting of nonlinear oscillators under measurement noise, a synthetic time series is created.




# [Integrative High Dimensional Multiple Testing with Heterogeneity under Data Sharing Constraints](https://jmlr.org/papers/volume22/20-774/20-774.pdf)
- In this paper, we propose a data shielding integrative large-scale testing (DSILT) procedure to fill this gap.

- In this paper, we propose a DSILT method for simultaneous inference of high dimensional covariate effects in the presence of between-study heterogeneity under the DataSHIELD framework.

- Algorithm 4 Individual-level meta-analysis (ILMA).

- 3. Perform multiple testing procedure in Section 2.5.




# [Geometric structure of graph Laplacian embeddings](https://jmlr.org/papers/volume22/19-683/19-683.pdf)
- Let us now introduce the notion of a well-separated mixture model.

- We are now ready to introduce the notion of a well-separated mixture model.

- Discrete and continuum limit graph Laplacian operators are introduced in Subsection 2.1; the notion of a well-separated mixture model is defined in Subsection 2.2.

- Section 4 is devoted to discussing our notion of well-separated mixture models.




# [Hoeffding's Inequality for General Markov Chains and Its Applications to Statistical Learning](https://jmlr.org/papers/volume22/19-479/19-479.pdf)
- Our last example is the multi-armed bandit problem with Markovian rewards.

- Hoeffding's lemma asserts

- Theorem 14 bounds the regret for the UCB algorithm for the MAB problem with Markovian rewards.

- To our best knowledge, there are few results on upper bound of similar form for reversible Markov chains on general state spaces.




# [RaSE: Random Subspace Ensemble Classification](https://jmlr.org/papers/volume22/20-600/20-600.pdf)
- At the end of Section 2, an iterative version of the RaSE algorithm is presented.

- Next, we provide a similar upper bound for the MC variance of the RaSE classifier.

- Third, we propose a new information criterion RIC with its theoretical properties analyzed under the high-dimensional setting.

- The RaSE classifier competes favorably with existing classification methods.




# [PyKEEN 1.0: A Python Library for Training and Evaluating Knowledge Graph Embeddings](https://jmlr.org/papers/volume22/20-825/20-825.pdf)
- In PyKEEN 1.0

- Knowledge graph embedding models (KGEMs) learn representations for entities and relations of KGs in vector spaces while preserving the graph structure.

- Finally, PyKEEN 1.0 is the only library that performs an automatic memory optimization that ensures that the memory is not exceeded during training and evaluation.

- Additionally, we implemented an automatic memory optimization that ensures that the available memory is best utilized.




# [Refined approachability algorithms and application to regret minimization with global costs](https://jmlr.org/papers/volume22/20-1019/20-1019.pdf)
- • We consider a class of Follow the Regularized Leader algorithms (FTRL) which we convert from regret minimization to approachability.

- However, one drawback of using Blackwell's approachability is that algorithms then usually minimize the Euclidean distance of the average payoffs to the target set, which is seldom the exact quantity of interest in online learning applications.

- This topic is now called Blackwell's approachability.

- One of the main objectives of the present work is to provide a flexible class of algorithms which are able to minimize various distance-like quantities, and not only the Euclidean distance.




# [A Bayes-Optimal View on Adversarial Examples](https://jmlr.org/papers/volume22/20-567/20-567.pdf)
- This section describes our method for generating realistic image datasets with computable Bayes-Optimal classifiers that are provably robust to all adversarial attacks.

- We construct realistic image datasets for which the Bayes-Optimal classifier can be efficiently computed and analyze the vulnerability of the optimal classifier.

- compared to the Bayes-Optimal and the RBF SVM classifiers

- The paper is organized as follows: In section 2.1 we present our method for constructing realistic image datasets that are equipped with computable Bayes-optimal classifiers.




# [Pathwise Conditioning of Gaussian Processes](https://jmlr.org/papers/volume22/20-1260/20-1260.pdf)
- We study the broader implications of this paradigm shift to develop a general framework for conditioning Gaussian processes at the level of random functions.

- In Section 4.5, we discussed finite-dimensional approximations of Gaussian process posteriors.

- Here, we overview the precise formalism that gives rise to the pathwise approach to conditioning Gaussian random variables and show how to derive this result from first principles.

- We provide a general framework for pathwise conditioning of Gaussian processes based on GPflow (Matthews et al., 2017).




# [Learning interaction kernels in heterogeneous systems of agents from multiple trajectories](https://jmlr.org/papers/volume22/19-861/19-861.pdf)
- We exhibit an efficient algorithm to compute the estimators based on the regularized least-squares problem (1.3), and demonstrate the learnability of interaction kernels on various systems, including opinion dynamics, predator-swarm dynamics and heterogeneous particle dynamics.

- Section 5 presents some theoretical results for the coercivity condition, a key condition for achieving the optimal convergence rate of interaction kernels.

- Next, we study the learnability of the estimated interaction kernels in this system.

- A fundamental question is the well-posedness of the inverse problem of learning the interaction kernels.




# [Optimal Minimax Variable Selection for Large-Scale Matrix Linear Regression Model](https://jmlr.org/papers/volume22/19-969/19-969.pdf)
- For implementation, an iterative hard-thresholding (IHT) algorithm is developed for matrix response linear regression models.

- Therefore, Theorem 5 generalizes their results to the matrix linear regression model (1).

- Conditions (C1) and (C2) state that both the coefficient matrix size (pq) and the number of predictors d n are allowed to grow at the exponential rate of the sample size n.

- We conduct simulation studies to examine the finite sample performance of the proposed method.




# [Learning partial correlation graphs and graphical models by covariance queries](https://jmlr.org/papers/volume22/20-1137/20-1137.pdf)
- Our work also provides a new way of performing constraint-based inference in Gaussian graphical models and partial correlation graphs.

- We propose randomized procedures that recover the correct graph and have low query and computational complexity with high probability.

- Our goal is to learn zeros in the inverse covariance matrix when covariances may be queried.

- Here the graphs are encoded by zeros in the inverse covariance matrix (or precision matrix ) K.




# [Classification vs regression in overparameterized regimes: Does the loss function matter?](https://jmlr.org/papers/volume22/20-603/20-603.pdf)
- Our study investigates differences and commonalities between classification and regression, using the overparameterized linear model with Gaussian features.

- We theoretically characterize the performance of solutions for classification and regression tasks using two representative ensembles, defined below.

- This paper introduces a direct comparison between the different loss functions used in classification and regression, in both the training and testing phases.

- On the other hand, we show that the choice of test loss function results in a significant asymptotic difference between classification and regression tasks.




# [Sparse Popularity Adjusted Stochastic Block Model](https://jmlr.org/papers/volume22/19-835/19-835.pdf)
- As a result, sparsity describes only the behavior of network as a whole, without distinguishing between the block-dependent sparsity patterns.

- In summary, to the best of our knowledge, our paper is the first paper that studies structural sparsity in stochastic block models and the PABM is the only block model that allows the treatment.

- The Popularity Adjusted Block Model (PABM), introduced by Sengupta and Chen (2018) and subsequently studied in Noroozi et al. (2021), provides a generalization of both the SBM and the DCBM.

- We are particularly interested in the PABM since, to the best of our knowledge, it is the only block model that allows to model structural sparsity in the connections between the nodes in the network.




# [Cooperative SGD: A Unified Framework for the Design and Analysis of Local-Update SGD Algorithms](https://jmlr.org/papers/volume22/20-147/20-147.pdf)
- More specifically, the main contributions of this paper are as follows: (i) We provide a unified convergence analysis for the cooperative SGD class of algorithms (i.e., distributed SGD algorithms with local updates).

- Elastic Averaging.

- Elastic Averaging SGD (EASGD).

- (ii) To the best of our knowledge, the unified analysis gives the first convergence guarantee for EASGD with non-convex objective functions.




# [Inference In High-dimensional Single-Index Models Under Symmetric Designs](https://jmlr.org/papers/volume22/19-744/19-744.pdf)
- Dudeja and Hsu (2018) and Pananjady and Foster (2019) consider estimation in single-and multi-index models by expanding the unknown link function in the Hermite polynomial basis.

- However, their method critically uses the form of the link function and also requires it to be adequately differentiable.

- In previous sections, a linear approximation of the link function was used to obtain estimates of β 1 .

- In this paper, we develop an inference scheme for the regression coefficients of a highdimensional single-index model with minimal restrictions on the (potentially random) link function-indeed, even discontinuous link functions are allowed-that completely bypasses the estimation of the link.




# [A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms](https://jmlr.org/papers/volume22/19-804/19-804.pdf)
- First, we describe a formalization of the robot manipulation learning problem that synthesizes existing research into a single coherent framework.

- of the plane

- In so doing, we highlight the diversity of the manipulation learning problems that these methods have been applied to as well as identify the many research opportunities and challenges that remain.

- Second, we aim to describe a representative subset of the research that has so far been carried out on robot learning for manipulation.




# [Incorporating Unlabeled Data into Distributionally-Robust Learning](https://jmlr.org/papers/volume22/19-1023/19-1023.pdf)
- In this section, we demonstrate another application, to active learning.

- We have explored an alternative to Wasserstein distributionally robust learning that incorporates unlabeled data to restrict the adversary's decision set.

- We have demonstrated an application to the problem of active learning that yields a distributionally-robust model change heuristic that empirically often outperforms the existing model change heuristics.

- In this section, we investigate the empirical performance of our proposed formulation of distributionally robust learning in the particular case of logistic regression.




# [Beyond English-Centric Multilingual Machine Translation](https://jmlr.org/papers/volume22/20-1307/20-1307.pdf)
- We introduced m2m-100, a new Many-to-Many multilingual translation model that can translate between the 9,900 directions of 100 languages.

- In this work, we create more diverse multilingual machine translation models by building a large-scale Many-to-Many data set for 100 languages.

- Overall, we build the first true Many-to-Many data set comprising 7.5B training sentences for 100 languages, providing direct training data for thousands of translation directions.

- We then systematically compare this Many-to-Many data set to an English-Centric approach (Section § 4).




# [Normalizing Flows for Probabilistic Modeling and Inference](https://jmlr.org/papers/volume22/19-1028/19-1028.pdf)
- Below we summarize the use of flows for sampling, variational inference, and likelihood-free inference.

- We have described normalizing flows and their use for probabilistic modeling and inference.

- Normalizing flows provide a general way of constructing flexible probability distributions over continuous random variables.

- Specifically, our review begins by establishing the formal and conceptual structure of normalizing flows in Section 2.




# [Residual Energy-Based Models for Text](https://jmlr.org/papers/volume22/20-326/20-326.pdf)
- In the next sections, we show that generations from even large models can be discriminated from real text by such classifiers.

- The goal of this work is to improve upon strong language models on large scale datasets.

- In the previous sections, we checked empirically that machine-generated text by current stateof-the-art locally normalized and auto-regressive language models can be easily discriminated, albeit to a lesser extent in extreme generalization conditions.

- This formulation enables locally-normalized auto-regressive models to be trained efficiently via maximum likelihood and generate samples of remarkable quality.




# [Bandit Learning in Decentralized Matching Markets](https://jmlr.org/papers/volume22/20-1429/20-1429.pdf)
- We propose a solution for the decentralized version of the two-sided matching bandit problem.

- We have shown that Algorithm 1 is not incentive compatible in the fully general setting.

- In this work we have made progress on the problem of stochastic bandits in decentralized matching markets.

- 2. Incentive compatibility in the decentralized setting.




# [Optimal Feedback Law Recovery by Gradient-Augmented Sparse Polynomial Regression](https://jmlr.org/papers/volume22/20-755/20-755.pdf)
- We have presented a sparse polynomial regression framework for the approximation of feedback laws arising in nonlinear optimal control.

- A data-driven method for computing optimal feedback laws.

- Next, we build a polynomial model for the value function based on a hyperbolic cross approximation.

- The reduction of the number of samples due to the inclusion of gradient information is particularly relevant for high-dimensional nonlinear optimal control problems, as sampling generation can be particularly costly.




# [Counterfactual Mean Embeddings](https://jmlr.org/papers/volume22/20-185/20-185.pdf)
- Treatment effect estimation.

- In this work, we focus on the distributional treatment effect (DTE), which involves the entire outcome distributions.

- Finally, we introduce the kernel treatment effect (KTE) as a way to evaluate the distributional treatment effect in Section 3.5.

- This means that the KTE informs the existence of any difference in the potential outcome distributions, quantifying the distributional treatment effect.




# [Universal consistency and rates of convergence of multiclass prototype algorithms in metric spaces](https://jmlr.org/papers/volume22/20-1081/20-1081.pdf)
- Rates for the Proto-k-NN rule.

- Rates for the k-NN rule.

- We now introduce a second prototype rule, termed Proto-k-NN, that hybridizes between k-NN and Proto-NN.

- The rates of convergence for the k-NN and Proto-k-NN classifiers of Section 3 are derived using the following decomposition of the excess error probability.




# [On Efficient Multilevel Clustering via Wasserstein Distances](https://jmlr.org/papers/volume22/19-782/19-782.pdf)
- We have proposed an optimization-based approach to multilevel clustering using Wasserstein metrics.

- We call the above optimization the problem of Multilevel Wasserstein Means (MWM).

- Therefore, the simultaneous local and global clusterings in the joint optimization formulation of MWM enable the discovery of nested multilevel structures hidden in grouped data.

- The innermost (blue) and outermost (green) rings depict local and global clusters respectively.




# [Consensus-Based Optimization on the Sphere: Convergence to Global Minimizers and Machine Learning](https://jmlr.org/papers/volume22/21-0259/21-0259.pdf)
- The algorithm is able to perform essentially as good as ad hoc state of the art methods and in some instances it obtains quantitatively better results.

- The algorithm is able to perform essentially as good as ad hoc state of the art methods and in some instances it obtains quantifiably better results.

- We presented the numerical implementation of a new consensus-based model for global optimization on the sphere, which is inspired by the kinetic Kolmogorov-Kuramoto-Vicsek equation.

- In the second part of this section, we present applications in signal processing and machine learning, namely the phase retrieval problem and the robust subspace detection and we provide comparisons with state of the art methods.




# [Structure Learning of Undirected Graphical Models for Count Data](https://jmlr.org/papers/volume22/18-401/18-401.pdf)
- The main contribution of this paper is a careful analysis of the numerical and statistical efficiency of PC-LPGM, a simple method for structure learning of undirected graphical models for count data.

- We aim to measure the ability of PC-LPGM to recover the true structure of the graphs, also in situations where relatively moderate sample sizes are available.

- A validation of the algorithm on two real cases is given in Section 7.

- In this paper, we concentrate on count data and introduce a simple algorithm for structure learning of undirected graphical models, called PC-LPGM, particularly useful when sparse graphs are under consideration.




# [Alibi Explain: Algorithms for Explaining Machine Learning Models](https://jmlr.org/papers/volume22/21-0017/21-0017.pdf)
- Code Snippet 1: Demo of the Alibi API with the AnchorTabular explanation algorithm.

- Extensive testing of code correctness and algorithm convergence is done using pytest under various Python versions.

- The library features comprehensive documentation and extensive in-depth examples of use cases 1 .

- Tests are executed on every pull request via a continuous integration setup using Github Actions.




# [Convolutional Neural Networks Are Not Invariant to Translation, but They Can Learn to Be](https://jmlr.org/papers/volume22/21-0019/21-0019.pdf)
- Two common misunderstanding have contributed to view that CNNs are architecturally invariant to translation.

- Secondly, we assessed the degree to which CNN can learn online translation invariance through pretraining.

- By contrast, we found that pretraining on a fully translated data set improved online translation invariance consistently.

- The network seem to be perfectly invariant to translation for some classes.




# [Integrated Principal Components Analysis](https://jmlr.org/papers/volume22/20-084/20-084.pdf)
- To this end, we propose Integrated Principal Components Analysis (iPCA), which extends a model-based framework of the classical Principal Components Analysis (PCA) to integrated data.

- Our work on iPCA is the first to consider the matrix-variate normal model in light of data integration.

- The key idea here behind iPCA is to leverage a new but natural connection between data integration and the matrix-variate normal model.

- iPCA is also a useful and effective tool in practice to discover interesting joint patterns that are shared across multiple data sets.




# [Integrative Generalized Convex Clustering Optimization and Feature Selection for Mixed Multi-View Data](https://jmlr.org/papers/volume22/19-1012/19-1012.pdf)
- In this paper, we develop a convex formulation of integrative clustering for high-dimensional mixed multi-view data.

- By construction, iGecco+ directly clusters mixed multi-view data and selects features from each data view simultaneously.

- Specifically, we show that clustering for mixed, multi-view data can be achieved using different data-specific convex losses with a joint fusion penalty.

- For feature selection using different distances and losses, we propose an adaptive shifted group-lasso penalty that will select features by shrinking them towards their appropriate centroid.




# [Continuous Time Analysis of Momentum Methods](https://jmlr.org/papers/volume22/19-466/19-466.pdf)
- We study momentum-based optimization algorithms for the minimization task (1), with learning rate independent momentum, fixed at every iteration step, focusing on deterministic methods for clarity of exposition.

- This demonstrates that introduction of momentum in the form used within both HB and NAG results in numerical methods that do not differ substantially from gradient descent.

- To the best of our knowledge, the first application of HB to neural network training appears in Rumelhart et al. (1986).

- Most notable amongst these momentum-based methods are the Heavy Ball Method (HB), due to Polyak (1964), and Nesterov's method of accelerated gradients (NAG) Nesterov (1983).




# [How to Gain on Power: Novel Conditional Independence Tests Based on Short Expansion of Conditional Mutual Information](https://jmlr.org/papers/volume22/19-600/19-600.pdf)
- We define Short Expansion of Conditional Mutual Information (SECM I) as the truncated Möbius expansion (17) which incorporates the leading term I(X, Y ) and interactions of order 2 i.e. the terms II(X, Z k , Y ).

- In this work, we discuss drawbacks of existing CM I-based procedures when the conditioning set consists of a large number of variables and in the view of them we propose a novel test procedure based on a sample analogue of SECM I (Short Expansion of Conditional Mutual Information), obtained from Möbius representation of CM I by truncation.

- It turns out that the conditional mutual information can be represented as a sum of interaction informations, see Section 4.1.

- By truncating the Möbius expansion we reduce the sizes of conditioning sets for the summands which are the source of the lack of power of the CM I. First we give some preliminaries on the Möbius representation of the conditional mutual information.




# [Context-dependent Networks in Multivariate Time Series: Models, Methods, and Risk Bounds in High Dimensions](https://jmlr.org/papers/volume22/20-244/20-244.pdf)
- In this paper, we develop two procedures that estimate context-dependent networks from autoregressive time series of annotated event data.

- Developing a statistical model for autoregressive time series of annotated event data is a non-trivial task.

- Our logistic-normal approach ( 7), (8) combines ideas from compositional time series and autoregressive process framework.

- The contribution of this paper focuses on estimation methods and theoretical guarantees for context-dependent network structures which exploit features associated with events.




# [PeerReview4All: Fair and Accurate Reviewer Assignment in Peer Review](https://jmlr.org/papers/volume22/20-190/20-190.pdf)
- Having defined the model and estimator, we now provide a sharp minimax analysis for the subjective-score model.

- We provide a sharp minimax analysis under this subjective setting and prove that our assignment algorithm PeerReview4All is also near-optimal for this subjective-score setting.

- In contrast, our goal is to design an assignment algorithm that can simultaneously achieve both the desired objectives of fairness and statistical accuracy.

- 2. In addition to quantifying the performance of PeerReview4All, an important contribution of Theorem 3 is a sharp minimax analysis of the performance of every assignment algorithm.




# [A Lyapunov Analysis of Accelerated Methods in Optimization](https://jmlr.org/papers/volume22/20-195/20-195.pdf)
- We provide a brief review of the technique of estimate sequences (Nesterov, 2004).

- It also makes the connection between estimate sequences and Lyapunov functions explicit.

- We derive continuous-time estimate sequences directly from our Lyapunov function arguments and show that these two techniques are equivalent.

- The main contributions in this paper are twofold: We have presented a unified analysis of a wide variety of algorithms using Lyapunov functions-equations ( 23) and ( 36)-and we have demonstrated the equivalence between Lyapunov arguments and estimate sequences of f , under the formalization of the latter due to Baes (2009).




# [Limit theorems for out-of-sample extensions of the adjacency and Laplacian spectral embeddings](https://jmlr.org/papers/volume22/19-852/19-852.pdf)
- Two natural approaches to the out-of-sample extension of ASE suggest themselves.

- We leave a more thorough exploration of adversarial variants of the out-of-sample extension problem for future work.

- To compare the out-of-sample extension to its in-sample counterpart, we consider the following set-up.

- This problem is well-studied in the dimensionality reduction literature, where it is known as the out-of-sample extension problem.




# [Unlinked Monotone Regression](https://jmlr.org/papers/volume22/20-689/20-689.pdf)
- Thus, it seems that a transitional regime occurs in the rate of convergence in case the noise distribution is known.

- 2. Finding (minimax) lower bounds for the rate of convergence seems to be hard to obtain in our setting.

- This means that the rate of convergence of the estimator is driven by the integral in the first summand.

- Not surprisingly, this rate depends on the smoothness of the noise distribution.




# [Subspace Clustering through Sub-Clusters](https://jmlr.org/papers/volume22/18-780/18-780.pdf)
- In this section, we introduce our sampling based algorithm for subspace clustering (SBSC).

- On a high level, we transfer the problem from "clustering points" to "clustering sub-clusters".

- We show that the clustering through sub-clusters algorithm is highly scalable and can significantly boost the clustering accuracy on both the subset and whole dataset.

- In this section we provide the algorithm for classifying the out-of-sample points.




# [Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations](https://jmlr.org/papers/volume22/21-0453/21-0453.pdf)
- As an application of the suggested framework, we study two numerical methods for the underdamped Langevin dynamics.

- 3. A general result that allows to obtain bounds for the 2-Wasserstein distance between the target distribution and its numerical approximations for general SDEs.

- Section 6 applies the general result to investigate two discretizations of the underdamped Langevin dynamics.

- 2. A study of the contractivity of integrators for the underdamped Langevin dynamics that takes into account the possible impact of increasing condition numbers.




# [A flexible model-free prediction-based framework for feature ranking](https://jmlr.org/papers/volume22/20-673/20-673.pdf)
- This work introduces model-free objective-based marginal feature ranking criteria-s-CC and s-NPC-for the purpose of binary decision-making.

- thanks to the robustness of s-NPC to sampling bias.

- In addition to the biomedical examples we show in this paper, model-free objective-based marginal feature ranking is also useful for finance applications, among others.

- In this section, we introduce two objective-based marginal feature ranking criteria, on the population level, under the classical paradigm and the Neyman-Pearson (NP) paradigm.




# [From Low Probability to High Confidence in Stochastic Convex Optimization](https://jmlr.org/papers/volume22/20-821/20-821.pdf)
- Our paper rests on two pillars: the proximal point method and robust distance estimation.

- The proximal point method was introduced by Martinet (1972Martinet ( , 1970 and further popularized by Rockafellar (1976).

- The current paper extends the proxBoost algorithm to constrained and regularized settings, develops consequences for both streaming and offline algorithms, and develops a smoothing technique that enables application of proxBoost for nonsmooth problems.

- In this section, we explore the consequences of the proxBoost algorithm for empirical risk minimization.




# [Langevin Monte Carlo: random coordinate descent and variance reduction](https://jmlr.org/papers/volume22/20-1205/20-1205.pdf)
- Secondly, we study variance reduction techniques.

- • We propose and rigorously analyze a new variance reduction method SVRG-O/U-LMC.

- More specifically, we explore how to incorporate random coordinate descent (RCD) in LMC.

- The idea of RCD is to surrogate the full gradient in Gradient Descent by a randomly selected partial derivative in each iteration (Nesterov, 2012).




# [COKE: Communication-Censored Decentralized Kernel Learning](https://jmlr.org/papers/volume22/20-070/20-070.pdf)
- • To increase the communication efficiency, we further develop a COmmunicationcensored KErnel learning (COKE) algorithm, which achieves desired learning performance given limited communication resources and energy supply.

- Decentralized kernel learning.

- To the best of our knowledge, this is the first work to solve decentralized kernel learning in the RF space by ADMM without any raw data exchange.

- Centralized kernel learning.




# [FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference](https://jmlr.org/papers/volume22/19-853/19-853.pdf)
- FLAME produces interpretable, high-quality matches.

- It learns a distance for matching from a hold-out training set.

- The main benefits of FLAME are: • It learns a weighted Hamming distance for matching based on a hold-out training set (rather than using a pre-specified distance).

- In the following subsection we discuss the importance of learning a distance metric.




# [A Two-Level Decomposition Framework Exploiting First and Second Order Information for SVM Training Problems](https://jmlr.org/papers/volume22/19-632/19-632.pdf)
- In this work we proposed a novel way to deal with sub-problems of more than two variables.

- Moreover, the computational effort required to solve the subproblem becomes greater as the size of the working set grows.

- When the working set is enlarged with the addition of the stored variables, however, the performance greatly improves.

- As we could reasonably expect, the number of iterations generally decreases as the size of the working set increases.




# [Inference for Multiple Heterogeneous Networks with a Common Invariant Subspace](https://jmlr.org/papers/volume22/19-558/19-558.pdf)
- In this paper, we resolve the following questions: first, can we construct a simple, flexible multiple random network model that can be used to approximate real-world data?

- Because the invariant subspaces defined by V are common to all the graphs, we call this model the common subspace independent edge (COSIE) random graph model.

- Definition 2 (Common Subspace Independent Edge graphs).

- We first apply our multiple adjacency spectral embedding method to the HNU1 data.




# [On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift](https://jmlr.org/papers/volume22/19-736/19-736.pdf)
- We characterize the computational, approximation, and sample size properties of these methods in the context of a discounted Markov Decision Process (MDP).

- Policy gradients.

- Overall, the results of this work place policy gradient methods under a solid theoretical footing, analogous to the global convergence guarantees of iterative value function based algorithms.

- In particular, we pay close attention to both log-linear policy classes and neural policy classes (see Section 6).




# [Gradient Methods Never Overfit On Separable Data](https://jmlr.org/papers/volume22/20-997/20-997.pdf)
- Since there are standard generalization bounds for predictors which achieve a large margin over the dataset, we get that asymptotically, gradient descent does not overfit, even if we just run it on the empirical risk function without any explicit regularization, and even if the number of iterations T diverges to infinity.

- In other words, at no point does gradient descent significantly overfit, regardless of the number of iterations.

- Could it be that gradient methods do not overfit only after so many iterations?

- To prove these results, we also establish more refined, nonasymptotic bounds on the margins attained on the dataset, which are also applicable to other losses.




# [Online stochastic gradient descent on non-convex losses from high-dimensional inference](https://jmlr.org/papers/volume22/20-1288/20-1288.pdf)
- We begin this section by proving a law of large numbers for the trajectory in the descent phase, Theorem 3.2.

- In this paper, we show that a key quantity governing the performance of online SGD is the following, which we call the information exponent for a population loss.

- Then Assumptions A and B hold and the population loss has information exponent 1.

- We study the dependence of the amount of data needed (i.e., the sample complexity) for recovery of the parameter using online SGD on the information exponent.




# [On the Estimation of Network Complexity: Dimension of Graphons](https://jmlr.org/papers/volume22/19-747/19-747.pdf)
- In this paper, we develop a statistical theory of graph complexity in a universal model of random graphs.

- We show universal non-asymptotic bounds for its risk (Theorem 1).

- In particular, our error bounds for the neighborhood distance and the covering number hold for any graphon.

- Theorem 7 gives non-asymptotic error bounds for the distance-estimator (11).




# [Is SGD a Bayesian sampler? Well, almost](https://jmlr.org/papers/volume22/20-676/20-676.pdf)
- One surprising property is that they typically perform best in the overparameterised regime, with many more parameters than data points.

- Thus, the bias in the prior is essentially translated over to the posterior.

- We argue here that the inductive bias found in DNNs trained by SGD or related optimisers, is, to first order, determined by the parameter-function map of an untrained DNN.

- In Section 7.3, we discussed work showing that DNNs may have an inductive bias towards simple functions in their parameter-function map.




# [Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime](https://jmlr.org/papers/volume22/20-974/20-974.pdf)
- pro-vided upper bounds on the population risk for the least-norm interpolant applied to a class of kernels including the Neural Tangent Kernel.

- Our main result is a finite-sample bound on the misclassification error of the maximum margin classifier.

- This simplifies the proofs without materially affecting the analysis, since rescaling the data does not affect the accuracy of the maximum margin algorithm.

- We note that a bound on the accuracy of the maximum margin classifier with respect to the distribution P without any label noise is implicit in our analysis.




# [An Inertial Newton Algorithm for Deep Learning](https://jmlr.org/papers/volume22/19-1024/19-1024.pdf)
- We introduced a novel stochastic optimization algorithm featuring inertial and Newtonian behavior motivated by applications to deep learning.

- We now study the convergence of INNA.

- -On D-criticality.

- Our result seems moreover to be the first one to be able to rigorously handle the analysis of mini-batch sub-sampling for ReLU DNNs via the introduction of the D-critical points.




# [River: machine learning for streaming data in Python](https://jmlr.org/papers/volume22/20-1380/20-1380.pdf)
- River is a machine learning package for data streams in Python.

- Creme (Halford et al., 2019) and scikit-multiflow (Montiel et al., 2018) are two opensource libraries to perform machine learning in the stream setting.

- River's architecture is the result from the lessons learned during the development of its parent packages Creme and scikit-multiflow.

- In machine learning, the conventional approach is to process data in batches or chunks.




# [Achieving Fairness in the Stochastic Multi-Armed Bandit Problem](https://jmlr.org/papers/volume22/20-704/20-704.pdf)
- This leads us to evaluate the cost of fairness in terms of the conventional notion of regret.

- We then evaluate the cost of fairness in Fair-MAB with respect to the conventional notion of regret in Section 5.

- In this section, we provide the framework of our proposed class of Fair-MAB algorithms.

- As our primary contribution, in Section 4, we define a class of Fair-MAB algorithms, called Fair-Learn, characterized by two parameters: the unfairness tolerance and the learning algorithm used as a black-box.




# [Factorization Machines with Regularization for Sparse Feature Interactions](https://jmlr.org/papers/volume22/20-1170/20-1170.pdf)
- In this paper, we present a new regularization scheme for feature interaction selection in FMs.

- Formula Feature Feature interaction selection selection

- We here introduce a preferable but hard to optimize regularizer for feature interaction selection in FMs.

- We next introduce a preferable but hard to optimize regularizer Ω * for feature interaction selection in FMs.




# [Learning with semi-definite programming: statistical bounds based on fixed point analysis and excess risk curvature](https://jmlr.org/papers/volume22/21-0021/21-0021.pdf)
- The aim of this paper is to put forward a methodology developed in Learning Theory for the study of SDP estimators.

- The goal of the present paper is to introduce a new fixed point approach to the statistical analysis of SDP-based estimators, and illustrate our method on four current problems of interest, namely community detection, signed clustering, angular group synchronization, and Max-Cut.

- An example of a Vapnik-Chervonenkis's type of analysis of SDP estimators can be found in for the community detection problem.

- This section encompasses the main contributions of our paper for the three problems we study, namely signed clustering, angular synchronization, and MAX-CUT.




# [Shape-Enforcing Operators for Generic Point and Interval Estimators of Functions](https://jmlr.org/papers/volume22/20-513/20-513.pdf)
- We propose a method to enforce shape restrictions ex post on any initial generic point and interval estimates of functions by applying functional operators.

- Section 3 discusses the use of shape-enforcing operators to obtain improved point and interval estimates of functions that satisfy shape restrictions.

- We show that enforcing the shape restrictions through these operators improves point and interval estimators, and provide computational algorithms to implement these shape-enforcing operators.

- We quantify the finite-sample improvement in the point and interval estimates of enforcing shape restrictions using simulations calibrated to the growth chart application.




# [A Unified Framework for Random Forest Prediction Error Estimation](https://jmlr.org/papers/volume22/18-558/18-558.pdf)
- We propose a unified framework for random forest prediction error estimation based on a novel estimator for the conditional prediction error distribution.

- To our knowledge, we are the first to propose a method of estimating the conditional prediction error distribution of random forests.

- This paper proposes a method of estimating the conditional prediction error distribution F E (e | x) of random forests.

- Under this framework, useful uncertainty metrics can be estimated by simply plugging in the estimated conditional prediction error distribution.




# [On Solving Probabilistic Linear Diophantine Equations](https://jmlr.org/papers/volume22/17-474/17-474.pdf)
- With these constraints in mind, the trimmed p-convolution tree method is always within a log-factor of the specialized methods.

- Trimmed p-convolution trees are benchmarked versus untrimmed p-convolution trees (Table 2).

- Trimmed p-convolution trees come within a log-factor of the best known specialized methods while having no restrictions on the data.

- Figure 2: Lazily, trimmed p-convolution tree.




# [Expanding Boundaries of Gap Safe Screening](https://jmlr.org/papers/volume22/21-0179/21-0179.pdf)
- Here the proposed approach reduces to the standard Gap Safe screening.

- In this paper, we proposed a safe screening framework that improves upon the existing Gap Safe screening approach, while extending its application to a wider range of problems-in particular, problems whose associated dual problem is not globally strongly concave.

- The standard Gap Safe screening approach is not applicable while the proposed extension is.

- We thus expect the proposed approach to improve over the standard Gap Safe screening.




# [The ensmallen library for flexible numerical optimization](https://jmlr.org/papers/volume22/20-416/20-416.pdf)
- The ensmallen numerical optimization provides a flexible framework for optimization of user-supplied objective functions in C++.

- A large set of pre-built optimizers is provided; at the time of writing, 46 optimizers are available.

- Unlike other frameworks, ensmallen supports many types of objective functions, provides a diverse set of pre-built optimizers, supports custom behavior via callback functions, and handles various element and matrix types used by objective functions.

- Lastly, ensmallen provides an extensible framework to easily allow the implementation of new optimization techniques.




# [What Causes the Test Error? Going Beyond Bias-Variance via ANOVA](https://jmlr.org/papers/volume22/20-1211/20-1211.pdf)
- While studied the unimodality of the overall variance, we study the properties the components individually.

- A key finding of our work is that in the specific neural net models considered here, the random training data, initialization, and label noise contribute highly non-additively to the test error.

- Moreover, we study the monotonicity and unimodality of MSE, bias, variance, and the various variance components in a specific variance decomposition.

- They further also decomposed the variance in a specific order into that stemming from label noise, initialization, and training features.




# [Locally Differentially-Private Randomized Response for Discrete Distribution Learning](https://jmlr.org/papers/volume22/18-726/18-726.pdf)
- Finally, we have derived inner and outer bounds to some specific instances of the fundamental privacy-fidelity trade-off curve, all of which depend on the random mechanism via Φ(W ) or ϕ(W ).

- We now introduce the metrics for characterizing this privacy-fidelity trade-off.

- Much like for the feasibility problem in Section 7.1, specializing W to the step mechanism W , allows us to derive upper bounds on the fundamental privacy-fidelity trade-off curves α ( ; P) for the minimax problem.

- Now that we have introduced the fidelity loss and privacy metrics in ( 5) and ( 7) respectively, we can formulate the privacy-fidelity trade-off problem.




# [Failures of Model-dependent Generalization Bounds for Least-norm Interpolation](https://jmlr.org/papers/volume22/20-1164/20-1164.pdf)
- We study the minimum norm linear interpolant.

- These observations illustrate the need to consider distributiondependent notions of complexity in understanding the generalization performance of deep networks.

- The distribution Q n is defined so that the least-norm interpolant performs poorly on Q n .

- If k * < n/c 1 , then, with probability at least 1 − δ, the least-norm interpolant h satisfies .




# [Risk Bounds for Unsupervised Cross-Domain Mapping with IPMs](https://jmlr.org/papers/volume22/18-489/18-489.pdf)
- The following theorem introduces a risk bound for unsupervised cross-domain mapping.

- In Section 5, we derived a novel risk bound for the unsupervised learning of mappings between domains.

- We derive risk bounds for unsupervised cross-domain mapping, comparing alternative hyperparameters ω ∈ Ω.

- 1. Theorem 1 provides a rigorous statement of a risk bound for unsupervised cross-domain mapping with IPMs, which is the basis of this work.




# [Multi-class Gaussian Process Classification with Noisy Inputs](https://jmlr.org/papers/volume22/20-107/20-107.pdf)
- The experiments show that the predictive distribution of the proposed methods is significantly better in terms of the test log-likelihood.

- In this paper we have proposed several multi-class GP classifiers that can account for input noise.

- Section 3 describes the proposed models and methods to account for input noise in the observed attributes.

- To account for input noise in the context of multi-class GP classification, we describe three different methods.




# [Convex Geometry and Duality of Over-parameterized Neural Networks](https://jmlr.org/papers/volume22/20-1447/20-1447.pdf)
- Our analysis showed that optimal solutions can be exactly characterized as the extreme points of a convex set.

- We studied two-layer ReLU networks and introduced a convex analytic framework based on duality to characterize a set of optimal solutions to the regularized training problem.

- Here, we provide the closed-form formulations for the optimal solutions to the regularized training problem (23)

- In addition to our closed-form solutions, we also propose a convex cutting plane based approach to optimize ReLU networks.




# [A Bayesian Contiguous Partitioning Method for Learning Clustered Latent Variables](https://jmlr.org/papers/volume22/20-136/20-136.pdf)
- The preceding Bayesian spanning tree partitioning prior model can be extended to other hierarchical model settings.

- Finally, the Bayesian spanning tree partitioning prior model introduced in Section 3.1 is adopted to model π.

- Our main contribution is to propose a Bayesian model-based spanning tree partitioning method, along with theoretical justifications and efficient computational algorithms, to model clustered latent variables with a focus on spatially clustered varying coefficient models.

- For computation, we propose an RJ-MCMC algorithm to sample spanning trees and partitions from their posterior distributions.




# [sklvq: Scikit Learning Vector Quantization Rick van Veen a](https://jmlr.org/papers/volume22/21-0029/21-0029.pdf)
- Learning vector quantization (LVQ) has, since its introduction by Kohonen (1990), become an important family of supervised learning algorithms.

- Here we present "sklvq" 1 , an open-source, Python based, and "scikit-learn" (Pedregosa et al., 2011) compatible 2 LVQ framework, including the following three variants: Generalized LVQ (GLVQ) by Sato and Yamada (1995), generalized matrix LVQ (GMLVQ), and localized GMLVQ (LGMLVQ) by Schneider et al. (2009); .

- In the previous section, we have shown the theory behind the design and implementation of sklvq and the resulting advantages.

- We have provided an overview of the resulting benefit ( variants not yet available in sklvq (Table 1a).




# [Explaining Explanations: Axiomatic Feature Interactions for Deep Networks](https://jmlr.org/papers/volume22/20-1223/20-1223.pdf)
- We proposed a novel method called Integrated Hessians to explain feature interactions in neural networks.

- Several existing methods explain feature interactions in neural networks.

- Finally, we demonstrate the utility of Integrated Hessians in a variety of applications where identifying feature interactions in neural networks is useful.

- First, we propose an approach, Integrated Hessians, to quantify pairwise feature interactions that can be applied to any neural network architecture.




# [Quasi-Monte Carlo Quasi-Newton in Variational Bayes](https://jmlr.org/papers/volume22/21-0498/21-0498.pdf)
- We propose to combine the stochastic quasi-Newton method with RQMC samples to create a randomized quasi-stochastic quasi-Newton (RQSQN) algorithm.

- This method is called SQN (stochastic quasi-Newton).

- Recently, randomized quasi-Monte Carlo (RQMC) methods, that we describe below, have been used in place of MC to improve upon SGD.

- In this section we investigate quasi-Newton quasi-Monte Carlo optimization for some VB problems.




# [Thompson Sampling Algorithms for Cascading Bandits](https://jmlr.org/papers/volume22/20-447/20-447.pdf)
- We derive a tighter bound on the regret than theirs.

- Finally, we derive a problem-independent lower bound on the regret incurred by any online algorithm for the standard cascading bandit problem.

- We are the first to theoretically establish a regret bound for Thompson sampling algorithm on the linear cascading bandit problem.

- In Section 6, we provide a lower bound on the regret of any algorithm in the cascading bandits with its proof sketch.




# [One-Shot Federated Learning: Theoretical Limits and Algorithms to Achieve Them *](https://jmlr.org/papers/volume22/19-1048/19-1048.pdf)
- We also derive a lower bound on the estimation error of any algorithm.

- We also addressed the problem of distributed learning under tiny (constant) communication budget.

- We studied the problem of statistical optimization of convex loss landscapes in a distributed system with one-shot communications.

- The next theorem shows that when n = 1, the expected error is lower bounded by a constant, even if m goes to infinity.




# [Wasserstein Barycenters can be Computed in Polynomial Time in Fixed Dimension](https://jmlr.org/papers/volume22/20-588/20-588.pdf)
- The literature on computing Wasserstein barycenters is extensive and rapidly growing.

- We use tools from computational geometry to solve the separation oracle for (MOT-D) in polynomial time.

- We now conclude the desired efficient algorithm for the separation oracle.

- That is, we can efficiently solve (MOT-D) so long as we can efficiently implement the corresponding separation oracle.




# [First-order Convergence Theory for Weakly-Convex-Weakly-Concave Min-max Problems](https://jmlr.org/papers/volume22/20-533/20-533.pdf)
- An inexact proximal point method is presented with different variations that employ different algorithms for solving the constructed strongly monotone variational inequalities.

- To the best of our knowledge, this is the first work that proves the non-asymptotic convergence of first-order methods to a nearly stationary solution of a class of non-smooth non-convex non-concave min-max problems.

- The method we propose is called the inexact proximal point (IPP) method.

- Remark: To the best of our knowledge, this is the first non-asymptotic convergence of stochastic subgradient algorithms for solving non-convex non-concave problems.




# [Langevin Dynamics for Adaptive Inverse Reinforcement Learning of Stochastic Gradient Algorithms](https://jmlr.org/papers/volume22/20-625/20-625.pdf)
- This paper has presented and analyzed the convergence of passive Langevin dynamics algorithms for adaptive inverse reinforcement learning (IRL).

- Finally, this paper analyzed the weak convergence and tracking properties of passive Langevin dynamic algorithms.

- Finally, we presented a complete weak convergence proof of the IRL algorithm using martingale averaging methods.

- Here we prove weak convergence of the multi-kernel variance reduction IRL algorithm (20).




# [A Contextual Bandit Bake-off](https://jmlr.org/papers/volume22/18-863/18-863.pdf)
- To our knowledge, this is the first evaluation of contextual bandit algorithms on such a large and diverse corpus of datasets.

- Simulated contextual bandit setting.

- In this paper, we presented an evaluation of practical contextual bandit algorithms on a large collection of supervised learning datasets with simulated bandit feedback.

- In this section, we present our evaluation of the contextual bandit algorithms described in Section 3.




# [Path Length Bounds for Gradient Descent and Flow](https://jmlr.org/papers/volume22/19-979/19-979.pdf)
- In this section we provide lower bounds on the path length for quadratic functions, PKL functions, and separable quasiconvex functions.

- For separable quasiconvex objectives, we give matching (up to constants) upper and lower bounds on the path length.

- In this work, we study the path length of GD and gradient flow (GF) curves as an independent object of interest.

- One such property is an upper bound on the path length of the GD curve.




# [ChainerRL: A Deep Reinforcement Learning Library](https://jmlr.org/papers/volume22/20-376/20-376.pdf)
- In this paper, we introduce ChainerRL, an open-source Python DRL library supporting both CPU and GPU training, built off of the Chainer (Tokui et al., 2019) deep learning framework.

- ChainerRL offers a comprehensive set of algorithms and abstractions, a set of "reproducibility scripts" that replicate research papers, and a companion visualizer to inspect agents.

- Since its resurgence in 2013 (Mnih et al., 2013), deep reinforcement learning (DRL) has undergone tremendous progress, and has enabled significant advances in numerous complex sequential decision-making problems

- ChainerRL's comprehensive suite of algorithms, flexible APIs, visualization tools, and faithful reproductions can accelerate the research and application of DRL algorithms.




# [Decentralized Stochastic Gradient Langevin Dynamics and Hamiltonian Monte Carlo](https://jmlr.org/papers/volume22/21-0307/21-0307.pdf)
- These results illustrate our theoretical results and show the performance of our methods for decentralized Bayesian logistic regression problems.

- In this paper, we studied DE-SGLD and DE-SGHMC methods which allow scalable Bayesian inference for decentralized learning settings.

- To our knowledge, these are first non-asymptotic performance guarantees for SGHMC methods in the decentralized setting.

- We recall from (2) that decentralized stochastic gradient Langevin dynamics (DE-SGLD) are based on stochastic estimates ∇f i (x) of the actual gradients ∇f i (x).




# [Hardness of Identity Testing for Restricted Boltzmann Machines and Potts models](https://jmlr.org/papers/volume22/20-1162/20-1162.pdf)
- For this, we establish rst the hardness of the identity testing problem for antiferromagnetic Ising models with bounded edge interactions.

- The speci c focus in this paper is on the computational complexity of the identity testing problem for undirected graphical models and its connections to the hardness of the counting problem.

- Speci cally, we devise a methodology to reduce the problem of approximate counting (i.e., approximating partition functions) to identity testing.

- In this section, we establish our lower bound for identity testing for ferromagnetic RBMs with inconsistent elds; speci cally, we prove Theorem 2 from the introduction.




# [Method of Contraction-Expansion (MOCE) for Simultaneous Inference in Linear Models](https://jmlr.org/papers/volume22/19-776/19-776.pdf)
- We have developed a new method of contraction and expansion (MOCE) for simultaneous inference in high-dimensional linear models.

- To address such challenges, we propose a new approach, with some mild regularity conditions, termed as Method of Contraction and Expansion (MOCE).

- This method will be adopted in this paper for a new paradigm of post-model selection inference.

- Thus, MOCE provides a realistic solution to valid simultaneous post-model selection inferences.




# [Estimation and Optimization of Composite Outcomes](https://jmlr.org/papers/volume22/20-429/20-429.pdf)
- 1 We propose a new paradigm for estimating optimal individualized treatment rules from observational data without eliciting patient preferences.

- We propose a novel framework for using observational data to estimate a composite outcome and the corresponding optimal individualized treatment rule.

- This represents a new paradigm for the use of observational data in the context of precision medicine in that clinical decisions are viewed as a (noisy) surrogate for patient preferences and leveraged to improve the quality of a learned treatment rule and to generate new insights into heterogeneity in patient preferences.

- An individualized treatment rule formalizes precision medicine as a map from the space of patient covariates into the space of allowable treatments (Murphy, 2003;Robins, 2004).




# [Determining the Number of Communities in Degree-corrected Stochastic Block Models](https://jmlr.org/papers/volume22/20-037/20-037.pdf)
- Based on these properties, we show the consistency of our estimator for the true number of communities.

- This estimation approach enables us to establish the limiting distribution of the pseudo likelihood ratio when the model is under-fitted, and derive the upper bound for it when the model is over-fitted.

- Lastly, we obtain the estimator of the true number of communities based on the change of the pseudo-LR.

- We propose a new pseudo conditional likelihood ratio method for selecting the number of communities in DCSBMs.




# [Regulating Greed Over Time in Multi-Armed Bandits](https://jmlr.org/papers/volume22/17-720/17-720.pdf)
- Some multiplier functions will work better than others for regulating greed over time,

- This section illustrates the problem, the proposed algorithms to regulate greed over time, and theoretical results on the bound on the expected regret of each policy.

- This is the simplest method we know that would allow regulating greed over time.

- However, the "smarter" algorithms do not regulate greed over time and their performance is worse than the algorithms that do this regulation.




# [Multi-view Learning as a Nonparametric Nonlinear Inter-Battery Factor Analysis](https://jmlr.org/papers/volume22/16-179/16-179.pdf)
- Therefore, we show how inter-battery factor analysis naturally manifests itself as a Gaussian process latent variable model.

- In summary, the main contributions of our paper are the following: • a probabilistic, nonlinear, nonparametric formulation of inter-battery factor analysis, • a variational framework for approximate, data-efficient Bayesian learning, • a framework for introducing priors that encourage specific latent space configurations, • a multi-view latent consolidation model that naturally extends beyond two views; to our knowledge, we present the first work which shows results of a data-efficient, factorized generative model with truly large number of views.

- A large portion of the multi-view learning literature has been motivated CCA.

- In (Sharma et al., 2012) it is shown how several different factor analysis models can be extended to the multi-view setting.




# [A general linear-time inference method for Gaussian Processes on one dimension](https://jmlr.org/papers/volume22/21-0072/21-0072.pdf)
- To prove this theorem, we develop a convenient family of Gaussian hidden Markov models on one dimension: the Latent Exponentially Generated (LEG) process.

- Second, we develop a new unconstrained parameterization of continuous-time state-space models, making it easy to use simple gradient-descent methods.

- Second, an unconstrained parameterization for state-space models is developed (the LEG family).

- Here we present a new theorem which lays the "generality" question to rest for time-series data with vector-valued observations.




# [Optimization with Momentum: Dynamical, Control-Theoretic, and Symplectic Perspectives](https://jmlr.org/papers/volume22/20-207/20-207.pdf)
- This article characterizes the convergence rate of momentum-based optimization algorithms by taking fundamental topological properties into account.

- We have presented a convergence-rate analysis of momentum-based optimization algorithms from a dynamical systems point of view.

- The subsequent result derived in Section 2.4 highlights our assertion that fundamental topological properties can be exploited for characterizing the convergence rate of momentum-based optimization algorithms.

- • Accelerated convergence is generic to momentum-based optimization algorithms, provided that the damping scales with 1/ √ κ for large κ.




# [An Empirical Study of Bayesian Optimization: Acquisition Versus Partition](https://jmlr.org/papers/volume22/18-220/18-220.pdf)
- We presented experimental results comparing PGO, PBO, and ABO methods within a common open-source evaluation framework.

- To perform these experiments, we built a custom, extendable C++ framework that executes easily-repeatable evaluations of arbitrary black-box optimization algorithms.

- We conduct this investigation using a common software framework, which will be publicly available and allow for complete reproducibility.

- Third, we describe two partitioning-based Bayesian optimization (PBO) algorithms, which incorporate a Bayesian prior into the PGO approaches.




# [MushroomRL: Simplifying Reinforcement Learning Research](https://jmlr.org/papers/volume22/18-056/18-056.pdf)
- We developed the MushroomRL Benchmarking Suite, a framework based on MushroomRL for running large-scale benchmarking experiments on the already provided algorithms, or new ones implemented by users.

- Our results are comparable with the ones in literature assert the quality of the implementation of the algorithms in MushroomRL.

- ChainerRL is a deep RL library based on the deep learning library Chainer.

- OpenAI Baselines (Dhariwal et al., 2017) is one of the most famous examples of deep RL libraries.




# [Empirical Bayes Matrix Factorization](https://jmlr.org/papers/volume22/20-589/20-589.pdf)
- Our primary contribution here is to develop and implement a more general EB approach to matrix factorization (EBMF).

- Here we apply them to matrix factorization problems.

- We demonstrate the utility of these methods through both numerical comparisons with competing methods and through a scientific application: analysis of data from the GTEx (Genotype Tissue Expression) project on genetic associations across 44 human tissues.

- Matrix factorization methods are widely used for inferring and summarizing structure in multivariate data.




# [Hamilton-Jacobi Deep Q-Learning for Deterministic Continuous-Time Systems with Lipschitz Continuous Controls](https://jmlr.org/papers/volume22/20-1235/20-1235.pdf)
- In Section 3, we propose a Q-learning method based on the semi-discrete HJB equation and analyze its convergence properties.

- A novel class of HJB equations for Q-functions has been derived and used to construct a Q-learning method for continuous-time control.

- Applying the dynamic programming principle to the continuous-time Q-function, we derive a novel class of HJB equations.

- A Q-learning algorithm and its DQN variant are newly designed in a principled manner to use transition data collected in discrete time with a theoretically consistent target.




# [Locally Private k-Means Clustering](https://jmlr.org/papers/volume22/20-721/20-721.pdf)
- In this work we study the Euclidean k-means problem in the local model of differential privacy (LDP).

- In this work, we design a new locally-private algorithm for the Euclidean k-means and kmedian problems.

- Indeed, in Section 5 we show that every locally-private algorithm for the k-means must have additive error Ω( √ n).

- One of the most well-studied problems in this context is the Euclidean k-means problem.




# [A Unified Sample Selection Framework for Output Noise Filtering: An Error-Bound Perspective](https://jmlr.org/papers/volume22/19-498/19-498.pdf)
- We propose a unified framework of optimal sample selection (OSS) for effectiveness determination and optimal sample selection in output noise filtering from the perspective of GE bound.

- In another word, they provide a unified framework, named as the optimal sample selection (OSS) framework, for the output noise filtering in regression.

- The covering distance is proposed for estimating the output noise, and it is integrated with the OSS framework, generating the CDF filtering algorithm.

- The theoretical foundations for the determination of effective noise filtering and the optimal sample selection are provided, and then a unified framework of the output noise filtering, which can be integrated with any noise estimator, is built.




# [NUQSGD: Provably Communication-efficient Data-parallel SGD via Nonuniform Quantization](https://jmlr.org/papers/volume22/20-255/20-255.pdf)
- • We propose a non-uniform gradient quantization method and establish strong theoretical guarantees for its excess variance and communication costs.

- We study data-parallel and communication-efficient version of stochastic gradient descent.

- In this paper, we propose and study a new scheme to quantize normalized gradient vectors.

- compared to the QSGDinf heuristic, which has no theoretical compression guarantees




# [How Well Generative Adversarial Networks Learn Distributions](https://jmlr.org/papers/volume22/20-911/20-911.pdf)
- The main technical contributions are the development of the oracle inequalities for analyzing GANs, and the formulation of the new generator-discriminator-pair regularization.

- We develop the oracle inequalities, which are the main tool for analyzing the rates of GANs.

- Further discussions on the generator-discriminator-pair regularization and connections to the regularity theory in optimal transport is deferred to Section 4.

- The goal of this section is to study the optimal minimax rates for learning a wide range of distributions, on a host of evaluation metric defined by the adversarial framework.




# [Prediction against a limited adversary](https://jmlr.org/papers/volume22/20-1234/20-1234.pdf)
- Using viscosity theory tools in the field of partial differential equation, we provided the growth rate of regret for the forecaster.

- Viscosity solution theory provides formidable tools to rigorously show this convergence and study the properties of the long-time behavior of the value function.

- (ii) Balanced strategies exist, Φ satisfies Assumption 2 (iii): the growth rate of regret is given by the solution of ( 24); see Theorem 12.

- Using these tools, we show that the long-time behavior of the regret have different regimes




# [A Greedy Algorithm for Quantizing Neural Networks](https://jmlr.org/papers/volume22/20-1233/20-1233.pdf)
- Specifically, • We propose a novel algorithm in (2) and (3) for sequentially quantizing layers of a pre-trained neural network in a data-dependent manner.

- • We provide numerical simulations in Section 6 for quantizing networks trained on the benchmark data sets MNIST and CIFAR10 using both multilayer perceptrons and convolutional neural networks.

- These complexities are already quite restrictive and only give the run-time for quantizing a single neuron.

- The goal of this paper is to propose a framework for quantizing neural networks without sacrificing their predictive power, and to provide theoretical justification for our framework.




# [Are we Forgetting about Compositional Optimisers in Bayesian Optimisation?](https://jmlr.org/papers/volume22/20-1422/20-1422.pdf)
- In this paper, we presented an in-depth study of acquisition function maximisation in Bayesian optimisation.

- Compositional acquisition function maximisation requires considerably larger memory relative to ERM.

- We provide a general overview of compositional optimisation and derive compositional forms for the four most popular myopic acquisition functions.

- In order to both improve and analyse the optimisation performance on the compositional form of the acquisition function, we introduce several algorithmic adaptations.




# [Projection-free Decentralized Online Learning for Submodular Maximization over Time-Varying Networks](https://jmlr.org/papers/volume22/18-407/18-407.pdf)
- • We propose a decentralized one-shot Frank-Wolfe online learning method over timevarying networks for submodular maximization in the stochastic online setting, where each agent uses local communication and local computation.

- In the stochastic online setting, we propose a decentralized one-shot Frank-Wolfe online learning method for solving the problem (8).

- Additionally, we have proposed a distributed one-shot Frank-Wolfe online learning algorithm for the stochastic online setting.

- • We also show that the decentralized one-shot Frank-Wolfe online learning method can achieve (1 − 1/e)-regret with a bound O(T 2/3 ).




# [Representer Theorems in Banach Spaces: Minimum Norm Interpolation, Regularized Learning and Semi-Discrete Inverse Problems](https://jmlr.org/papers/volume22/20-751/20-751.pdf)
- In the literature there are a few existing representer theorems for regularized learning problems in a Banach space.

- We also present the infimum of the MNI problem in a Banach space.

- We first describe the MNI problem in a Banach space.

- In section 5, we propose fixed-point equations for the MNI problem in a Banach space.




# [Stochastic Proximal AUC Maximization](https://jmlr.org/papers/volume22/19-418/19-418.pdf)
- • We propose a novel stochastic proximal algorithm for AUC maximization which accommodates general convex regularizers with favorable O(d) space and per-iteration time complexities.

- Our objective is to develop efficient SGD-type algorithms for AUC maximization scalable to large-scale streaming data.

- In this paper, we presented a new stochastic gradient descent method for AUC maximization which can accommodate general penalty terms.

- Natole et al. (2018) developed a stochastic proximal algorithm for AUC maximization with a convergence rate O(1/T ) for strongly convex objective function.




# [Unfolding-Model-Based Visualization: Theory, Method and Applications](https://jmlr.org/papers/volume22/18-846/18-846.pdf)
- Second, an estimator is proposed for the ideal points and an asymptotic theory is provided for this estimator, shedding lights on the validity of model-based visualization.

- Finally, an efficient alternating projected gradient algorithm is proposed for the computation which is scalable to large-scale problems.

- Under reasonable conditions, we provide asymptotic results for the recovery of ideal-point configuration.

- Then it is relatively easy to recover the configuration of the ideal points from a perturbed partial distance matrix.




# [The Decoupled Extended Kalman Filter for Dynamic Exponential-Family Factorization Models](https://jmlr.org/papers/volume22/18-417/18-417.pdf)
- A reader familiar with the extended Kalman filter may find it difficult to map the above expressions onto the standard EKF expressions.

- In factorization models, typically d k.

- Algorithm 2: The DEKF optimized for dynamic factorization models.

- This derivation directly illustrates why the DEKF is particularly appropriate for factorization models.




# [Conditional independences and causal relations implied by sets of equations](https://jmlr.org/papers/volume22/20-863/20-863.pdf)
- In Section 4 we will show how the Markov ordering graph can be constructed from a causal ordering graph.

- We then prove that Simon's causal ordering algorithm is well-defined and has a unique output.

- We showed that the causal ordering graph, on the other hand, does encode the effects of soft and certain perfect interventions.

- 17 We prove that the causal ordering graph represents the effects of both soft interventions on equations and perfect interventions on 17.




# [Bandit Convex Optimization in Non-stationary Environments](https://jmlr.org/papers/volume22/20-763/20-763.pdf)
- As a result, our proposed algorithm is more adaptive to the non-stationary environments than BCO algorithms designed for minimizing static regret or worse-case dynamic regret.

- • We establish the first minimax lower bound of the universal dynamic regret for bandit convex optimization problems.

- In the experiments, we choose the cumulative loss as the performance measure of different learning algorithms.

- In this paper, we study the bandit convex optimization (BCO) problems in non-stationary environments.




# [Simple and Fast Algorithms for Interactive Machine Learning with Random Counter-examples](https://jmlr.org/papers/volume22/19-372/19-372.pdf)
- In this work we provided simple and efficient algorithms for interactively learning nonbinary concepts in the recently proposed setting of exact learning from random counterexamples (LRC).

- We also provided an analysis that shows that interactive LRC learning, regardless of the learning algorithm, is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning.

- It also establishes that LRC learning, regardless of the learning algorithm, is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning (Kearns and Vazirani, 1994).

- We also provide a mathematical proof that interactive LRC learning is at least as efficient as non-interactive Probably Approximately Correct (PAC) learning (Kearns and Vazirani, 1994) regardless of the learning algorithm used by the interactive learner.




# [mvlearn: Multiview Machine Learning in Python](https://jmlr.org/papers/volume22/20-1370/20-1370.pdf)
- Compose: Several functions for integrating single-view and multiview methods are implemented, facilitating operations such as preprocessing, merging, or creating multiview data sets.

- Also, plotting tools extend matplotlib and seaborn to facilitate visualizing multiview data.

- Cluster: mvlearn contains multiple algorithms for multiview clustering, which can better take advantage of multiview data by using unsupervised adaptations of cotraining.

- With these methods accessible to non-specialists, multiview learning algorithms will be able to improve results in academic and industry applications of machine learning.




# [Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions](https://jmlr.org/papers/volume22/21-0287/21-0287.pdf)
- We undertake a novel perturbation analysis for low multilinear rank tensor CUR approximations, and prove error bounds for the two primary tensor CUR decompositions discussed here.

- There are very few approximation bounds for any tensor CUR decompositions.

- The runtime and approximation performance of the tensor CUR decompositions are compared against the other state-of-the-art methods.

- It is interesting, yet challenging, to derive upper bounds for tensor CUR decompositions in terms of the optimal low multilinear rank approximations of the given tensor.




# [A Unified Analysis of First-Order Methods for Smooth Games via Integral Quadratic Constraints](https://jmlr.org/papers/volume22/20-1068/20-1068.pdf)
- Table 1: Global convergence rates of algorithms for smooth and strongly-monotone games.

- Further, we adapt the IQC framework to analyze stochastic games.

- • We also show that the optimistic gradient method achieves the optimal convergence rate provable in our framework among algorithms with one step of memory (6).

- • We derive a slightly improved convergence rate for the optimistic gradient method (even though the existing analysis (Gidel et al., 2018) is fairly involved).




# [A Distributed Method for Fitting Laplacian Regularized Stratified Models](https://jmlr.org/papers/volume22/19-345/19-345.pdf)
- Algorithm 4.1 Distributed method for fitting stratified models with Laplacian regularization.

- We now explore the idea of fitting stratified models with nonconvex local loss and regularization functions.

- Compared to simple stratified models, our Laplacian regularized stratified models have several advantages.

- Second, we propose an efficient distributed solution method based on the alternating direction method of multipliers (ADMM) (Boyd et al., 2011), which allows us to solve the optimization problem in a distributed fashion and at very large scale, in spite of coupling of the model parameters in the Laplacian regularization term.




# [Prediction Under Latent Factor Regression: Adaptive PCR, Interpolating Predictors and Beyond](https://jmlr.org/papers/volume22/20-768/20-768.pdf)
- 1. General finite sample risk bounds for linear predictors, under factor regression models.

- 2. Finite sample risk bounds for PCRs, with data-adaptive s principal components.

- Our main applications will be to the finite sample risk bounds of the three classes of predictors discussed in the previous section.

- The following corollary states the prediction risk of the OLS under the factor regression model.




# [Testing Conditional Independence via Quantile Regression Based Partial Copulas](https://jmlr.org/papers/volume22/20-1074/20-1074.pdf)
- The second main contribution of this paper is an analysis of a nonparametric test for conditional independence based on the partial copula construction.

- based on quantile regression.

- In this work we take a novel approach to testing conditional independence using the partial copula by using quantile regression for estimating the conditional distribution functions.

- Independence in the partial copula relates to conditional independence in the following way.




